"""
ç«–å±çŸ­å‰§è¯„ä¼°Agent
 ä¸“æ³¨äºæ•…äº‹æ–‡æœ¬è¯„ä¼°å’Œæ‰“åˆ†

ä¸šåŠ¡å¤„ç†é€»è¾‘ï¼š
1. è¾“å…¥å¤„ç†ï¼šæ¥æ”¶ç«–å±çŸ­å‰§æ•…äº‹æ–‡æœ¬ï¼Œæ”¯æŒå¤šç§è¾“å…¥æ ¼å¼
2. æ·±åº¦è¯„ä¼°ï¼šå¯¹æ•…äº‹æ–‡æœ¬è¿›è¡Œæ·±åº¦è¯„ä¼°ä¸æ‰“åˆ†
3. å¤šç»´åº¦åˆ†æï¼šä»æ ¸å¿ƒçˆ½ç‚¹ã€æ•…äº‹ç±»å‹ã€äººç‰©è®¾å®šç­‰ç»´åº¦è¿›è¡Œä¸“ä¸šåˆ†æ
4. å¸‚åœºç«äº‰åŠ›åˆ†æï¼šåˆ†ææ•…äº‹åœ¨ç«–å±çŸ­å‰§å¸‚åœºçš„ç«äº‰åŠ›
5. å¼€å‘ä»·å€¼è¯„ä¼°ï¼šè¯„ä¼°æ•…äº‹çš„å½±è§†å¼€å‘ä»·å€¼å’Œæ½œåŠ›
6. ä¼˜åŒ–å»ºè®®ï¼šæä¾›å…·ä½“çš„ä¼˜åŒ–å»ºè®®å’Œæ”¹è¿›æ–¹å‘
7. æ–‡ä»¶å¤„ç†ï¼šæ”¯æŒæ–‡ä»¶å†…å®¹æå–å’Œå¤„ç†
8. è¾“å‡ºæ ¼å¼åŒ–ï¼šè¿”å›ç»“æ„åŒ–çš„è¯„ä¼°ç»“æœå’Œå»ºè®®
9. Agent as Toolï¼šæ”¯æŒè¢«å…¶ä»–æ™ºèƒ½ä½“è°ƒç”¨ï¼Œå®ç°ä¸Šä¸‹æ–‡éš”ç¦»

ä»£ç ä½œè€…ï¼šå®«çµç‘
åˆ›å»ºæ—¶é—´ï¼š2025å¹´10æœˆ19æ—¥
"""
import asyncio
import logging
import re
from typing import Dict, Any, List, Optional, AsyncGenerator
from pathlib import Path
from datetime import datetime
from dataclasses import dataclass, field, asdict
from enum import Enum
import json

try:
    from .base_juben_agent import BaseJubenAgent
    from ..utils.intent_recognition import IntentRecognizer
    from ..utils.url_extractor import URLExtractor
except ImportError:
    # å¤„ç†ç›¸å¯¹å¯¼å…¥é—®é¢˜
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))

    from agents.base_juben_agent import BaseJubenAgent
    from utils.intent_recognition import IntentRecognizer
    from utils.url_extractor import URLExtractor


# ==================== æ•°æ®æ¨¡å‹ ====================

class ScoreCategory(Enum):
    """è¯„æµ‹ç»´åº¦"""
    LOGIC = "logic"
    CHARACTER = "character"
    HOOK = "hook"
    DIALOGUE = "dialogue"
    PACING = "pacing"
    EMOTION = "emotion"
    CREATIVITY = "creativity"
    COMMERCIAL = "commercial"


@dataclass
class EvaluationResult:
    """è¯„æµ‹ç»“æœ"""
    overall_score: float
    scores: Dict[str, float]
    reasons: Dict[str, str]
    overall_reason: str
    suggestions: List[str]
    strengths: List[str]
    weaknesses: List[str]
    commercial_potential: float
    target_audience_match: float
    metadata: Dict[str, Any] = field(default_factory=dict)
    evaluated_at: str = field(default_factory=lambda: datetime.now().isoformat())

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


@dataclass
class ComparisonResult:
    """å¯¹æ¯”è¯„æµ‹ç»“æœ"""
    version_a: Dict[str, Any]
    version_b: Dict[str, Any]
    winner: str  # "A", "B", "TIE"
    score_delta: Dict[str, float]
    overall_delta: float
    comparison_summary: str
    recommendation: str
    compared_at: str = field(default_factory=lambda: datetime.now().isoformat())

    def to_dict(self) -> Dict[str, Any]:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return asdict(self)


class ShortDramaEvaluationAgent(BaseJubenAgent):
    """
    ç«–å±çŸ­å‰§è¯„ä¼°Agent
    
    åŠŸèƒ½ï¼š
    1. æ•…äº‹æ–‡æœ¬æ·±åº¦è¯„ä¼°ä¸æ‰“åˆ†
    2. å¤šç»´åº¦ä¸“ä¸šåˆ†æï¼ˆæ ¸å¿ƒçˆ½ç‚¹ã€æ•…äº‹ç±»å‹ã€äººç‰©è®¾å®šç­‰ï¼‰
    3. å¸‚åœºç«äº‰åŠ›åˆ†æ
    4. å¼€å‘ä»·å€¼è¯„ä¼°
    5. ä¼˜åŒ–å»ºè®®æä¾›
    6. æ–‡ä»¶å†…å®¹æå–å’Œå¤„ç†
    """
    
    def __init__(self):
        super().__init__("short_drama_evaluation", model_provider="zhipu")
        
        # ç³»ç»Ÿæç¤ºè¯é…ç½®ï¼ˆä»promptsæ–‡ä»¶å¤¹åŠ è½½ï¼‰
        self._load_system_prompt()
        
        # åˆå§‹åŒ–ä¸“ç”¨ç»„ä»¶
        self.intent_recognizer = IntentRecognizer()
        self.url_extractor = URLExtractor()
        
        # è¯„ä¼°ç»´åº¦é…ç½®
        self.evaluation_dimensions = {
            "core_satisfaction": "æ ¸å¿ƒçˆ½ç‚¹",
            "story_type": "æ•…äº‹ç±»å‹", 
            "character_setting": "äººç‰©è®¾å®š",
            "character_relationship": "äººç‰©å…³ç³»",
            "plot_bridge": "æƒ…èŠ‚æ¡¥æ®µ"
        }
        
        # è¯„åˆ†æ ‡å‡†
        self.scoring_criteria = {
            "excellent": {"min": 8.5, "max": 10.0, "description": "ä¼˜ç§€ï¼Œå¯ç›´æ¥å¼€å‘"},
            "potential": {"min": 8.0, "max": 8.4, "description": "æœ‰æ½œåŠ›ï¼Œéœ€ä¿®æ”¹åå¼€å‘"},
            "average": {"min": 7.5, "max": 7.9, "description": "ä¸€èˆ¬ï¼Œéœ€å¤§å¹…ä¿®æ”¹"},
            "poor": {"min": 0.0, "max": 7.4, "description": "è¾ƒå·®ï¼Œå¼€å‘ä»·å€¼ä½"}
        }
        
        self.logger.info("ç«–å±çŸ­å‰§è¯„ä¼°Agentåˆå§‹åŒ–å®Œæˆ")
    
    async def process_request(
        self, 
        request_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¤„ç†ç«–å±çŸ­å‰§è¯„ä¼°è¯·æ±‚
        
        Args:
            request_data: è¯·æ±‚æ•°æ®
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Yields:
            Dict: æµå¼å“åº”äº‹ä»¶
        """
        try:
            # æå–è¯·æ±‚ä¿¡æ¯
            user_input = request_data.get("input", "")
            user_id = context.get("user_id", "unknown") if context else "unknown"
            session_id = context.get("session_id", "unknown") if context else "unknown"
            
            self.logger.info(f"å¼€å§‹å¤„ç†çŸ­å‰§è¯„ä¼°è¯·æ±‚: {user_input}")
            
            # åˆå§‹åŒ–Tokenç´¯åŠ å™¨
            await self.initialize_token_accumulator(user_id, session_id)
            
            # å‘é€å¼€å§‹å¤„ç†äº‹ä»¶
            yield await self._emit_event("system", "ğŸ“Š å¼€å§‹åˆ†ææ‚¨çš„çŸ­å‰§è¯„ä¼°éœ€æ±‚...")
            
            # 1. æ„å›¾è¯†åˆ«
            yield await self._emit_event("system", "ğŸ” æ­£åœ¨åˆ†ææ‚¨çš„è¯„ä¼°æ„å›¾...")
            intent_result = await self._analyze_intent(user_input)
            yield await self._emit_event("system", f"âœ… æ„å›¾è¯†åˆ«å®Œæˆ: {intent_result['intent']}")
            
            # 2. URLæå–å’Œå†…å®¹è·å–
            urls = self.url_extractor.extract_urls(user_input)
            url_contents = []
            if urls:
                yield await self._emit_event("system", f"ğŸ“ å‘ç°{len(urls)}ä¸ªé“¾æ¥ï¼Œæ­£åœ¨æå–å†…å®¹...")
                url_contents = await self._extract_url_contents(urls)
                yield await self._emit_event("system", "âœ… URLå†…å®¹æå–å®Œæˆ")
            
            # 3. ä¿¡æ¯æ”¶é›†
            search_results = {}
            knowledge_results = {}
            
            # ç½‘ç»œæœç´¢
            if intent_result.get("needs_web_search", False):
                yield await self._emit_event("system", "ğŸŒ æ­£åœ¨æœç´¢æœ€æ–°å¸‚åœºä¿¡æ¯...")
                search_query = self._build_search_query(user_input, intent_result)
                search_results = await self._search_web(search_query)
                yield await self._emit_event("system", "âœ… ç½‘ç»œæœç´¢å®Œæˆ")
            
            # çŸ¥è¯†åº“æ£€ç´¢
            if intent_result.get("needs_knowledge_base", False):
                yield await self._emit_event("system", "ğŸ“š æ­£åœ¨æ£€ç´¢é«˜èƒ½çŸ­å‰§åº“...")
                knowledge_query = self._build_knowledge_query(user_input, intent_result)
                knowledge_results = await self._search_knowledge_base(knowledge_query)
                yield await self._emit_event("system", "âœ… çŸ¥è¯†åº“æ£€ç´¢å®Œæˆ")
            
            # 4. æ„å»ºä¸Šä¸‹æ–‡
            context_data = {
                "user_input": user_input,
                "intent": intent_result,
                "search_results": search_results,
                "knowledge_results": knowledge_results,
                "url_contents": url_contents,
                "user_id": user_id,
                "session_id": session_id,
                "history": context.get("history", []) if context else []
            }
            
            # 5. ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
            yield await self._emit_event("system", "ğŸ“‹ æ­£åœ¨ç”Ÿæˆä¸“ä¸šçš„è¯„ä¼°æŠ¥å‘Š...")
            
            async for chunk in self._generate_evaluation_response(context_data):
                yield chunk
            
            # 6. è·å–Tokenè®¡è´¹æ‘˜è¦
            billing_summary = await self.get_token_billing_summary()
            if billing_summary:
                yield await self._emit_event("billing", f"ğŸ“Š Tokenæ¶ˆè€—: {billing_summary['total_tokens']} tokens, ç§¯åˆ†æ‰£å‡: {billing_summary['deducted_points']} ç§¯åˆ†")
            
            # 7. å‘é€å®Œæˆäº‹ä»¶
            yield await self._emit_event("system", "ğŸ¯ çŸ­å‰§è¯„ä¼°æŠ¥å‘Šç”Ÿæˆå®Œæˆï¼")
            
        except Exception as e:
            self.logger.error(f"å¤„ç†è¯·æ±‚å¤±è´¥: {e}")
            yield await self._emit_event("error", f"å¤„ç†å¤±è´¥: {str(e)}")
    
    async def _analyze_intent(self, user_input: str) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·è¯„ä¼°æ„å›¾"""
        try:
            # è¯„ä¼°ç›¸å…³çš„æ„å›¾è¯†åˆ«
            intent_result = await self.intent_recognizer.analyze(user_input)
            
            # æ ¹æ®è¯„ä¼°éœ€æ±‚è°ƒæ•´æ„å›¾
            if "è¯„ä¼°" in user_input or "æ‰“åˆ†" in user_input or "åˆ†æ" in user_input:
                intent_result.update({
                    "intent": "story_evaluation",
                    "needs_knowledge_base": True,
                    "needs_web_search": True
                })
            elif "çˆ½ç‚¹" in user_input or "æ ¸å¿ƒ" in user_input:
                intent_result.update({
                    "intent": "core_satisfaction_analysis",
                    "needs_knowledge_base": True,
                    "needs_web_search": False
                })
            elif "äººç‰©" in user_input or "è§’è‰²" in user_input:
                intent_result.update({
                    "intent": "character_analysis",
                    "needs_knowledge_base": True,
                    "needs_web_search": False
                })
            elif "æƒ…èŠ‚" in user_input or "å‰§æƒ…" in user_input:
                intent_result.update({
                    "intent": "plot_analysis",
                    "needs_knowledge_base": True,
                    "needs_web_search": False
                })
            elif "å¸‚åœº" in user_input or "ç«å“" in user_input:
                intent_result.update({
                    "intent": "market_analysis",
                    "needs_knowledge_base": True,
                    "needs_web_search": True
                })
            
            return intent_result
        except Exception as e:
            self.logger.error(f"æ„å›¾è¯†åˆ«å¤±è´¥: {e}")
            return {
                "intent": "story_evaluation",
                "confidence": 0.5,
                "needs_web_search": True,
                "needs_knowledge_base": True
            }
    
    async def _extract_url_contents(self, urls: List[str]) -> List[Dict[str, Any]]:
        """æå–URLå†…å®¹"""
        contents = []
        for url in urls:
            try:
                content = await self.url_extractor.extract_content(url)
                contents.append(content)
            except Exception as e:
                self.logger.error(f"æå–URLå†…å®¹å¤±è´¥ {url}: {e}")
                contents.append({
                    "url": url,
                    "success": False,
                    "error": str(e)
                })
        return contents
    
    def _build_search_query(self, user_input: str, intent_result: Dict[str, Any]) -> str:
        """æ„å»ºæœç´¢æŸ¥è¯¢"""
        intent = intent_result.get("intent", "story_evaluation")
        
        if intent == "story_evaluation":
            return f"{user_input} ç«–å±çŸ­å‰§ å¸‚åœºåˆ†æ çˆ†æ¬¾æ¡ˆä¾‹"
        elif intent == "core_satisfaction_analysis":
            return f"{user_input} æ ¸å¿ƒçˆ½ç‚¹ æƒ…ç»ªè®¾è®¡"
        elif intent == "character_analysis":
            return f"{user_input} äººç‰©è®¾å®š è§’è‰²å¡‘é€ "
        elif intent == "plot_analysis":
            return f"{user_input} æƒ…èŠ‚è®¾è®¡ å‰§æƒ…ç»“æ„"
        elif intent == "market_analysis":
            return f"{user_input} ç«–å±çŸ­å‰§å¸‚åœº ç«å“åˆ†æ"
        else:
            return user_input
    
    def _build_knowledge_query(self, user_input: str, intent_result: Dict[str, Any]) -> str:
        """æ„å»ºçŸ¥è¯†åº“æŸ¥è¯¢"""
        intent = intent_result.get("intent", "story_evaluation")
        
        if intent == "story_evaluation":
            return f"{user_input} è¯„ä¼°æ ‡å‡† è¯„åˆ†ä½“ç³»"
        elif intent == "core_satisfaction_analysis":
            return f"{user_input} çˆ½ç‚¹è®¾è®¡ æƒ…ç»ªæ§åˆ¶"
        elif intent == "character_analysis":
            return f"{user_input} äººç‰©è®¾å®š è§’è‰²åŠŸèƒ½"
        elif intent == "plot_analysis":
            return f"{user_input} æƒ…èŠ‚æ¡¥æ®µ å‰§æƒ…èŠ‚å¥"
        elif intent == "market_analysis":
            return f"{user_input} å¸‚åœºè¶‹åŠ¿ çˆ†æ¬¾ç‰¹å¾"
        else:
            return user_input
    
    async def _generate_evaluation_response(self, context_data: Dict[str, Any]) -> AsyncGenerator[Dict[str, Any], None]:
        """ç”Ÿæˆè¯„ä¼°å“åº”"""
        try:
            # æ„å»ºæç¤ºè¯
            prompt = self._build_evaluation_prompt(context_data)
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "user", "content": prompt}
            ]
            
            # è·å–ç”¨æˆ·IDå’Œä¼šè¯ID
            user_id = context_data.get("user_id", "unknown")
            session_id = context_data.get("session_id", "unknown")
            
            # æµå¼è°ƒç”¨LLMï¼ˆå¸¦è¿½è¸ªï¼‰
            async for chunk in self._stream_llm(messages, user_id=user_id, session_id=session_id):
                yield await self._emit_event("llm_chunk", chunk)
                
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆè¯„ä¼°å“åº”å¤±è´¥: {e}")
            yield await self._emit_event("error", f"ç”Ÿæˆå“åº”å¤±è´¥: {str(e)}")
    
    def _build_evaluation_prompt(self, context_data: Dict[str, Any]) -> str:
        """
        æ„å»ºè¯„ä¼°æç¤ºè¯ï¼ˆå¥å£®ç‰ˆï¼‰

        è¯´æ˜ï¼š
        - å¯¹ context_data ä¸­çš„å„å­—æ®µåšç±»å‹æ£€æŸ¥ï¼Œé¿å…å‡ºç°å¦‚ slice(None, 3, None) è¿™ç±»é”™è¯¯ï¼›
        - ä»»æ„ä¸€å¤„ä¸Šä¸‹æ–‡ç»“æ„å¼‚å¸¸æ—¶ï¼Œä¼šè®°å½•æ—¥å¿—å¹¶é€€åŒ–ä¸ºæœ€å°å¯ç”¨ Promptï¼Œè€Œä¸æ˜¯æŠ›å‡ºå¼‚å¸¸é˜»æ–­è¯„ä¼°ã€‚
        """
        try:
            user_input = context_data.get("user_input", "")
            intent = context_data.get("intent") or {}
            if not isinstance(intent, dict):
                intent = {}

            search_results = context_data.get("search_results") or {}
            if not isinstance(search_results, dict):
                search_results = {}

            knowledge_results = context_data.get("knowledge_results") or {}
            if not isinstance(knowledge_results, dict):
                knowledge_results = {}

            url_contents = context_data.get("url_contents") or []
            if not isinstance(url_contents, list):
                url_contents = []

            history = context_data.get("history") or []

            # æ„å»ºç”¨æˆ·æŸ¥è¯¢éƒ¨åˆ†
            user_query_section = f"""
## ç”¨æˆ·è¯„ä¼°éœ€æ±‚
{user_input}

## éœ€æ±‚åˆ†æ
- è¯„ä¼°ç±»å‹: {intent.get('intent', 'unknown')}
- ç½®ä¿¡åº¦: {intent.get('confidence', 0)}

## äº’è”ç½‘æœç´¢ä¿¡æ¯
"""

            # æ·»åŠ æœç´¢ç»“æœ
            results_list = search_results.get("results") if search_results.get("success") else None
            if isinstance(results_list, list) and results_list:
                user_query_section += "\n### æœ€æ–°å¸‚åœºä¿¡æ¯\n"
                for i, result in enumerate(results_list[:3], 1):
                    if not isinstance(result, dict):
                        continue
                    user_query_section += f"{i}. {result.get('title', '')}\n"
                    content_snippet = str(result.get("content", ""))[:200]
                    if content_snippet:
                        user_query_section += f"   {content_snippet}...\n"

            # æ·»åŠ çŸ¥è¯†åº“ç»“æœ
            kb_list = knowledge_results.get("results") if knowledge_results.get("success") else None
            if isinstance(kb_list, list) and kb_list:
                user_query_section += "\n### é«˜èƒ½çŸ­å‰§åº“è¯­ä¹‰æœç´¢ç»“æœ\n"
                for i, result in enumerate(kb_list[:3], 1):
                    if not isinstance(result, dict):
                        continue
                    user_query_section += f"{i}. {result.get('title', '')}\n"
                    content_snippet = str(result.get("content", ""))[:200]
                    if content_snippet:
                        user_query_section += f"   {content_snippet}...\n"

            # æ·»åŠ URLå†…å®¹
            if url_contents:
                user_query_section += "\n### ç”¨æˆ·ä¸Šä¼ æ–‡ä»¶å†…å®¹\n"
                for i, content in enumerate(url_contents[:2], 1):
                    if not isinstance(content, dict):
                        continue
                    if content.get("success"):
                        user_query_section += f"{i}. {content.get('url', '')}\n"
                        content_snippet = str(content.get("content", ""))[:200]
                        if content_snippet:
                            user_query_section += f"   {content_snippet}...\n"

            # æ·»åŠ å¯¹è¯å†å²ï¼ˆå…¼å®¹å¤šç§ç±»å‹ï¼Œé¿å…åˆ‡ç‰‡é”™è¯¯ï¼‰
            if history:
                try:
                    if isinstance(history, list):
                        recent_history = history[-3:]  # åªæ˜¾ç¤ºæœ€è¿‘3æ¡
                        if recent_history:
                            user_query_section += "\n### ç”¨æˆ·å¯¹è¯å†å²\n"
                            for i, hist in enumerate(recent_history, 1):
                                user_query_section += f"{i}. {hist}\n"
                except Exception as e:
                    # å†å²è®°å½•å¼‚å¸¸ä¸å½±å“ä¸»æµç¨‹
                    self.logger.warning(f"å¤„ç†å†å²å¯¹è¯æ—¶å‡ºé”™ï¼Œå·²å¿½ç•¥: {e}")

            # å°†ç”¨æˆ·æŸ¥è¯¢éƒ¨åˆ†æ·»åŠ åˆ°ç³»ç»Ÿæç¤ºè¯åé¢
            full_prompt = f"{self.system_prompt}\n\n{user_query_section}"

            return full_prompt

        except Exception as e:
            # ä»»ä½•æ„å»º Prompt çš„å¼‚å¸¸éƒ½ä¸åº”è¯¥é˜»å¡è¯„ä¼°ï¼Œé€€åŒ–ä¸ºç®€å• Prompt
            self.logger.error(f"æ„å»ºè¯„ä¼°æç¤ºè¯å¤±è´¥ï¼Œä½¿ç”¨é™çº§Prompt: {e}")
            user_input = context_data.get("user_input", "")
            return f"{self.system_prompt}\n\nè¯·æ ¹æ®ä»¥ä¸‹å†…å®¹ç”ŸæˆçŸ­å‰§ä¸“ä¸šè¯„ä¼°æŠ¥å‘Šï¼š\n\n{user_input}"
    
    def extract_scores_from_response(self, response_text: str) -> Dict[str, float]:
        """ä»å“åº”ä¸­æå–è¯„åˆ†"""
        scores = {}
        
        # åŒ¹é…è¯„åˆ†æ¨¡å¼
        score_pattern = r'(\w+)[ï¼š:]\s*è¯„åˆ†[ï¼š:]\s*([0-9.]+)'
        matches = re.findall(score_pattern, response_text)
        
        for dimension, score_str in matches:
            try:
                score = float(score_str)
                if 0 <= score <= 10:
                    scores[dimension] = score
            except ValueError:
                continue
        
        return scores
    
    def calculate_overall_score(self, scores: Dict[str, float]) -> float:
        """è®¡ç®—ç»¼åˆè¯„åˆ†"""
        if not scores:
            return 0.0
        
        # å„ç»´åº¦æƒé‡
        weights = {
            "æ ¸å¿ƒçˆ½ç‚¹": 0.3,
            "æ•…äº‹ç±»å‹": 0.2,
            "äººç‰©è®¾å®š": 0.2,
            "äººç‰©å…³ç³»": 0.15,
            "æƒ…èŠ‚æ¡¥æ®µ": 0.15
        }
        
        weighted_sum = 0.0
        total_weight = 0.0
        
        for dimension, score in scores.items():
            weight = weights.get(dimension, 0.1)
            weighted_sum += score * weight
            total_weight += weight
        
        return weighted_sum / total_weight if total_weight > 0 else 0.0
    
    def get_score_level(self, score: float) -> str:
        """è·å–è¯„åˆ†ç­‰çº§"""
        if score >= 8.5:
            return "excellent"
        elif score >= 8.0:
            return "potential"
        elif score >= 7.5:
            return "average"
        else:
            return "poor"

    # ==================== ğŸ†• ç»“æ„åŒ–è¯„æµ‹æ–¹æ³• ====================

    async def evaluate_content(
        self,
        content: str,
        evaluation_type: str = "full",
        target_audience: str = "å¤§ä¼—",
        metadata: Dict[str, Any] = None
    ) -> EvaluationResult:
        """
        è¯„æµ‹å‰§æœ¬å†…å®¹ï¼ˆç»“æ„åŒ–è¯„åˆ†ï¼‰

        Args:
            content: å‰§æœ¬å†…å®¹
            evaluation_type: è¯„æµ‹ç±»å‹ (full=å…¨é¢, quick=å¿«é€Ÿ)
            target_audience: ç›®æ ‡å—ä¼—
            metadata: å…ƒæ•°æ®

        Returns:
            EvaluationResult: åŒ…å« scores, reasons, suggestions çš„è¯„æµ‹ç»“æœ
        """
        try:
            # æ„å»ºç»“æ„åŒ–è¯„æµ‹æç¤ºè¯
            eval_prompt = self._build_structured_evaluation_prompt(
                content=content,
                evaluation_type=evaluation_type,
                target_audience=target_audience
            )

            messages = [
                {"role": "system", "content": self.system_prompt},
                {"role": "user", "content": eval_prompt}
            ]

            # è°ƒç”¨LLMè¿›è¡Œè¯„æµ‹
            response = await self._call_llm(messages, user_id="system", session_id="evaluation")

            # è§£æè¯„æµ‹ç»“æœ
            result = self._parse_structured_evaluation_response(response, metadata or {})

            # ä¿å­˜è¯„æµ‹ç»“æœ
            await self._save_evaluation_result(result)

            return result

        except Exception as e:
            self.logger.error(f"ç»“æ„åŒ–è¯„æµ‹å¤±è´¥: {e}")
            # è¿”å›é»˜è®¤è¯„æµ‹ç»“æœ
            return self._create_default_evaluation(content)

    def _build_structured_evaluation_prompt(
        self,
        content: str,
        evaluation_type: str,
        target_audience: str
    ) -> str:
        """æ„å»ºç»“æ„åŒ–è¯„æµ‹æç¤ºè¯"""
        categories_desc = "\n".join([
            f"- **logic (é€»è¾‘æ€§)**: æƒ…èŠ‚å‘å±•æ˜¯å¦åˆç†ã€å‰åæ˜¯å¦è‡ªæ´½",
            f"- **character (äººè®¾ä¸€è‡´æ€§)**: è§’è‰²æ€§æ ¼ã€è¡Œä¸ºæ˜¯å¦ç¬¦åˆè®¾å®š",
            f"- **hook (çˆ†ç‚¹è®¾è®¡)**: å¼€ç¯‡æ˜¯å¦æœ‰å¸å¼•åŠ›ã€æ˜¯å¦æœ‰æƒ…ç»ªçˆ†ç‚¹",
            f"- **dialogue (å¯¹è¯è´¨é‡)**: å¯¹ç™½æ˜¯å¦ç”ŸåŠ¨ã€ç¬¦åˆè§’è‰²ç‰¹ç‚¹",
            f"- **pacing (èŠ‚å¥æŠŠæ§)**: èŠ‚å¥æ˜¯å¦ç´§å‡‘ã€æ˜¯å¦æœ‰æ‹–æ²“",
            f"- **emotion (æƒ…æ„Ÿå¼ åŠ›)**: æƒ…æ„Ÿè¡¨è¾¾æ˜¯å¦åˆ°ä½ã€æ˜¯å¦æœ‰æ„ŸæŸ“åŠ›",
            f"- **creativity (åˆ›æ„æ–°é¢–æ€§)**: æ˜¯å¦æœ‰åˆ›æ–°ã€æ˜¯å¦é¿å…å¥—è·¯åŒ–",
            f"- **commercial (å•†ä¸šä»·å€¼)**: æ˜¯å¦ç¬¦åˆå¸‚åœºéœ€æ±‚ã€æ˜¯å¦æœ‰ä¼ æ’­æ½œåŠ›"
        ])

        if evaluation_type == "quick":
            quick_note = "\nã€å¿«é€Ÿè¯„æµ‹æ¨¡å¼ã€‘è¯·ç»™å‡ºç®€æ´ä½†å‡†ç¡®çš„è¯„åˆ†å’Œå»ºè®®ã€‚"
        else:
            quick_note = "\nã€å…¨é¢è¯„æµ‹æ¨¡å¼ã€‘è¯·ç»™å‡ºè¯¦ç»†çš„åˆ†æå’Œå…·ä½“çš„æ”¹è¿›å»ºè®®ã€‚"

        prompt = f"""
ä½œä¸ºèµ„æ·±å‰§æœ¬ç›‘åˆ¶ï¼Œè¯·å¯¹ä»¥ä¸‹çŸ­å‰§å‰§æœ¬è¿›è¡Œä¸“ä¸šè¯„æµ‹ï¼š

ã€ç›®æ ‡å—ä¼—ã€‘{target_audience}

ã€è¯„æµ‹ç»´åº¦ã€‘
{categories_desc}

ã€å¾…è¯„æµ‹å‰§æœ¬ã€‘
{content}

{quick_note}

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›è¯„æµ‹ç»“æœï¼š
{{
  "scores": {{
    "logic": è¯„åˆ†(1-10),
    "character": è¯„åˆ†(1-10),
    "hook": è¯„åˆ†(1-10),
    "dialogue": è¯„åˆ†(1-10),
    "pacing": è¯„åˆ†(1-10),
    "emotion": è¯„åˆ†(1-10),
    "creativity": è¯„åˆ†(1-10),
    "commercial": è¯„åˆ†(1-10)
  }},
  "reasons": {{
    "logic": "é€»è¾‘æ€§è¯„åˆ†ç†ç”±",
    "character": "äººè®¾ä¸€è‡´æ€§è¯„åˆ†ç†ç”±",
    "hook": "çˆ†ç‚¹è®¾è®¡è¯„åˆ†ç†ç”±",
    "dialogue": "å¯¹è¯è´¨é‡è¯„åˆ†ç†ç”±",
    "pacing": "èŠ‚å¥æŠŠæ§è¯„åˆ†ç†ç”±",
    "emotion": "æƒ…æ„Ÿå¼ åŠ›è¯„åˆ†ç†ç”±",
    "creativity": "åˆ›æ„æ–°é¢–æ€§è¯„åˆ†ç†ç”±",
    "commercial": "å•†ä¸šä»·å€¼è¯„åˆ†ç†ç”±"
  }},
  "overall_score": æ€»ä½“è¯„åˆ†(1-10, å¯ä¸ºå°æ•°),
  "overall_reason": "æ€»ä½“è¯„ä»·ï¼ˆ200å­—ä»¥å†…ï¼‰",
  "strengths": ["ä¼˜ç‚¹1", "ä¼˜ç‚¹2", "ä¼˜ç‚¹3"],
  "weaknesses": ["ä¸è¶³1", "ä¸è¶³2", "ä¸è¶³3"],
  "suggestions": ["å»ºè®®1", "å»ºè®®2", "å»ºè®®3"],
  "commercial_potential": å•†ä¸šæ½œåŠ›è¯„åˆ†(1-10),
  "target_audience_match": å—ä¼—åŒ¹é…åº¦è¯„åˆ†(1-10)
}}

è¯·åªè¿”å›JSONï¼Œä¸è¦åŒ…å«å…¶ä»–è¯´æ˜æ–‡å­—ã€‚
"""
        return prompt

    def _parse_structured_evaluation_response(
        self,
        response: str,
        metadata: Dict[str, Any]
    ) -> EvaluationResult:
        """è§£æç»“æ„åŒ–è¯„æµ‹å“åº”"""
        try:
            # å°è¯•æå–JSON
            response = response.strip()
            if "```json" in response:
                response = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                response = response.split("```")[1].split("```")[0].strip()

            data = json.loads(response)

            # è®¡ç®—æ€»ä½“è¯„åˆ†ï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if "overall_score" not in data:
                scores = data.get("scores", {})
                weights = {
                    "logic": 1.0,
                    "character": 1.2,
                    "hook": 1.5,
                    "dialogue": 1.0,
                    "pacing": 1.1,
                    "emotion": 1.2,
                    "creativity": 0.8,
                    "commercial": 1.3
                }
                overall_score = sum(
                    scores.get(cat, 5) * weights.get(cat, 1.0)
                    for cat in weights.keys()
                ) / sum(weights.values())
                data["overall_score"] = round(overall_score, 1)

            result = EvaluationResult(
                overall_score=data.get("overall_score", 0.0),
                scores=data.get("scores", {}),
                reasons=data.get("reasons", {}),
                overall_reason=data.get("overall_reason", ""),
                suggestions=data.get("suggestions", []),
                strengths=data.get("strengths", []),
                weaknesses=data.get("weaknesses", []),
                commercial_potential=data.get("commercial_potential", 0.0),
                target_audience_match=data.get("target_audience_match", 0.0),
                metadata=metadata
            )

            self.logger.info(f"âœ… ç»“æ„åŒ–è¯„æµ‹å®Œæˆ: æ€»ä½“è¯„åˆ† {result.overall_score}")
            return result

        except json.JSONDecodeError as e:
            self.logger.error(f"JSONè§£æå¤±è´¥: {e}")
            return self._create_default_evaluation("")
        except Exception as e:
            self.logger.error(f"è§£æè¯„æµ‹å“åº”å¤±è´¥: {e}")
            return self._create_default_evaluation("")

    def _create_default_evaluation(self, content: str) -> EvaluationResult:
        """åˆ›å»ºé»˜è®¤è¯„æµ‹ç»“æœ"""
        return EvaluationResult(
            overall_score=5.0,
            scores={cat.value: 5.0 for cat in ScoreCategory},
            reasons={cat.value: "æœªèƒ½å®Œæˆè¯„æµ‹" for cat in ScoreCategory},
            overall_reason="è¯„æµ‹ç³»ç»Ÿå‡ºç°é”™è¯¯ï¼Œæ— æ³•ç»™å‡ºå‡†ç¡®è¯„åˆ†",
            suggestions=["è¯·ç¨åé‡è¯•", "æ£€æŸ¥å‰§æœ¬æ ¼å¼æ˜¯å¦æ­£ç¡®"],
            strengths=[],
            weaknesses=[],
            commercial_potential=5.0,
            target_audience_match=5.0,
            metadata={"error": "default_evaluation"}
        )

    # ==================== ğŸ†• å¯¹æ¯”è¯„æµ‹æ–¹æ³• ====================

    async def evaluate_comparison(
        self,
        content_a: str,
        content_b: str,
        target_audience: str = "å¤§ä¼—"
    ) -> ComparisonResult:
        """
        å¯¹æ¯”è¯„æµ‹ä¸¤ä¸ªç‰ˆæœ¬

        Args:
            content_a: ç‰ˆæœ¬Aå†…å®¹
            content_b: ç‰ˆæœ¬Bå†…å®¹
            target_audience: ç›®æ ‡å—ä¼—

        Returns:
            ComparisonResult: å¯¹æ¯”ç»“æœ
        """
        try:
            self.logger.info("ğŸ”„ å¼€å§‹å¯¹æ¯”è¯„æµ‹...")

            # å¹¶å‘è¯„æµ‹ä¸¤ä¸ªç‰ˆæœ¬
            result_a, result_b = await asyncio.gather(
                self.evaluate_content(content_a, "quick", target_audience, {"version": "A"}),
                self.evaluate_content(content_b, "quick", target_audience, {"version": "B"})
            )

            # è®¡ç®—åˆ†å·®
            score_delta = {}
            for category in result_a.scores.keys():
                if category in result_b.scores:
                    score_delta[category] = result_b.scores[category] - result_a.scores[category]

            overall_delta = result_b.overall_score - result_a.overall_score

            # åˆ¤æ–­èƒœè€…
            if overall_delta > 0.5:
                winner = "B"
            elif overall_delta < -0.5:
                winner = "A"
            else:
                winner = "TIE"

            # ç”Ÿæˆå¯¹æ¯”æ€»ç»“
            comparison_summary = await self._generate_comparison_summary(
                result_a, result_b, score_delta, overall_delta, winner
            )

            # ç”Ÿæˆæ¨è
            recommendation = self._generate_recommendation(
                result_a, result_b, winner, overall_delta
            )

            comparison_result = ComparisonResult(
                version_a=result_a.to_dict(),
                version_b=result_b.to_dict(),
                winner=winner,
                score_delta=score_delta,
                overall_delta=overall_delta,
                comparison_summary=comparison_summary,
                recommendation=recommendation
            )

            # ä¿å­˜å¯¹æ¯”ç»“æœ
            await self._save_comparison_result(comparison_result)

            return comparison_result

        except Exception as e:
            self.logger.error(f"å¯¹æ¯”è¯„æµ‹å¤±è´¥: {e}")
            raise

    async def _generate_comparison_summary(
        self,
        result_a: EvaluationResult,
        result_b: EvaluationResult,
        score_delta: Dict[str, float],
        overall_delta: float,
        winner: str
    ) -> str:
        """ç”Ÿæˆå¯¹æ¯”æ€»ç»“"""
        # æ‰¾å‡ºæœ€å¤§ä¼˜åŠ¿å’Œæœ€å¤§åŠ£åŠ¿çš„å·®å¼‚
        max_advantage = max(score_delta.items(), key=lambda x: x[1], default=("N/A", 0))
        max_disadvantage = min(score_delta.items(), key=lambda x: x[1], default=("N/A", 0))

        summary_parts = [
            f"## å¯¹æ¯”è¯„æµ‹æ€»ç»“",
            f"",
            f"**æ€»ä½“è¯„åˆ†**: ç‰ˆæœ¬A {result_a.overall_score:.1f}åˆ† vs ç‰ˆæœ¬B {result_b.overall_score:.1f}åˆ†",
            f"**åˆ†å·®**: {abs(overall_delta):.1f}åˆ†",
            f"**èƒœå‡º**: ç‰ˆæœ¬{winner if winner != 'TIE' else 'å¹³å±€'}",
            f"",
            f"**ä¸»è¦å·®å¼‚**:"
        ]

        category_names = {
            "logic": "é€»è¾‘æ€§",
            "character": "äººè®¾ä¸€è‡´æ€§",
            "hook": "çˆ†ç‚¹è®¾è®¡",
            "dialogue": "å¯¹è¯è´¨é‡",
            "pacing": "èŠ‚å¥æŠŠæ§",
            "emotion": "æƒ…æ„Ÿå¼ åŠ›",
            "creativity": "åˆ›æ„æ–°é¢–æ€§",
            "commercial": "å•†ä¸šä»·å€¼"
        }

        if max_advantage[0] != "N/A" and max_advantage[1] > 1:
            summary_parts.append(f"- ç‰ˆæœ¬B åœ¨ã€Œ{category_names.get(max_advantage[0], max_advantage[0])}ã€ä¸Šé¢†å…ˆ {max_advantage[1]:.1f} åˆ†")

        if max_disadvantage[0] != "N/A" and max_disadvantage[1] < -1:
            summary_parts.append(f"- ç‰ˆæœ¬A åœ¨ã€Œ{category_names.get(max_disadvantage[0], max_disadvantage[0])}ã€ä¸Šé¢†å…ˆ {abs(max_disadvantage[1]):.1f} åˆ†")

        summary_parts.append(f"")
        summary_parts.append(f"**ç‰ˆæœ¬Aç‰¹ç‚¹**: {', '.join(result_a.strengths[:3]) if result_a.strengths else 'æ— æ˜æ˜¾ä¼˜åŠ¿'}")
        summary_parts.append(f"**ç‰ˆæœ¬Bç‰¹ç‚¹**: {', '.join(result_b.strengths[:3]) if result_b.strengths else 'æ— æ˜æ˜¾ç‰¹ç‚¹'}")

        return "\n".join(summary_parts)

    def _generate_recommendation(
        self,
        result_a: EvaluationResult,
        result_b: EvaluationResult,
        winner: str,
        overall_delta: float
    ) -> str:
        """ç”Ÿæˆæ¨èæ„è§"""
        if winner == "A":
            base = f"æ¨èä½¿ç”¨ç‰ˆæœ¬Aã€‚"
            if overall_delta > 2:
                base += f" ç‰ˆæœ¬Aåœ¨å„æ–¹é¢è¡¨ç°æ˜¾è‘—ä¼˜äºç‰ˆæœ¬Bï¼ˆé¢†å…ˆ{overall_delta:.1f}åˆ†ï¼‰ã€‚"
            else:
                base += f" ç‰ˆæœ¬Aç•¥ä¼˜äºç‰ˆæœ¬Bï¼Œä½†å·®å¼‚ä¸å¤§ï¼ˆé¢†å…ˆ{overall_delta:.1f}åˆ†ï¼‰ã€‚"

            if result_a.suggestions:
                base += f"\n\nå»ºè®®ä¼˜åŒ–ï¼š{result_a.suggestions[0]}"

        elif winner == "B":
            base = f"æ¨èä½¿ç”¨ç‰ˆæœ¬Bã€‚"
            if overall_delta > 2:
                base += f" ç‰ˆæœ¬Båœ¨å„æ–¹é¢è¡¨ç°æ˜¾è‘—ä¼˜äºç‰ˆæœ¬Aï¼ˆé¢†å…ˆ{overall_delta:.1f}åˆ†ï¼‰ã€‚"
            else:
                base += f" ç‰ˆæœ¬Bç•¥ä¼˜äºç‰ˆæœ¬Aï¼Œä½†å·®å¼‚ä¸å¤§ï¼ˆé¢†å…ˆ{overall_delta:.1f}åˆ†ï¼‰ã€‚"

            if result_b.suggestions:
                base += f"\n\nå»ºè®®ä¼˜åŒ–ï¼š{result_b.suggestions[0]}"
        else:
            base = "ä¸¤ä¸ªç‰ˆæœ¬è¯„åˆ†ç›¸è¿‘ï¼Œå»ºè®®æ ¹æ®å…·ä½“éœ€æ±‚é€‰æ‹©ï¼š"
            base += f"\n\n- ç‰ˆæœ¬Aä¼˜åŠ¿ï¼š{', '.join(result_a.strengths[:2]) if result_a.strengths else 'æ— æ˜æ˜¾ç‰¹ç‚¹'}"
            base += f"\n- ç‰ˆæœ¬Bä¼˜åŠ¿ï¼š{', '.join(result_b.strengths[:2]) if result_b.strengths else 'æ— æ˜æ˜¾ç‰¹ç‚¹'}"

        return base

    # ==================== ğŸ†• æŒä¹…åŒ–æ–¹æ³• ====================

    async def _save_evaluation_result(self, result: EvaluationResult) -> bool:
        """ä¿å­˜è¯„æµ‹ç»“æœåˆ° ProjectFile"""
        try:
            from utils.storage_manager import get_project_file_manager
            import uuid

            file_manager = get_project_file_manager()

            # ç”Ÿæˆè¯„æµ‹è®°å½•ID
            evaluation_id = f"eval_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}"

            # ä¿å­˜è¯„æµ‹æ•°æ®
            evaluation_data = {
                "evaluation_id": evaluation_id,
                "result": result.to_dict(),
                "created_at": datetime.now().isoformat()
            }

            # ä½¿ç”¨ ProjectFile ä¿å­˜
            file_id = await file_manager.save_project_file(
                project_id="evaluation_history",
                file_type="evaluation",
                content=json.dumps(evaluation_data, ensure_ascii=False, indent=2),
                filename=f"{evaluation_id}.json",
                metadata={"evaluation_id": evaluation_id}
            )

            self.logger.info(f"ğŸ’¾ è¯„æµ‹ç»“æœå·²ä¿å­˜: {evaluation_id} ({file_id})")
            return True

        except Exception as e:
            self.logger.warning(f"ä¿å­˜è¯„æµ‹ç»“æœå¤±è´¥: {e}")
            return False

    async def _save_comparison_result(self, result: ComparisonResult) -> bool:
        """ä¿å­˜å¯¹æ¯”è¯„æµ‹ç»“æœåˆ° ProjectFile"""
        try:
            from utils.storage_manager import get_project_file_manager
            import uuid

            file_manager = get_project_file_manager()

            # ç”Ÿæˆå¯¹æ¯”è¯„æµ‹è®°å½•ID
            comparison_id = f"compare_{datetime.now().strftime('%Y%m%d_%H%M%S')}_{str(uuid.uuid4())[:8]}"

            # ä¿å­˜å¯¹æ¯”æ•°æ®
            comparison_data = {
                "comparison_id": comparison_id,
                "result": result.to_dict(),
                "created_at": datetime.now().isoformat()
            }

            # ä½¿ç”¨ ProjectFile ä¿å­˜
            file_id = await file_manager.save_project_file(
                project_id="comparison_history",
                file_type="comparison",
                content=json.dumps(comparison_data, ensure_ascii=False, indent=2),
                filename=f"{comparison_id}.json",
                metadata={"comparison_id": comparison_id}
            )

            self.logger.info(f"ğŸ’¾ å¯¹æ¯”è¯„æµ‹ç»“æœå·²ä¿å­˜: {comparison_id} ({file_id})")
            return True

        except Exception as e:
            self.logger.warning(f"ä¿å­˜å¯¹æ¯”è¯„æµ‹ç»“æœå¤±è´¥: {e}")
            return False

    async def get_evaluation_history(
        self,
        limit: int = 10
    ) -> List[Dict[str, Any]]:
        """
        è·å–è¯„æµ‹å†å²è®°å½•

        Args:
            limit: è¿”å›è®°å½•æ•°é‡

        Returns:
            List[Dict]: è¯„æµ‹å†å²åˆ—è¡¨
        """
        try:
            from utils.storage_manager import get_project_file_manager
            file_manager = get_project_file_manager()

            # è·å–è¯„æµ‹å†å²æ–‡ä»¶
            files = await file_manager.list_project_files("evaluation_history", limit=limit)

            history = []
            for file_info in files:
                content = await file_manager.get_project_file_content(file_info["file_id"])
                if content:
                    try:
                        data = json.loads(content)
                        history.append(data)
                    except json.JSONDecodeError:
                        continue

            return sorted(history, key=lambda x: x.get("created_at", ""), reverse=True)

        except Exception as e:
            self.logger.error(f"è·å–è¯„æµ‹å†å²å¤±è´¥: {e}")
            return []

    # ==================== ğŸ†• ä¾¿æ·æ–¹æ³• ====================

    async def quick_eval(self, content: str) -> float:
        """å¿«é€Ÿè¯„æµ‹ï¼Œè¿”å›æ€»ä½“è¯„åˆ†"""
        result = await self.evaluate_content(content, "quick")
        return result.overall_score

    async def batch_eval(self, contents: List[str]) -> List[EvaluationResult]:
        """æ‰¹é‡è¯„æµ‹å¤šä¸ªå‰§æœ¬"""
        tasks = [
            self.evaluate_content(content, "quick")
            for content in contents
        ]
        return await asyncio.gather(*tasks)

    def get_agent_info(self) -> Dict[str, Any]:
        """è·å–Agentä¿¡æ¯"""
        base_info = super().get_agent_info()
        base_info.update({
            "agent_type": "short_drama_evaluation",
            "description": "ç«–å±çŸ­å‰§è¯„ä¼°ä¸“å®¶ï¼Œä¸“æ³¨äºæ•…äº‹æ–‡æœ¬è¯„ä¼°ã€æ‰“åˆ†å’Œå¸‚åœºåˆ†æ",
            "role": "èµ„æ·±å‰§æœ¬ç›‘åˆ¶",
            "capabilities": [
                "æ•…äº‹æ–‡æœ¬æ·±åº¦è¯„ä¼°ä¸æ‰“åˆ†",
                "å¤šç»´åº¦ä¸“ä¸šåˆ†æï¼ˆæ ¸å¿ƒçˆ½ç‚¹ã€æ•…äº‹ç±»å‹ã€äººç‰©è®¾å®šç­‰ï¼‰",
                "å¸‚åœºç«äº‰åŠ›åˆ†æ",
                "å¼€å‘ä»·å€¼è¯„ä¼°",
                "ä¼˜åŒ–å»ºè®®æä¾›",
                "æ–‡ä»¶å†…å®¹æå–å’Œå¤„ç†"
            ],
            "evaluation_dimensions": list(self.evaluation_dimensions.values()),
            "scoring_criteria": self.scoring_criteria,
            "supported_evaluation_types": ["full", "quick", "compare"],
            "features": {
                "structured_evaluation": True,
                "comparison_evaluation": True,
                "persistence": True
            }
        })
        return base_info


# ==================== å…¨å±€å®ä¾‹ ====================

_evaluation_agent: Optional[ShortDramaEvaluationAgent] = None


def get_evaluation_agent() -> ShortDramaEvaluationAgent:
    """è·å–è¯„æµ‹ Agent å•ä¾‹"""
    global _evaluation_agent
    if _evaluation_agent is None:
        _evaluation_agent = ShortDramaEvaluationAgent()
    return _evaluation_agent


# å‘åå…¼å®¹çš„å…¨å±€å®ä¾‹
evaluation_agent: ShortDramaEvaluationAgent = get_evaluation_agent()


# ==================== ä¾¿æ·å‡½æ•° ====================

async def evaluate_drama(content: str, evaluation_type: str = "full") -> EvaluationResult:
    """è¯„æµ‹çŸ­å‰§å‰§æœ¬"""
    agent = get_evaluation_agent()
    return await agent.evaluate_content(content, evaluation_type)


async def compare_dramas(content_a: str, content_b: str) -> ComparisonResult:
    """å¯¹æ¯”è¯„æµ‹ä¸¤ä¸ªçŸ­å‰§å‰§æœ¬"""
    agent = get_evaluation_agent()
    return await agent.evaluate_comparison(content_a, content_b)
