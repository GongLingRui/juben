from typing import AsyncGenerator, Dict, Any, Optional, List
import json
import re

"""
æ•…äº‹äº”å…ƒç´ å·¥ä½œæµ - å¤§æƒ…èŠ‚ç‚¹åˆ†ææ™ºèƒ½ä½“
 ä¸“é—¨ç”¨äºåˆ†ææ•…äº‹ä¸­çš„ä¸»è¦æƒ…èŠ‚ç‚¹
ä½œä¸ºæ•…äº‹äº”å…ƒç´ åˆ†æç³»ç»Ÿçš„ä¸“ä¸šå­æ™ºèƒ½ä½“ä¹‹ä¸€

ä¸šåŠ¡å¤„ç†é€»è¾‘ï¼š
1. è¾“å…¥å¤„ç†ï¼šæ¥æ”¶æ•…äº‹æ–‡æœ¬æˆ–inputå­—æ®µï¼Œæ”¯æŒé•¿æ–‡æœ¬å¤„ç†
2. ç»“æ„åˆ†æï¼šæ·±å…¥åˆ†ææ•…äº‹æ–‡æœ¬å†…å®¹ï¼Œæ¢³ç†ä¸»è¦è„‰ç»œå’Œæ•…äº‹ç»“æ„
3. æƒ…èŠ‚ç‚¹æå–ï¼šè¯†åˆ«å’Œæå–æ•…äº‹ä¸­çš„å…³é”®æƒ…èŠ‚ç‚¹
4. æƒ…èŠ‚ç‚¹æ€»ç»“ï¼šä¸ºæ¯ä¸ªæƒ…èŠ‚ç‚¹ç”Ÿæˆè¯¦ç»†æè¿°ï¼ˆæ¯ä¸ªä¸è¶…è¿‡150å­—ï¼‰
5. é˜¶æ®µæ’åˆ—ï¼šæŒ‰å‘å±•é˜¶æ®µï¼ˆé˜¶æ®µä¸€åˆ°é˜¶æ®µå››ï¼‰æ’åˆ—æƒ…èŠ‚ç‚¹
6. è´¨é‡æ§åˆ¶ï¼šé¿å…å¹»è§‰ï¼Œä¸¥æ ¼æŒ‰ç…§åŸæ–‡åˆ†æï¼Œæ— é—æ¼
7. è¾“å‡ºæ ¼å¼åŒ–ï¼šè¿”å›ç»“æ„åŒ–çš„å¤§æƒ…èŠ‚ç‚¹åˆ†ææ•°æ®
8. Agent as Toolï¼šæ”¯æŒè¢«å…¶ä»–æ™ºèƒ½ä½“è°ƒç”¨ï¼Œä¸Šä¸‹æ–‡éš”ç¦»

ä»£ç ä½œè€…ï¼šå®«çµç‘
åˆ›å»ºæ—¶é—´ï¼š2024å¹´10æœˆ19æ—¥
"""

try:
    from .base_juben_agent import BaseJubenAgent
except ImportError:
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from agents.base_juben_agent import BaseJubenAgent


class PlotPointsAnalyzerAgent(BaseJubenAgent):
    """å¤§æƒ…èŠ‚ç‚¹åˆ†ææ™ºèƒ½ä½“ç±»"""

    def __init__(self, model_provider: str = "zhipu"):
        """åˆå§‹åŒ–æƒ…èŠ‚ç‚¹åˆ†ææ™ºèƒ½ä½“"""
        super().__init__(
            agent_name="plot_points_analyzer_agent",
            model_provider=model_provider
        )

    async def process_request(
        self,
        request_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¤„ç†æƒ…èŠ‚ç‚¹åˆ†æè¯·æ±‚ï¼ˆä¸»å…¥å£ï¼‰

        Args:
            request_data: è¯·æ±‚æ•°æ®
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Yields:
            Dict: æµå¼å“åº”äº‹ä»¶
        """
        async for event in self.process_plot_analysis(request_data, context):
            yield event

    async def process_plot_analysis(
        self,
        request_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[Dict, None]:
        """
        å¤„ç†å¤§æƒ…èŠ‚ç‚¹åˆ†æè¯·æ±‚

        Args:
            request_data: è¯·æ±‚æ•°æ®
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Yields:
            Dict: æµå¼å“åº”äº‹ä»¶
        """
        # æå–è¯·æ±‚ä¿¡æ¯
        input_text = request_data.get("input", "")
        user_id = context.get("user_id", "unknown") if context else "unknown"
        session_id = context.get("session_id", "unknown") if context else "unknown"
        parent_agent = context.get("parent_agent", "") if context else ""
        tool_call = context.get("tool_call", False) if context else False

        if tool_call:
            self.logger.info(f"ğŸ”§ Agent as Toolæ¨¡å¼ï¼Œçˆ¶æ™ºèƒ½ä½“: {parent_agent}")

        # åˆå§‹åŒ–Tokenç´¯åŠ å™¨
        await self.initialize_token_accumulator(user_id, session_id)

        # å‘é€å¼€å§‹äº‹ä»¶
        yield await self.emit_juben_event(
            "analysis_start",
            "å¼€å§‹åˆ†æå¤§æƒ…èŠ‚ç‚¹...",
            {"stage": "init"}
        )

        try:
            # ç¬¬ä¸€æ­¥ï¼šé¢„å¤„ç†æ–‡æœ¬
            yield await self.emit_juben_event(
                "preprocessing",
                "æ­£åœ¨é¢„å¤„ç†æ–‡æœ¬...",
                {"stage": "preprocessing"}
            )

            processed_text = await self._preprocess_text(input_text)

            # ç¬¬äºŒæ­¥ï¼šåˆ†ææ•…äº‹ç»“æ„
            yield await self.emit_juben_event(
                "analyzing_structure",
                "æ­£åœ¨åˆ†ææ•…äº‹ç»“æ„...",
                {"stage": "structure_analysis"}
            )

            structure_info = await self._analyze_story_structure(processed_text)

            # ç¬¬ä¸‰æ­¥ï¼šæå–æƒ…èŠ‚ç‚¹
            yield await self.emit_juben_event(
                "extracting_plot_points",
                "æ­£åœ¨æå–å…³é”®æƒ…èŠ‚ç‚¹...",
                {"stage": "extraction"}
            )

            plot_points = await self._extract_plot_points(processed_text, structure_info)

            # ç¬¬å››æ­¥ï¼šç»„ç»‡æƒ…èŠ‚ç‚¹
            yield await self.emit_juben_event(
                "organizing_plot_points",
                "æ­£åœ¨ç»„ç»‡æƒ…èŠ‚ç‚¹...",
                {"stage": "organization"}
            )

            organized_points = await self._organize_plot_points_by_stage(plot_points)

            # ç¬¬äº”æ­¥ï¼šç”Ÿæˆæ€»ç»“
            yield await self.emit_juben_event(
                "generating_summary",
                "æ­£åœ¨ç”Ÿæˆåˆ†ææ€»ç»“...",
                {"stage": "summary"}
            )

            summary = await self._generate_analysis_summary(organized_points)

            # ç¬¬å…­æ­¥ï¼šæ ¼å¼åŒ–è¾“å‡º
            yield await self.emit_juben_event(
                "formatting_output",
                "æ­£åœ¨æ ¼å¼åŒ–è¾“å‡º...",
                {"stage": "formatting"}
            )

            formatted_output = self._format_plot_points_output(organized_points, summary)

            # ä¿å­˜è¾“å‡º
            await self.auto_save_output(
                output_content=formatted_output,
                user_id=user_id,
                session_id=session_id,
                file_type="json"
            )

            # å‘é€å®Œæˆäº‹ä»¶
            yield await self.emit_juben_event(
                "analysis_complete",
                formatted_output,
                {
                    "stage": "complete",
                    "plot_points_count": len(organized_points.get("stages", [])),
                    "summary": summary
                }
            )

        except Exception as e:
            self.logger.error(f"æƒ…èŠ‚ç‚¹åˆ†æå¤±è´¥: {e}")
            yield await self.emit_juben_event(
                "analysis_error",
                f"åˆ†æå¤±è´¥: {str(e)}",
                {"stage": "error", "error": str(e)}
            )

    async def _preprocess_text(self, text: str) -> str:
        """
        é¢„å¤„ç†æ–‡æœ¬
        - æ¸…ç†å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        - è¯†åˆ«ç« èŠ‚/åœºæ™¯æ ‡è®°
        - ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        """
        # æ¸…ç†å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        # ä¿ç•™æ®µè½ç»“æ„
        text = re.sub(r'([ã€‚ï¼ï¼Ÿï¼›])\s+', r'\1\n', text)
        return text.strip()

    async def _analyze_story_structure(self, text: str) -> Dict[str, Any]:
        """
        åˆ†ææ•…äº‹ç»“æ„
        - è¯†åˆ«å¼€å¤´ã€å‘å±•ã€é«˜æ½®ã€ç»“å±€
        - æ£€æµ‹è½¬æŠ˜ç‚¹
        - è¯†åˆ«ä¸»è¦å†²çª
        """
        structure_prompt = f"""è¯·åˆ†æä»¥ä¸‹æ•…äº‹æ–‡æœ¬çš„ç»“æ„ï¼Œè¯†åˆ«ï¼š
1. å¼€å¤´éƒ¨åˆ†ï¼ˆèƒŒæ™¯ä»‹ç»ã€äººç‰©ç™»åœºï¼‰
2. å‘å±•éƒ¨åˆ†ï¼ˆå†²çªå»ºç«‹ã€æƒ…èŠ‚æ¨è¿›ï¼‰
3. é«˜æ½®éƒ¨åˆ†ï¼ˆå†²çªçˆ†å‘ã€å…³é”®è½¬æŠ˜ï¼‰
4. ç»“å±€éƒ¨åˆ†ï¼ˆé—®é¢˜è§£å†³ã€æ”¶å°¾ï¼‰

æ•…äº‹æ–‡æœ¬ï¼š
{text[:5000]}

è¯·ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœï¼š
{{
    "stages": {{
        "opening": "å¼€å¤´éƒ¨åˆ†çš„ç®€è¦æè¿°",
        "development": "å‘å±•éƒ¨åˆ†çš„ç®€è¦æè¿°",
        "climax": "é«˜æ½®éƒ¨åˆ†çš„ç®€è¦æè¿°",
        "resolution": "ç»“å±€éƒ¨åˆ†çš„ç®€è¦æè¿°"
    }},
    "key_conflicts": ["ä¸»è¦å†²çª1", "ä¸»è¦å†²çª2"],
    "turning_points": ["è½¬æŠ˜ç‚¹1", "è½¬æŠ˜ç‚¹2"]
}}"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ•…äº‹ç»“æ„åˆ†æä¸“å®¶ã€‚"},
            {"role": "user", "content": structure_prompt}
        ]

        try:
            response = await self._call_llm(messages, user_id="analysis", session_id="structure")
            # å°è¯•è§£æJSON
            json_match = re.search(r'\{[\s\S]*\}', response)
            if json_match:
                return json.loads(json_match.group())
        except Exception as e:
            self.logger.warning(f"ç»“æ„åˆ†æè§£æå¤±è´¥: {e}")

        # è¿”å›é»˜è®¤ç»“æ„
        return {
            "stages": {
                "opening": "æ•…äº‹å¼€å¤´",
                "development": "æ•…äº‹å‘å±•",
                "climax": "æ•…äº‹é«˜æ½®",
                "resolution": "æ•…äº‹ç»“å±€"
            },
            "key_conflicts": [],
            "turning_points": []
        }

    async def _extract_plot_points(self, text: str, structure_info: Dict[str, Any]) -> List[Dict[str, Any]]:
        """
        æå–æƒ…èŠ‚ç‚¹
        - è¯†åˆ«å…³é”®äº‹ä»¶
        - æå–é‡è¦è½¬æŠ˜
        - æ ‡è®°äººç‰©å†³ç­–ç‚¹
        """
        extraction_prompt = f"""è¯·ä»ä»¥ä¸‹æ•…äº‹æ–‡æœ¬ä¸­æå–å…³é”®æƒ…èŠ‚ç‚¹ã€‚

æ•…äº‹ç»“æ„ä¿¡æ¯ï¼š
{json.dumps(structure_info, ensure_ascii=False, indent=2)}

æ•…äº‹æ–‡æœ¬ï¼š
{text[:8000]}

è¯·æå–8-12ä¸ªå…³é”®æƒ…èŠ‚ç‚¹ï¼Œæ¯ä¸ªæƒ…èŠ‚ç‚¹åŒ…æ‹¬ï¼š
- é˜¶æ®µï¼ˆopening/development/climax/resolutionï¼‰
- æ ‡é¢˜ï¼ˆç®€çŸ­æè¿°ï¼‰
- æè¿°ï¼ˆä¸è¶…è¿‡150å­—çš„è¯¦ç»†è¯´æ˜ï¼‰
- é‡è¦æ€§è¯„åˆ†ï¼ˆ1-10ï¼‰

è¯·ä»¥JSONæ ¼å¼è¿”å›ï¼š
[
    {{
        "stage": "development",
        "title": "æƒ…èŠ‚ç‚¹æ ‡é¢˜",
        "description": "è¯¦ç»†æè¿°ï¼ˆä¸è¶…è¿‡150å­—ï¼‰",
        "importance": 8
    }}
]"""

        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æƒ…èŠ‚ç‚¹æå–ä¸“å®¶ã€‚"},
            {"role": "user", "content": extraction_prompt}
        ]

        try:
            response = await self._call_llm(messages, user_id="analysis", session_id="extraction")
            # å°è¯•è§£æJSONæ•°ç»„
            json_match = re.search(r'\[[\s\S]*\]', response)
            if json_match:
                plot_points = json.loads(json_match.group())
                return plot_points
        except Exception as e:
            self.logger.warning(f"æƒ…èŠ‚ç‚¹æå–è§£æå¤±è´¥: {e}")

        return []

    async def _organize_plot_points_by_stage(self, plot_points: List[Dict[str, Any]]) -> Dict[str, Any]:
        """
        æŒ‰é˜¶æ®µç»„ç»‡æƒ…èŠ‚ç‚¹
        - å°†æƒ…èŠ‚ç‚¹æŒ‰æ•…äº‹é˜¶æ®µåˆ†ç»„
        - ç¡®ä¿æ—¶é—´é¡ºåº
        - æ·»åŠ é˜¶æ®µç»Ÿè®¡
        """
        stages = {
            "é˜¶æ®µä¸€ï¼šå¼€ç«¯": {
                "code": "opening",
                "description": "æ•…äº‹èƒŒæ™¯ã€äººç‰©ä»‹ç»ã€åˆå§‹çŠ¶æ€",
                "points": []
            },
            "é˜¶æ®µäºŒï¼šå‘å±•": {
                "code": "development",
                "description": "å†²çªå»ºç«‹ã€æƒ…èŠ‚æ¨è¿›ã€çŸ›ç›¾å‡çº§",
                "points": []
            },
            "é˜¶æ®µä¸‰ï¼šé«˜æ½®": {
                "code": "climax",
                "description": "å†²çªçˆ†å‘ã€å…³é”®è½¬æŠ˜ã€å†³å®šæ€§æ—¶åˆ»",
                "points": []
            },
            "é˜¶æ®µå››ï¼šç»“å±€": {
                "code": "resolution",
                "description": "é—®é¢˜è§£å†³ã€æ”¶å°¾ã€æ–°çŠ¶æ€",
                "points": []
            }
        }

        # æŒ‰é‡è¦æ€§æ’åºå¹¶åˆ†ç»„
        sorted_points = sorted(plot_points, key=lambda x: x.get("importance", 5), reverse=True)

        for point in sorted_points:
            stage_code = point.get("stage", "development")
            for stage_name, stage_info in stages.items():
                if stage_info["code"] == stage_code:
                    stage_info["points"].append({
                        "title": point.get("title", ""),
                        "description": point.get("description", ""),
                        "importance": point.get("importance", 5)
                    })
                    break

        return {
            "stages": stages,
            "total_points": len(plot_points),
            "stage_distribution": {
                stage_name: len(stage_info["points"])
                for stage_name, stage_info in stages.items()
            }
        }

    async def _generate_analysis_summary(self, organized_points: Dict[str, Any]) -> str:
        """
        ç”Ÿæˆåˆ†ææ€»ç»“
        - æ¦‚æ‹¬æƒ…èŠ‚ç‚¹åˆ†å¸ƒ
        - è¯†åˆ«å…³é”®è½¬æŠ˜
        - è¯„ä¼°æ•…äº‹èŠ‚å¥
        """
        stages = organized_points.get("stages", {})
        distribution = organized_points.get("stage_distribution", {})

        summary_parts = []

        # é˜¶æ®µåˆ†å¸ƒ
        summary_parts.append("## æƒ…èŠ‚ç‚¹åˆ†å¸ƒ")
        for stage_name, count in distribution.items():
            if count > 0:
                summary_parts.append(f"- {stage_name}ï¼š{count}ä¸ªå…³é”®æƒ…èŠ‚ç‚¹")

        # å…³é”®æƒ…èŠ‚ç‚¹
        summary_parts.append("\n## å…³é”®æƒ…èŠ‚ç‚¹")
        for stage_name, stage_info in stages.items():
            top_points = sorted(
                stage_info["points"],
                key=lambda x: x.get("importance", 0),
                reverse=True
            )[:2]
            for point in top_points:
                summary_parts.append(f"- {point['title']}ï¼ˆé‡è¦æ€§ï¼š{point['importance']}/10ï¼‰")

        return "\n".join(summary_parts)

    def _format_plot_points_output(self, organized_points: Dict[str, Any], summary: str) -> Dict[str, Any]:
        """
        æ ¼å¼åŒ–æƒ…èŠ‚ç‚¹è¾“å‡º
        - ç»“æ„åŒ–æ•°æ®
        - Markdownæ ¼å¼
        - ç»Ÿè®¡ä¿¡æ¯
        """
        return {
            "analysis_type": "å¤§æƒ…èŠ‚ç‚¹åˆ†æ",
            "total_points": organized_points.get("total_points", 0),
            "stage_distribution": organized_points.get("stage_distribution", {}),
            "stages": organized_points.get("stages", {}),
            "summary": summary,
            "metadata": {
                "agent": "plot_points_analyzer_agent",
                "format_version": "1.0"
            }
        }