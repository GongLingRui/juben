"""
ç«–å±çŸ­å‰§ç½‘ç»œæ£€ç´¢æ™ºèƒ½ä½“
 ä¸“æ³¨äºç½‘ç»œæœç´¢å’Œä¿¡æ¯æ£€ç´¢

ä¸šåŠ¡å¤„ç†é€»è¾‘ï¼š
1. è¾“å…¥å¤„ç†ï¼šæ¥æ”¶æœç´¢æŸ¥è¯¢è¯·æ±‚ï¼Œæ”¯æŒå¤šç§æœç´¢æ ¼å¼
2. æ„å›¾è¯†åˆ«ï¼šä½¿ç”¨IntentRecognizeråˆ†ææœç´¢æ„å›¾å’Œå…³é”®è¯
3. URLæå–ï¼šä»æŸ¥è¯¢ä¸­æå–ç›¸å…³çš„URLé“¾æ¥ä¿¡æ¯
4. ç½‘ç»œæœç´¢ï¼šæ‰§è¡Œç½‘ç»œæœç´¢ï¼Œè·å–ç›¸å…³çš„ç½‘é¡µå†…å®¹
5. å†…å®¹è§£æï¼šè§£ææœç´¢ç»“æœï¼Œæå–æœ‰ç”¨çš„ä¿¡æ¯
6. ä¿¡æ¯è¿‡æ»¤ï¼šæ ¹æ®æœç´¢æ„å›¾è¿‡æ»¤å’Œæ’åºæœç´¢ç»“æœ
7. å†…å®¹ç”Ÿæˆï¼šåŸºäºæœç´¢ç»“æœç”Ÿæˆç»“æ„åŒ–çš„å›ç­”
8. ä¸Šä¸‹æ–‡ç®¡ç†ï¼šç»´æŠ¤æœç´¢å†å²å’Œä¸Šä¸‹æ–‡ä¿¡æ¯
9. è¾“å‡ºæ ¼å¼åŒ–ï¼šè¿”å›ç»“æ„åŒ–çš„ç½‘ç»œæœç´¢ç»“æœ
10. Agent as Toolï¼šæ”¯æŒè¢«å…¶ä»–æ™ºèƒ½ä½“è°ƒç”¨ï¼Œå®ç°ä¸Šä¸‹æ–‡éš”ç¦»

ä»£ç ä½œè€…ï¼šå®«çµç‘
åˆ›å»ºæ—¶é—´ï¼š2024å¹´10æœˆ19æ—¥
"""
import asyncio
import logging
import re
from typing import Dict, Any, List, Optional, AsyncGenerator
from datetime import datetime
import json

try:
    from .base_juben_agent import BaseJubenAgent
    from ..utils.intent_recognition import IntentRecognizer
    from ..utils.url_extractor import URLExtractor
except ImportError:
    # å¤„ç†ç›¸å¯¹å¯¼å…¥é—®é¢˜
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    
    from agents.base_juben_agent import BaseJubenAgent
    from utils.intent_recognition import IntentRecognizer
    from utils.url_extractor import URLExtractor


class WebSearchAgent(BaseJubenAgent):
    """
    ç«–å±çŸ­å‰§ç½‘ç»œæ£€ç´¢æ™ºèƒ½ä½“
    
    åŠŸèƒ½ï¼š
    1. æ™ºè°±AIç½‘ç»œæœç´¢
    2. æœç´¢ç»“æœæ™ºèƒ½æ€»ç»“
    3. å¤šé˜¶æ®µæœç´¢æµç¨‹
    4. æœç´¢ç»“æœæ ¼å¼åŒ–
    5. ä¿¡æ¯æå–å’Œæ•´ç†
    """
    
    def __init__(self):
        super().__init__("websearch", model_provider="zhipu")
        
        # ç³»ç»Ÿæç¤ºè¯é…ç½®ï¼ˆä»promptsæ–‡ä»¶å¤¹åŠ è½½ï¼‰
        self._load_system_prompt()
        
        # åˆå§‹åŒ–ä¸“ç”¨ç»„ä»¶
        self.intent_recognizer = IntentRecognizer()
        self.url_extractor = URLExtractor()
        
        # æœç´¢é…ç½®
        self.search_config = {
            "default_count": 5,
            "max_count": 10,
            "timeout": 30
        }
        
        self.logger.info("ç«–å±çŸ­å‰§ç½‘ç»œæ£€ç´¢æ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    async def process_request(
        self, 
        request_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¤„ç†ç½‘ç»œæœç´¢è¯·æ±‚
        
        Args:
            request_data: è¯·æ±‚æ•°æ®
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
            
        Yields:
            Dict: æµå¼å“åº”äº‹ä»¶
        """
        try:
            # æå–è¯·æ±‚ä¿¡æ¯
            query = request_data.get("query", request_data.get("input", ""))
            instruction = request_data.get("instruction", query)
            user_id = context.get("user_id", "unknown") if context else "unknown"
            session_id = context.get("session_id", "unknown") if context else "unknown"
            count = request_data.get("count", self.search_config["default_count"])
            
            self.logger.info(f"å¼€å§‹å¤„ç†ç½‘ç»œæœç´¢è¯·æ±‚: {instruction}")
            
            # åˆå§‹åŒ–Tokenç´¯åŠ å™¨
            await self.initialize_token_accumulator(user_id, session_id)
            
            # å‘é€å¼€å§‹å¤„ç†äº‹ä»¶
            yield await self._emit_event("system", f"ğŸ” å¼€å§‹ç½‘ç»œæœç´¢: {instruction}")
            
            # 1. æ„å›¾è¯†åˆ«
            yield await self._emit_event("system", "ğŸ” æ­£åœ¨åˆ†ææœç´¢æ„å›¾...")
            intent_result = await self._analyze_intent(instruction)
            yield await self._emit_event("system", f"âœ… æ„å›¾è¯†åˆ«å®Œæˆ: {intent_result['intent']}")
            
            # 2. URLæå–å’Œå†…å®¹è·å–
            urls = self.url_extractor.extract_urls(instruction)
            url_contents = []
            if urls:
                yield await self._emit_event("system", f"ğŸ“ å‘ç°{len(urls)}ä¸ªé“¾æ¥ï¼Œæ­£åœ¨æå–å†…å®¹...")
                url_contents = await self._extract_url_contents(urls)
                yield await self._emit_event("system", "âœ… URLå†…å®¹æå–å®Œæˆ")
            
            # 3. ç½‘ç»œæœç´¢
            yield await self._emit_event("system", "ğŸŒ æ­£åœ¨æœç´¢ç½‘ç»œä¿¡æ¯...")
            search_results = await self._execute_web_search(instruction, count=count)
            yield await self._emit_event("system", "âœ… ç½‘ç»œæœç´¢å®Œæˆ")
            
            # 4. æ ¼å¼åŒ–æœç´¢ç»“æœ
            formatted_results = self._format_search_results(search_results, instruction)
            
            # 5. å‘é€åŸå§‹æœç´¢ç»“æœ
            yield await self._emit_event("search_results", formatted_results)
            
            # 6. æ™ºèƒ½æ€»ç»“æœç´¢ç»“æœ
            if formatted_results:
                yield await self._emit_event("system", "ğŸ“ æ­£åœ¨åˆ†æå’Œæ€»ç»“æœç´¢ç»“æœ...")
                
                async for chunk in self._generate_search_summary(instruction, search_results, user_id, session_id):
                    yield chunk
                
                yield await self._emit_event("system", "âœ… æœç´¢ç»“æœæ€»ç»“å®Œæˆ")
            
            # 7. è·å–Tokenè®¡è´¹æ‘˜è¦
            billing_summary = await self.get_token_billing_summary()
            if billing_summary:
                yield await self._emit_event("billing", f"ğŸ“Š Tokenæ¶ˆè€—: {billing_summary['total_tokens']} tokens, ç§¯åˆ†æ‰£å‡: {billing_summary['deducted_points']} ç§¯åˆ†")
            
            # 8. å‘é€å®Œæˆäº‹ä»¶
            yield await self._emit_event("system", "ğŸ¯ ç½‘ç»œæœç´¢ä»»åŠ¡å®Œæˆï¼")
            
        except Exception as e:
            self.logger.error(f"å¤„ç†ç½‘ç»œæœç´¢è¯·æ±‚å¤±è´¥: {e}")
            yield await self._emit_event("error", f"å¤„ç†å¤±è´¥: {str(e)}")
    
    async def _analyze_intent(self, user_input: str) -> Dict[str, Any]:
        """åˆ†æç”¨æˆ·æœç´¢æ„å›¾"""
        try:
            # æœç´¢ç›¸å…³çš„æ„å›¾è¯†åˆ«
            intent_result = await self.intent_recognizer.analyze(user_input)
            
            # æ ¹æ®æœç´¢éœ€æ±‚è°ƒæ•´æ„å›¾
            if "æœç´¢" in user_input or "æŸ¥æ‰¾" in user_input or "å¯»æ‰¾" in user_input:
                intent_result.update({
                    "intent": "web_search",
                    "needs_web_search": True,
                    "needs_knowledge_base": False
                })
            elif "æ–°é—»" in user_input or "æœ€æ–°" in user_input or "èµ„è®¯" in user_input:
                intent_result.update({
                    "intent": "news_search",
                    "needs_web_search": True,
                    "needs_knowledge_base": False
                })
            elif "å¸‚åœº" in user_input or "è¶‹åŠ¿" in user_input or "åˆ†æ" in user_input:
                intent_result.update({
                    "intent": "market_search",
                    "needs_web_search": True,
                    "needs_knowledge_base": True
                })
            
            return intent_result
        except Exception as e:
            self.logger.error(f"æ„å›¾è¯†åˆ«å¤±è´¥: {e}")
            return {
                "intent": "web_search",
                "confidence": 0.5,
                "needs_web_search": True,
                "needs_knowledge_base": False
            }
    
    async def _extract_url_contents(
        self,
        urls: List[str],
        timeout: int = 15,
        max_retries: int = 2,
        parallel: bool = True
    ) -> List[Dict[str, Any]]:
        """
        æå–URLå†…å®¹ï¼ˆå¢å¼ºç‰ˆï¼šå¸¦è¶…æ—¶ã€é‡è¯•å’Œå¹¶è¡Œå¤„ç†ï¼‰

        Args:
            urls: URLåˆ—è¡¨
            timeout: å•ä¸ªURLè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
            max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
            parallel: æ˜¯å¦å¹¶è¡Œå¤„ç†

        Returns:
            List[Dict]: æå–ç»“æœåˆ—è¡¨
        """
        import asyncio

        if not urls:
            return []

        async def extract_single_url(url: str) -> Dict[str, Any]:
            """æå–å•ä¸ªURLçš„å†…å®¹"""
            for attempt in range(max_retries + 1):
                try:
                    self.logger.debug(f"æå–URLå†…å®¹(å°è¯•{attempt + 1}): {url}")

                    # ä½¿ç”¨asyncio.wait_forå®ç°è¶…æ—¶
                    async def do_extract():
                        return await self.url_extractor.extract_content(url)

                    result = await asyncio.wait_for(do_extract(), timeout=timeout)

                    # éªŒè¯è¿”å›ç»“æœ
                    if result is None:
                        raise ValueError("extract_contentè¿”å›None")

                    # ç¡®ä¿æœ‰urlå­—æ®µ
                    if isinstance(result, dict):
                        result["url"] = url
                        result["success"] = True
                    else:
                        result = {
                            "url": url,
                            "success": True,
                            "content": str(result)
                        }

                    self.logger.info(f"URLå†…å®¹æå–æˆåŠŸ: {url}")
                    return result

                except asyncio.TimeoutError:
                    if attempt < max_retries:
                        self.logger.warning(f"URLæå–è¶…æ—¶({timeout}s): {url}, é‡è¯•{attempt + 1}/{max_retries}")
                        await asyncio.sleep(0.5 * (attempt + 1))
                    else:
                        self.logger.error(f"URLæå–è¶…æ—¶({timeout}s): {url}, å·²è¾¾æœ€å¤§é‡è¯•æ¬¡æ•°")
                        return {
                            "url": url,
                            "success": False,
                            "error": f"æå–è¶…æ—¶({timeout}ç§’)"
                        }

                except ValueError as e:
                    self.logger.error(f"URLæå–å‚æ•°é”™è¯¯: {url}, {e}")
                    return {
                        "url": url,
                        "success": False,
                        "error": f"å‚æ•°é”™è¯¯: {str(e)}"
                    }

                except Exception as e:
                    if attempt < max_retries:
                        self.logger.warning(f"URLæå–å¤±è´¥(å°è¯•{attempt + 1}): {url}, {e}")
                        await asyncio.sleep(0.5 * (attempt + 1))
                    else:
                        self.logger.error(f"URLæå–æœ€ç»ˆå¤±è´¥: {url}, {e}")
                        return {
                            "url": url,
                            "success": False,
                            "error": str(e)
                        }

        # å¹¶è¡Œæˆ–ä¸²è¡Œå¤„ç†
        if parallel and len(urls) > 1:
            tasks = [extract_single_url(url) for url in urls]
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # å¤„ç†å¯èƒ½çš„å¼‚å¸¸
            contents = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"URLå¤„ç†å¼‚å¸¸: {urls[i]}, {result}")
                    contents.append({
                        "url": urls[i],
                        "success": False,
                        "error": f"å¤„ç†å¼‚å¸¸: {str(result)}"
                    })
                else:
                    contents.append(result)
        else:
            # ä¸²è¡Œå¤„ç†
            contents = []
            for url in urls:
                result = await extract_single_url(url)
                contents.append(result)

        # ç»Ÿè®¡ç»“æœ
        success_count = sum(1 for c in contents if c.get("success", False))
        self.logger.info(f"URLå†…å®¹æå–å®Œæˆ: æˆåŠŸ{success_count}/{len(urls)}")

        return contents
    
    async def _execute_web_search(self, query: str, count: int = 5) -> Dict[str, Any]:
        """æ‰§è¡Œç½‘ç»œæœç´¢"""
        try:
            self.logger.info(f"å¼€å§‹ç½‘ç»œæœç´¢: query={query}, count={count}")
            result = await self._search_web(query, count=count)
            self.logger.info(f"ç½‘ç»œæœç´¢å®Œæˆ: success={result.get('success', True)}")
            return result
        except Exception as e:
            self.logger.error(f"ç½‘ç»œæœç´¢å¤±è´¥: {e}")
            return {"success": False, "error": str(e)}
    
    def _format_search_results(self, search_results: Dict[str, Any], query: str) -> List[Dict[str, Any]]:
        """æ ¼å¼åŒ–æœç´¢ç»“æœ"""
        formatted_results = []
        
        if not search_results.get("success", True):
            return formatted_results
        
        # æ ¹æ®æœç´¢ç»“æœæ ¼å¼è¿›è¡Œå¤„ç†
        results_data = search_results.get("search_results", {})
        content = results_data.get("content", {})
        
        if isinstance(content, dict):
            search_result_list = content.get("search_result", [])
        else:
            # å¦‚æœæ˜¯å¯¹è±¡ï¼Œå°è¯•è·å–search_resultå±æ€§
            search_result_list = getattr(content, "search_result", [])
        
        for i, result in enumerate(search_result_list):
            if isinstance(result, dict):
                formatted_result = {
                    "id": f"search_result_{i+1}",
                    "type": "web_search",
                    "title": result.get("title", ""),
                    "content": result.get("content", ""),
                    "url": result.get("link", ""),
                    "publish_date": result.get("publish_date", ""),
                    "icon": result.get("icon", ""),
                    "media": result.get("media", "")
                }
            else:
                # å¦‚æœæ˜¯å¯¹è±¡ï¼Œä½¿ç”¨getattrè·å–å±æ€§
                formatted_result = {
                    "id": f"search_result_{i+1}",
                    "type": "web_search",
                    "title": getattr(result, "title", ""),
                    "content": getattr(result, "content", ""),
                    "url": getattr(result, "link", ""),
                    "publish_date": getattr(result, "publish_date", ""),
                    "icon": getattr(result, "icon", ""),
                    "media": getattr(result, "media", "")
                }
            
            formatted_results.append(formatted_result)
        
        return formatted_results
    
    def _extract_search_content(self, search_results: Dict[str, Any]) -> str:
        """ä»æœç´¢ç»“æœä¸­æå–æ–‡æœ¬å†…å®¹ç”¨äºæ€»ç»“"""
        content_list = []
        
        results_data = search_results.get("search_results", {})
        content = results_data.get("content", {})
        
        if isinstance(content, dict):
            search_result_list = content.get("search_result", [])
        else:
            search_result_list = getattr(content, "search_result", [])
        
        if not search_result_list:
            return "æ— æœç´¢ç»“æœ"
        
        for i, result in enumerate(search_result_list):
            if isinstance(result, dict):
                title = result.get("title", "")
                content_text = result.get("content", "")
                publish_date = result.get("publish_date", "")
            else:
                title = getattr(result, "title", "")
                content_text = getattr(result, "content", "")
                publish_date = getattr(result, "publish_date", "")
            
            result_text = f"æœç´¢ç»“æœ{i+1}:\næ ‡é¢˜: {title}\nå‘å¸ƒæ—¶é—´: {publish_date}\nå†…å®¹: {content_text}\n"
            content_list.append(result_text)
        
        return "\n".join(content_list)
    
    def _build_summary_prompt(self, original_query: str, search_content: str) -> str:
        """æ„å»ºæœç´¢æ€»ç»“çš„ç”¨æˆ·æç¤ºè¯"""
        return f"""ç”¨æˆ·æœç´¢éœ€æ±‚: {original_query}

ä»¥ä¸‹æ˜¯ç½‘ç»œæœç´¢è¿”å›çš„ç»“æœ:
{search_content}

è¯·æ ¹æ®ç”¨æˆ·çš„æœç´¢éœ€æ±‚ï¼Œå°†ä¸Šè¿°æœç´¢ç»“æœæ•´ç†ä¸ºæ•°ä¸ªæœ‰ç”¨çš„ä¿¡æ¯å—ã€‚æ¯ä¸ªä¿¡æ¯å—åº”è¯¥æœ‰å®Œæ•´çš„æ—¶é—´ã€æ¥é¾™å»è„‰ï¼Œè€Œä¸æ˜¯ç¢ç‰‡åŒ–çš„ä¿¡æ¯ã€‚ä½¿ç”¨åˆé€‚çš„é¢—ç²’åº¦è¿›è¡Œæ•´ç†ã€‚

è¦æ±‚ï¼š
1. ä¿æŒä¿¡æ¯çš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§
2. æŒ‰ç…§æ—¶é—´é¡ºåºæˆ–é€»è¾‘é¡ºåºç»„ç»‡ä¿¡æ¯
3. çªå‡ºä¸æœç´¢éœ€æ±‚æœ€ç›¸å…³çš„å†…å®¹
4. é¿å…é‡å¤ä¿¡æ¯
5. æä¾›æ¸…æ™°çš„æ ‡é¢˜å’Œå†…å®¹
"""
    
    async def _generate_search_summary(self, query: str, search_results: Dict[str, Any], user_id: str, session_id: str) -> AsyncGenerator[Dict[str, Any], None]:
        """ç”Ÿæˆæœç´¢æ€»ç»“"""
        try:
            # æ„å»ºæ€»ç»“æç¤ºè¯
            search_content = self._extract_search_content(search_results)
            user_prompt = self._build_summary_prompt(query, search_content)
            
            # æ„å»ºæ¶ˆæ¯
            messages = [
                {"role": "user", "content": user_prompt}
            ]
            
            # æµå¼è°ƒç”¨LLMï¼ˆå¸¦è¿½è¸ªï¼‰
            async for chunk in self._stream_llm(messages, user_id=user_id, session_id=session_id):
                yield await self._emit_event("llm_chunk", chunk)
                
        except Exception as e:
            self.logger.error(f"ç”Ÿæˆæœç´¢æ€»ç»“å¤±è´¥: {e}")
            yield await self._emit_event("error", f"ç”Ÿæˆæ€»ç»“å¤±è´¥: {str(e)}")
    
    def get_agent_info(self) -> Dict[str, Any]:
        """è·å–Agentä¿¡æ¯"""
        base_info = super().get_agent_info()
        base_info.update({
            "agent_type": "websearch",
            "description": "ç«–å±çŸ­å‰§ç½‘ç»œæ£€ç´¢æ™ºèƒ½ä½“ï¼Œä¸“æ³¨äºç½‘ç»œæœç´¢å’Œä¿¡æ¯æ£€ç´¢",
            "capabilities": [
                "æ™ºè°±AIç½‘ç»œæœç´¢",
                "æœç´¢ç»“æœæ™ºèƒ½æ€»ç»“",
                "å¤šé˜¶æ®µæœç´¢æµç¨‹",
                "æœç´¢ç»“æœæ ¼å¼åŒ–",
                "ä¿¡æ¯æå–å’Œæ•´ç†"
            ],
            "search_config": self.search_config
        })
        return base_info
