from typing import AsyncGenerator, Dict, Any, Optional, List
import asyncio

"""
æƒ…èŠ‚ç‚¹æˆå‰§åŠŸèƒ½åˆ†æå·¥ä½œæµæ™ºèƒ½ä½“
åŸºäºagent as toolæœºåˆ¶ï¼Œå®ç°æ™ºèƒ½ä½“é—´çš„æ¨¡å—åŒ–å¤–åŒ…å’Œä¸Šä¸‹æ–‡éš”ç¦»

ä¸šåŠ¡å¤„ç†é€»è¾‘ï¼š
1. è¾“å…¥å¤„ç†ï¼šæ¥æ”¶æ•…äº‹æ–‡æœ¬ï¼Œæ”¯æŒé•¿æ–‡æœ¬æˆªæ–­å’Œåˆ†å‰²å¤„ç†
2. å·¥ä½œæµç¼–æ’ï¼šåè°ƒæƒ…èŠ‚ç‚¹æˆå‰§åŠŸèƒ½åˆ†æçš„å®Œæ•´æµç¨‹
3. æ™ºèƒ½ä½“è°ƒç”¨ï¼šä½¿ç”¨Agent as Toolæœºåˆ¶è°ƒç”¨ä¸“ä¸šåˆ†ææ™ºèƒ½ä½“
   - TextTruncatorAgentï¼šæ–‡æœ¬æˆªæ–­å¤„ç†
   - TextSplitterAgentï¼šæ–‡æœ¬åˆ†å‰²å¤„ç†
   - DramaAnalysisAgentï¼šæƒ…èŠ‚ç‚¹æˆå‰§åŠŸèƒ½åˆ†æ
   - ResultIntegratorAgentï¼šç»“æœæ•´åˆ
4. ä¸Šä¸‹æ–‡éš”ç¦»ï¼šç¡®ä¿æ¯æ¬¡è°ƒç”¨çš„ç‹¬ç«‹æ€§å’Œå‡†ç¡®æ€§
5. å¹¶è¡Œå¤„ç†ï¼šæ”¯æŒå¤šä¸ªæ™ºèƒ½ä½“çš„å¹¶è¡Œè°ƒç”¨ï¼Œæé«˜æ•ˆç‡
6. ç»“æœæ•´åˆï¼šæ±‡æ€»å„ä¸ªæ™ºèƒ½ä½“çš„åˆ†æç»“æœ
7. è¾“å‡ºæ ¼å¼åŒ–ï¼šç”Ÿæˆå®Œæ•´çš„æƒ…èŠ‚ç‚¹æˆå‰§åŠŸèƒ½åˆ†ææŠ¥å‘Š
8. è´¨é‡æ§åˆ¶ï¼šç¡®ä¿åˆ†æç»“æœçš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§

ä»£ç ä½œè€…ï¼šå®«çµç‘
åˆ›å»ºæ—¶é—´ï¼š2025å¹´10æœˆ19æ—¥
"""
from datetime import datetime

# ğŸ”§ ä¿®å¤ï¼šå¯¼å…¥ç¼ºå¤±çš„Tokenç´¯åŠ å™¨å‡½æ•°
try:
    from ..utils.token_accumulator import create_token_accumulator, get_billing_summary
except ImportError:
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))
    from utils.token_accumulator import create_token_accumulator, get_billing_summary

try:
    from .base_juben_agent import BaseJubenAgent
    from .text_truncator_agent import TextTruncatorAgent
    from .text_splitter_agent import TextSplitterAgent
    from .drama_analysis_agent import DramaAnalysisAgent
    from .result_integrator_agent import ResultIntegratorAgent
except ImportError:
    # å¤„ç†ç›¸å¯¹å¯¼å…¥é—®é¢˜
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))

from agents.base_juben_agent import BaseJubenAgent
from agents.text_truncator_agent import TextTruncatorAgent
from agents.text_splitter_agent import TextSplitterAgent
from agents.drama_analysis_agent import DramaAnalysisAgent
from agents.result_integrator_agent import ResultIntegratorAgent


class DramaWorkflowAgent(BaseJubenAgent):
    """
    æƒ…èŠ‚ç‚¹æˆå‰§åŠŸèƒ½åˆ†æå·¥ä½œæµæ™ºèƒ½ä½“
    
    åŠŸèƒ½ï¼š
    1. ç¼–æ’æ•´ä¸ªæƒ…èŠ‚ç‚¹åˆ†æå·¥ä½œæµ
    2. ç®¡ç†æ™ºèƒ½ä½“é—´çš„è°ƒç”¨å’Œä¸Šä¸‹æ–‡éš”ç¦»
    3. å®ç°agent as toolæœºåˆ¶
    4. æ”¯æŒå¹¶è¡Œå¤„ç†å’Œç»“æœæ•´åˆ
    """
    
    def __init__(self, model_provider: str = "zhipu"):
        """åˆå§‹åŒ–æƒ…èŠ‚ç‚¹å·¥ä½œæµæ™ºèƒ½ä½“"""
        super().__init__("drama_workflow", model_provider)
        
        # åŠ è½½ç³»ç»Ÿæç¤ºè¯
        # åˆå§‹åŒ–å­æ™ºèƒ½ä½“ï¼ˆä½œä¸ºå·¥å…·ä½¿ç”¨ï¼‰
        self.text_truncator = TextTruncatorAgent(model_provider)
        self.text_splitter = TextSplitterAgent(model_provider)
        self.drama_analysis = DramaAnalysisAgent(model_provider)
        self.result_integrator = ResultIntegratorAgent(model_provider)
        
        # å·¥ä½œæµé…ç½®
        self.max_chunk_size = 10000  # æ–‡æœ¬å—æœ€å¤§å¤§å°
        self.max_parallel_analysis = 10  # æœ€å¤§å¹¶è¡Œåˆ†ææ•°é‡
        
        self.logger.info("æƒ…èŠ‚ç‚¹æˆå‰§åŠŸèƒ½åˆ†æå·¥ä½œæµæ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    # ç³»ç»Ÿæç¤ºè¯ç”±åŸºç±»è‡ªåŠ¨åŠ è½½ï¼Œæ— éœ€é‡å†™
    
    async def _call_text_truncator(self, text: str, max_length: int, user_id: str, session_id: str) -> Dict[str, Any]:
        """
        è°ƒç”¨æ–‡æœ¬æˆªæ–­æ™ºèƒ½ä½“ï¼ˆä½œä¸ºå·¥å…·ï¼‰ï¼ˆå¢å¼ºç‰ˆï¼šå¸¦è¶…æ—¶æ§åˆ¶ï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID

        Returns:
            Dict: æˆªæ–­ç»“æœ
        """
        try:
            # ========== å‚æ•°éªŒè¯ ==========
            if not text or not isinstance(text, str):
                return {"code": 400, "data": "", "msg": "è¾“å…¥æ–‡æœ¬ä¸ºç©ºæˆ–ç±»å‹ä¸æ­£ç¡®"}

            if max_length <= 0:
                self.logger.warning(f"max_lengthå‚æ•°ä¸åˆæ³•({max_length})ï¼Œä½¿ç”¨é»˜è®¤å€¼50000")
                max_length = 50000

            request_data = {
                "text": text,
                "max_length": max_length,
                "user_id": user_id,
                "session_id": session_id
            }

            # ç›´æ¥è°ƒç”¨æˆªæ–­é€»è¾‘ï¼Œé¿å…ä¾èµ–äº‹ä»¶æ ¼å¼å·®å¼‚
            result = await self.text_truncator.truncate_text(
                text,
                max_length=max_length,
            )

            if result.get("success"):
                return {
                    "code": 200,
                    "data": result.get("data", text[:max_length]),
                    "msg": result.get("msg", "æˆªæ–­æˆåŠŸ"),
                }

            # é™çº§ï¼šå³ä¾¿å¤±è´¥ä¹Ÿè¿”å›éƒ¨åˆ†æ–‡æœ¬ï¼Œé¿å…æ•´ä¸ªå·¥ä½œæµå¤±è´¥
            return {
                "code": 500,
                "data": result.get("data", text[:max_length]),
                "msg": result.get("msg", "æˆªæ–­å¤±è´¥"),
            }

        except ValueError as e:
            self.logger.error(f"è°ƒç”¨æ–‡æœ¬æˆªæ–­æ™ºèƒ½ä½“å‚æ•°é”™è¯¯: {e}")
            return {"code": 400, "data": "", "msg": f"å‚æ•°é”™è¯¯: {str(e)}"}
        except Exception as e:
            self.logger.error(f"è°ƒç”¨æ–‡æœ¬æˆªæ–­æ™ºèƒ½ä½“å¤±è´¥: {e}")
            return {"code": 500, "data": "", "msg": f"æˆªæ–­å¤±è´¥: {str(e)}"}
    
    async def _call_text_splitter(self, text: str, chunk_size: int, user_id: str, session_id: str) -> List[str]:
        """
        è°ƒç”¨æ–‡æœ¬åˆ†å‰²æ™ºèƒ½ä½“ï¼ˆä½œä¸ºå·¥å…·ï¼‰ï¼ˆå¢å¼ºç‰ˆï¼šå¸¦å‚æ•°éªŒè¯ï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬
            chunk_size: åˆ†å‰²å¤§å°
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID

        Returns:
            List[str]: åˆ†å‰²åçš„æ–‡æœ¬å—
        """
        try:
            # ========== å‚æ•°éªŒè¯ ==========
            if not text or not isinstance(text, str):
                self.logger.error("è¾“å…¥æ–‡æœ¬ä¸ºç©ºæˆ–ç±»å‹ä¸æ­£ç¡®")
                return []

            if chunk_size <= 0:
                self.logger.warning(f"chunk_sizeå‚æ•°ä¸åˆæ³•({chunk_size})ï¼Œä½¿ç”¨é»˜è®¤å€¼10000")
                chunk_size = 10000

            request_data = {
                "text": text,
                "chunk_size": chunk_size,
                "user_id": user_id,
                "session_id": session_id
            }

            # ç›´æ¥è°ƒç”¨åˆ†å‰²é€»è¾‘ï¼Œé¿å…ä¾èµ–äº‹ä»¶æ ¼å¼å·®å¼‚
            # è¿™é‡Œä½¿ç”¨é»˜è®¤æ¨¡å¼çš„ split_text
            chunks = await self.text_splitter.split_text(
                text,
                chunk_size=chunk_size,
                overlap=200,
                preserve_sentences=True,
            )

            # éªŒè¯è¿”å›ç»“æœ
            if not isinstance(chunks, list):
                self.logger.error(f"åˆ†å‰²è¿”å›éåˆ—è¡¨ç±»å‹: {type(chunks)}")
                return [text]

            # è¿‡æ»¤ç©ºchunk
            chunks = [c for c in chunks if c and isinstance(c, str) and len(c.strip()) > 0]

            if not chunks:
                self.logger.warning("åˆ†å‰²åæ²¡æœ‰æœ‰æ•ˆchunkï¼Œä½¿ç”¨åŸæ–‡ä½œä¸ºå”¯ä¸€chunk")
                return [text]

            return chunks

        except ValueError as e:
            self.logger.error(f"è°ƒç”¨æ–‡æœ¬åˆ†å‰²æ™ºèƒ½ä½“å‚æ•°é”™è¯¯: {e}")
            # é™çº§å¤„ç†ï¼šä½¿ç”¨å®‰å…¨çš„ç®€å•åˆ†å‰²
            safe_chunk_size = max(1, min(chunk_size if chunk_size > 0 else 10000, len(text)))
            return [text[i:min(i + safe_chunk_size, len(text))] for i in range(0, len(text), safe_chunk_size)]
        except Exception as e:
            self.logger.error(f"è°ƒç”¨æ–‡æœ¬åˆ†å‰²æ™ºèƒ½ä½“å¤±è´¥: {e}")
            # é™çº§å¤„ç†ï¼šè¿”å›åŸæ–‡ä½œä¸ºå”¯ä¸€chunk
            return [text]
    
    async def _call_drama_analysis(self, text: str, user_id: str, session_id: str) -> str:
        """
        è°ƒç”¨æƒ…èŠ‚ç‚¹åˆ†ææ™ºèƒ½ä½“ï¼ˆä½œä¸ºå·¥å…·ï¼‰ï¼ˆå¢å¼ºç‰ˆï¼šå¸¦è¶…æ—¶æ§åˆ¶ï¼‰

        Args:
            text: è¾“å…¥æ–‡æœ¬
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID

        Returns:
            str: åˆ†æç»“æœ
        """
        import asyncio

        try:
            # ========== å‚æ•°éªŒè¯ ==========
            if not text or not isinstance(text, str):
                return "é”™è¯¯ï¼šè¾“å…¥æ–‡æœ¬ä¸ºç©ºæˆ–ç±»å‹ä¸æ­£ç¡®"

            request_data = {
                "input": text,
                "user_id": user_id,
                "session_id": session_id
            }

            # æ”¶é›†æµå¼å“åº”ï¼ˆå¸¦è¶…æ—¶ï¼‰
            result = ""

            async def collect_result():
                nonlocal result
                async for event in self.drama_analysis.process_request(request_data):
                    et = event.get("type") or event.get("event_type")
                    # DramaAnalysisAgent ä½¿ç”¨ type="drama_analysis" è¾“å‡ºå®Œæ•´åˆ†ææ–‡æœ¬
                    if et == "drama_analysis":
                        content = event.get("content") or event.get("data") or ""
                        if isinstance(content, str):
                            result = content
                        break

            # ä½¿ç”¨è¶…æ—¶æ§åˆ¶
            try:
                await asyncio.wait_for(collect_result(), timeout=120)
            except asyncio.TimeoutError:
                self.logger.error("æƒ…èŠ‚ç‚¹åˆ†ææ™ºèƒ½ä½“è°ƒç”¨è¶…æ—¶(120ç§’)")
                return "åˆ†æè¶…æ—¶(120ç§’)"

            return result

        except ValueError as e:
            self.logger.error(f"è°ƒç”¨æƒ…èŠ‚ç‚¹åˆ†ææ™ºèƒ½ä½“å‚æ•°é”™è¯¯: {e}")
            return f"å‚æ•°é”™è¯¯: {str(e)}"
        except Exception as e:
            self.logger.error(f"è°ƒç”¨æƒ…èŠ‚ç‚¹åˆ†ææ™ºèƒ½ä½“å¤±è´¥: {e}")
            return f"åˆ†æå¤±è´¥: {str(e)}"

    async def _call_result_integrator(self, results: List[str], user_id: str, session_id: str) -> str:
        """
        è°ƒç”¨ç»“æœæ•´åˆæ™ºèƒ½ä½“ï¼ˆä½œä¸ºå·¥å…·ï¼‰ï¼ˆå¢å¼ºç‰ˆï¼šå¸¦è¶…æ—¶æ§åˆ¶ï¼‰

        Args:
            results: åˆ†æç»“æœåˆ—è¡¨
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID

        Returns:
            str: æ•´åˆç»“æœ
        """
        import asyncio

        try:
            # ========== å‚æ•°éªŒè¯ ==========
            if not results or not isinstance(results, list):
                return "é”™è¯¯ï¼šåˆ†æç»“æœä¸ºç©ºæˆ–ç±»å‹ä¸æ­£ç¡®"

            request_data = {
                "results": results,
                "user_id": user_id,
                "session_id": session_id
            }

            # æ”¶é›†æµå¼å“åº”ï¼ˆå¸¦è¶…æ—¶ï¼‰
            result = ""

            async def collect_result():
                nonlocal result
                async for event in self.result_integrator.process_request(request_data):
                    if event["type"] == "result_integration_result":
                        result = event["data"]["integrated_result"]
                        break

            # ä½¿ç”¨è¶…æ—¶æ§åˆ¶
            try:
                await asyncio.wait_for(collect_result(), timeout=60)
            except asyncio.TimeoutError:
                self.logger.error("ç»“æœæ•´åˆæ™ºèƒ½ä½“è°ƒç”¨è¶…æ—¶(60ç§’)")
                return "\n\n".join(results)  # é™çº§ï¼šç®€å•æ‹¼æ¥

            return result

        except ValueError as e:
            self.logger.error(f"è°ƒç”¨ç»“æœæ•´åˆæ™ºèƒ½ä½“å‚æ•°é”™è¯¯: {e}")
            return f"å‚æ•°é”™è¯¯: {str(e)}"
        except Exception as e:
            self.logger.error(f"è°ƒç”¨ç»“æœæ•´åˆæ™ºèƒ½ä½“å¤±è´¥: {e}")
            return f"æ•´åˆå¤±è´¥: {str(e)}"
    
    async def process_request(
        self,
        request_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¤„ç†è¯·æ±‚ - æ”¯æŒAgent as Toolæœºåˆ¶
        
        Args:
            request_data: è¯·æ±‚æ•°æ®
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯
                - user_id: ç”¨æˆ·ID
                - session_id: ä¼šè¯ID  
                - parent_agent: çˆ¶æ™ºèƒ½ä½“åç§°ï¼ˆAgent as Toolæ¨¡å¼ï¼‰
                - tool_call: æ˜¯å¦ä¸ºå·¥å…·è°ƒç”¨
                
        Yields:
            Dict: æµå¼å“åº”äº‹ä»¶
        """
        
        # æå–è¯·æ±‚å‚æ•°ï¼ˆå…¼å®¹ /juben/chat çš„ input/query å­—æ®µï¼‰
        raw_text = (
            request_data.get("text")
            or request_data.get("input")
            or request_data.get("query")
            or ""
        )
        text = raw_text if isinstance(raw_text, str) else str(raw_text)

        # ä¼˜å…ˆä» context å– user/sessionï¼Œå…¶æ¬¡é€€å› request_data
        user_id = (context or {}).get("user_id") or request_data.get("user_id", "unknown")
        session_id = (context or {}).get("session_id") or request_data.get("session_id", "unknown")
        
        self.logger.info(f"å¤„ç†æƒ…èŠ‚ç‚¹æˆå‰§åŠŸèƒ½åˆ†æå·¥ä½œæµè¯·æ±‚: text_length={len(text)}")
        
        try:
            # å¦‚æœæ–‡æœ¬ä¸ºç©ºï¼Œç›´æ¥è¿”å›å‹å¥½çš„é”™è¯¯äº‹ä»¶
            if not text.strip():
                self.logger.error("æƒ…èŠ‚ç‚¹æˆå‰§åŠŸèƒ½åˆ†æå·¥ä½œæµå¤±è´¥: è¾“å…¥æ–‡æœ¬ä¸ºç©º")
                yield {
                    "type": "workflow_error",
                    "data": {"error": "è¾“å…¥æ–‡æœ¬ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œåˆ†æ"},
                    "timestamp": datetime.now().isoformat()
                }
                return

            # åˆå§‹åŒ–Tokenç´¯åŠ å™¨
            await self._init_token_accumulator(user_id, session_id)
            
            # å‘é€å·¥ä½œæµå¼€å§‹äº‹ä»¶
            yield {
                "type": "workflow_start",
                "data": {"message": "å¼€å§‹æƒ…èŠ‚ç‚¹æˆå‰§åŠŸèƒ½åˆ†æå·¥ä½œæµ"},
                "timestamp": datetime.now().isoformat()
            }
            
            # æ­¥éª¤1: æ–‡æœ¬æˆªæ–­å¤„ç†
            yield {
                "type": "workflow_step",
                "data": {"step": 1, "message": "æ­¥éª¤1: æ–‡æœ¬æˆªæ–­å¤„ç†"},
                "timestamp": datetime.now().isoformat()
            }
            
            truncated_result = await self._call_text_truncator(text, 50000, user_id, session_id)
            if truncated_result["code"] != 200:
                raise Exception(f"æ–‡æœ¬æˆªæ–­å¤±è´¥: {truncated_result['msg']}")
            
            truncated_text = truncated_result["data"]
            
            # æ­¥éª¤2: æ–‡æœ¬åˆ†å‰²å¤„ç†
            yield {
                "type": "workflow_step",
                "data": {"step": 2, "message": "æ­¥éª¤2: æ–‡æœ¬åˆ†å‰²å¤„ç†"},
                "timestamp": datetime.now().isoformat()
            }
            
            text_chunks = await self._call_text_splitter(truncated_text, self.max_chunk_size, user_id, session_id)
            if not text_chunks:
                raise Exception("æ–‡æœ¬åˆ†å‰²å¤±è´¥")
            
            yield {
                "type": "workflow_progress",
                "data": {"message": f"æ–‡æœ¬å·²åˆ†å‰²ä¸º{len(text_chunks)}ä¸ªç‰‡æ®µ"},
                "timestamp": datetime.now().isoformat()
            }
            
            # æ­¥éª¤3: å¹¶è¡Œæƒ…èŠ‚ç‚¹åˆ†æ
            yield {
                "type": "workflow_step",
                "data": {"step": 3, "message": "æ­¥éª¤3: å¹¶è¡Œæƒ…èŠ‚ç‚¹åˆ†æ"},
                "timestamp": datetime.now().isoformat()
            }
            
            # åˆ›å»ºåˆ†æä»»åŠ¡
            analysis_tasks = []
            for i, chunk in enumerate(text_chunks[:self.max_parallel_analysis]):
                task = self._call_drama_analysis(chunk, user_id, session_id)
                analysis_tasks.append(task)
            
            # å¹¶è¡Œæ‰§è¡Œåˆ†æ
            analysis_results = await asyncio.gather(*analysis_tasks, return_exceptions=True)
            
            # è¿‡æ»¤æˆåŠŸçš„ç»“æœ
            valid_results = []
            for i, result in enumerate(analysis_results):
                if isinstance(result, str) and result and not result.startswith("åˆ†æå¤±è´¥"):
                    valid_results.append(result)
                else:
                    self.logger.warning(f"ç¬¬{i+1}ä¸ªæ–‡æœ¬ç‰‡æ®µåˆ†æå¤±è´¥")
            
            yield {
                "type": "workflow_progress",
                "data": {"message": f"å®Œæˆ{len(valid_results)}ä¸ªæ–‡æœ¬ç‰‡æ®µçš„åˆ†æ"},
                "timestamp": datetime.now().isoformat()
            }
            
            # æ­¥éª¤4: ç»“æœæ•´åˆ
            if valid_results:
                yield {
                    "type": "workflow_step",
                    "data": {"step": 4, "message": "æ­¥éª¤4: æ•´åˆåˆ†æç»“æœ"},
                    "timestamp": datetime.now().isoformat()
                }
                
                integrated_result = await self._call_result_integrator(valid_results, user_id, session_id)

                # å…ˆä»¥ llm_chunk å½¢å¼è¾“å‡ºå¯ç›´æ¥å±•ç¤ºçš„æ–‡æœ¬ç»“æœï¼Œä¾›å‰ç«¯èŠå¤©çª—å£ä½¿ç”¨
                if isinstance(integrated_result, str) and integrated_result.strip():
                    yield {
                        "type": "llm_chunk",
                        "content": integrated_result,
                        "timestamp": datetime.now().isoformat()
                    }

                # å†å‘é€ç»“æ„åŒ–çš„å·¥ä½œæµç»“æœï¼Œä¾¿äºåç»­å¯è§†åŒ–æˆ–å¯¼å‡º
                yield {
                    "type": "workflow_result",
                    "data": {
                        "final_result": integrated_result,
                        "processed_chunks": len(valid_results),
                        "total_chunks": len(text_chunks)
                    },
                    "timestamp": datetime.now().isoformat()
                }
                
                yield {
                    "type": "workflow_complete",
                    "data": {"message": "æƒ…èŠ‚ç‚¹æˆå‰§åŠŸèƒ½åˆ†æå·¥ä½œæµå®Œæˆ"},
                    "timestamp": datetime.now().isoformat()
                }
            else:
                yield {
                    "type": "workflow_error",
                    "data": {"message": "æ²¡æœ‰æœ‰æ•ˆçš„åˆ†æç»“æœ"},
                    "timestamp": datetime.now().isoformat()
                }
            
            # å‘é€Tokenç»Ÿè®¡
            await self._send_token_summary()
            
        except Exception as e:
            self.logger.error(f"æƒ…èŠ‚ç‚¹æˆå‰§åŠŸèƒ½åˆ†æå·¥ä½œæµå¤±è´¥: {e}")
            yield {
                "type": "workflow_error",
                "data": {"error": str(e)},
                "timestamp": datetime.now().isoformat()
            }
        finally:
            # æ¸…ç†Tokenç´¯åŠ å™¨
            await self._cleanup_token_accumulator()
    
    async def _init_token_accumulator(self, user_id: str, session_id: str):
        """åˆå§‹åŒ–Tokenç´¯åŠ å™¨"""
        try:
            # create_token_accumulator ä¸ºåŒæ­¥å‡½æ•°ï¼Œä¸æ¥å— agent_name å‚æ•°
            self.current_token_accumulator_key = create_token_accumulator(
                user_id=user_id,
                session_id=session_id
            )
            self.logger.info(f"Tokenç´¯åŠ å™¨åˆå§‹åŒ–æˆåŠŸ: {self.current_token_accumulator_key}")
        except Exception as e:
            self.logger.error(f"Tokenç´¯åŠ å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
    
    async def _send_token_summary(self):
        """å‘é€Tokenç»Ÿè®¡æ‘˜è¦"""
        try:
            if self.current_token_accumulator_key:
                # get_billing_summary ä¸ºåŒæ­¥å‡½æ•°ï¼Œæ— éœ€ await
                summary = get_billing_summary(self.current_token_accumulator_key)
                if summary:
                    self.logger.info(f"Tokenä½¿ç”¨ç»Ÿè®¡: {summary}")
        except Exception as e:
            self.logger.error(f"è·å–Tokenç»Ÿè®¡å¤±è´¥: {e}")
    
    async def _cleanup_token_accumulator(self):
        """æ¸…ç†Tokenç´¯åŠ å™¨"""
        try:
            if self.current_token_accumulator_key:
                # è¿™é‡Œå¯ä»¥æ·»åŠ æ¸…ç†é€»è¾‘
                self.current_token_accumulator_key = None
        except Exception as e:
            self.logger.error(f"æ¸…ç†Tokenç´¯åŠ å™¨å¤±è´¥: {e}")
