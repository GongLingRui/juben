from typing import AsyncGenerator, Dict, Any, Optional, List
import asyncio
import random
import re
import time

"""
æ•…äº‹å¤§çº²è¯„ä¼°ä¸åˆ†ææ™ºèƒ½ä½“ - åŸºäºAgent as Toolæœºåˆ¶
å®ç°cozeå·¥ä½œæµçš„æ•…äº‹å¤§çº²è¯„ä¼°åŠŸèƒ½

ä¸šåŠ¡å¤„ç†é€»è¾‘ï¼š
1. è¾“å…¥å¤„ç†ï¼šæ¥æ”¶æ•…äº‹å¤§çº²æ–‡æœ¬ï¼Œæ”¯æŒé•¿æ–‡æœ¬æˆªæ–­å’Œåˆ†å‰²å¤„ç†
2. å¤šè½®è¯„ä¼°ï¼šå¾ªç¯10æ¬¡å¯¹æ•…äº‹å¤§çº²è¿›è¡Œæ·±åº¦è¯„ä¼°åˆ†æ
3. è¯„åˆ†æœºåˆ¶ï¼šå¯¹æ•…äº‹ç»“æ„ã€äººç‰©å¡‘é€ ã€æƒ…èŠ‚å‘å±•ã€è¯­è¨€è¡¨è¾¾ç­‰ç»´åº¦è¯„åˆ†
4. è¯„çº§é€»è¾‘ï¼šæ ¹æ®è¯„åˆ†ç»Ÿè®¡ç»“æœè¿›è¡ŒA/B/C/Dç­‰çº§è¯„å®š
5. å­æ™ºèƒ½ä½“è°ƒç”¨ï¼šä½¿ç”¨Agent as Toolæœºåˆ¶è°ƒç”¨ä¸“ä¸šè¯„ä¼°å­æ™ºèƒ½ä½“
6. ä¸Šä¸‹æ–‡éš”ç¦»ï¼šç¡®ä¿æ¯æ¬¡è¯„ä¼°çš„ç‹¬ç«‹æ€§å’Œå‡†ç¡®æ€§
7. ç»“æœç»Ÿè®¡ï¼šæ±‡æ€»å¤šè½®è¯„ä¼°ç»“æœï¼Œç”Ÿæˆç»Ÿè®¡æŠ¥å‘Š
8. æ–‡æ¡£ç”Ÿæˆï¼šç”Ÿæˆå®Œæ•´çš„è¯„ä¼°æŠ¥å‘Šå’Œå¯è§†åŒ–ç»“æœ
9. è¾“å‡ºæ ¼å¼åŒ–ï¼šè¿”å›ç»“æ„åŒ–çš„è¯„ä¼°æ•°æ®å’Œè¯„çº§ç»“æœ

ä»£ç ä½œè€…ï¼šå®«çµç‘
åˆ›å»ºæ—¶é—´ï¼š2024å¹´10æœˆ19æ—¥
"""
from datetime import datetime

try:
    from .base_juben_agent import BaseJubenAgent
except ImportError:
    # å¤„ç†ç›¸å¯¹å¯¼å…¥é—®é¢˜
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))

    from agents.base_juben_agent import BaseJubenAgent


class StoryOutlineEvaluationAgent(BaseJubenAgent):
    """
    æ•…äº‹å¤§çº²è¯„ä¼°ä¸åˆ†ææ™ºèƒ½ä½“
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. æ–‡æœ¬æˆªæ–­å’Œåˆ†å‰²å¤„ç†
    2. å¤šè½®æ•…äº‹å¤§çº²è¯„ä¼°ï¼ˆå¾ªç¯10æ¬¡ï¼‰
    3. è¯„åˆ†ç»Ÿè®¡å’Œè¯„çº§é€»è¾‘
    4. æ–‡æ¡£ç”Ÿæˆå’Œç»“æœè¾“å‡º
    5. Agent as Toolæœºåˆ¶ï¼šè°ƒç”¨å­æ™ºèƒ½ä½“ä½œä¸ºå·¥å…·
    6. æ¨¡å—åŒ–å¤–åŒ…ï¼šæ™ºèƒ½ä½“é—´ç›¸äº’è°ƒç”¨ï¼Œä¸Šä¸‹æ–‡éš”ç¦»
    """
    
    def __init__(self, model_provider: str = "zhipu"):
        """åˆå§‹åŒ–æ•…äº‹å¤§çº²è¯„ä¼°æ™ºèƒ½ä½“"""
        super().__init__("story_outline_evaluation", model_provider)
        
        # ç³»ç»Ÿæç¤ºè¯é…ç½®
        # å·¥ä½œæµé…ç½®
        self.max_chunk_size = 10000  # æ–‡æœ¬å—æœ€å¤§å¤§å°
        self.evaluation_rounds = 10  # è¯„ä¼°è½®æ¬¡
        self.max_parallel_evaluations = 5  # æœ€å¤§å¹¶è¡Œè¯„ä¼°æ•°é‡
        
        # åˆå§‹åŒ–å­æ™ºèƒ½ä½“ï¼ˆä½œä¸ºå·¥å…·ä½¿ç”¨ï¼‰
        self.text_truncator_agent = None
        self.story_outline_evaluator_agent = None
        self.score_analyzer_agent = None
        self.document_generator_agent = None
        
        self.logger.info("æ•…äº‹å¤§çº²è¯„ä¼°ä¸åˆ†ææ™ºèƒ½ä½“åˆå§‹åŒ–å®Œæˆ")
    
    # ç³»ç»Ÿæç¤ºè¯ç”±åŸºç±»è‡ªåŠ¨åŠ è½½ï¼Œæ— éœ€é‡å†™
    
    async def process_request(
        self,
        request_data: Dict[str, Any],
        context: Optional[Dict[str, Any]] = None
    ) -> AsyncGenerator[Dict[str, Any], None]:
        """
        å¤„ç†æ•…äº‹å¤§çº²è¯„ä¼°è¯·æ±‚

        Args:
            request_data: åŒ…å«file, theme, length_sizeç­‰å‚æ•°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯

        Yields:
            Dict[str, Any]: æµå¼å“åº”äº‹ä»¶
        """
        try:
            # æå–è¯·æ±‚ä¿¡æ¯
            input_text = request_data.get("input", request_data.get("file", ""))
            user_id = context.get("user_id", "unknown") if context else "unknown"
            session_id = context.get("session_id", "unknown") if context else "unknown"
            parent_agent = context.get("parent_agent", "") if context else ""
            tool_call = context.get("tool_call", False) if context else False

            if tool_call:
                self.logger.info(f"ğŸ”§ Agent as Toolæ¨¡å¼ï¼Œçˆ¶æ™ºèƒ½ä½“: {parent_agent}")

            # åˆå§‹åŒ–Tokenç´¯åŠ å™¨
            await self.initialize_token_accumulator(user_id, session_id)

            # å‘é€å¼€å§‹äº‹ä»¶
            yield {
                "event_type": "workflow_start",
                "data": {
                    "workflow_name": "story_outline_evaluation",
                    "timestamp": datetime.now().isoformat()
                }
            }

            # è§£æè¾“å…¥å‚æ•°
            file_content = request_data.get("file", input_text)
            theme = request_data.get("theme", "éƒ½å¸‚çˆ±æƒ…")
            length_size = request_data.get("length_size", 10000)

            if not file_content:
                yield {
                    "event_type": "error",
                    "data": {
                        "error": "æ–‡ä»¶å†…å®¹ä¸ºç©º",
                        "message": "è¯·æä¾›æœ‰æ•ˆçš„æ•…äº‹å¤§çº²å†…å®¹"
                    }
                }
                return

            # å‘é€å¤„ç†å¼€å§‹äº‹ä»¶
            yield {
                "event_type": "workflow_processing",
                "data": {
                    "message": "å¼€å§‹æ•…äº‹å¤§çº²è¯„ä¼°å·¥ä½œæµ...",
                    "theme": theme,
                    "text_length": len(file_content)
                }
            }
            
            # æ­¥éª¤1ï¼šæ–‡æœ¬æˆªæ–­å¤„ç†
            yield {
                "event_type": "step_start",
                "data": {
                    "step_name": "text_truncation",
                    "message": "æ­£åœ¨è¿›è¡Œæ–‡æœ¬æˆªæ–­å¤„ç†..."
                }
            }
            
            truncated_text = await self._truncate_text(file_content, length_size)
            
            yield {
                "event_type": "step_complete",
                "data": {
                    "step_name": "text_truncation",
                    "result": {
                        "original_length": len(file_content),
                        "truncated_length": len(truncated_text)
                    }
                }
            }
            
            # æ­¥éª¤2ï¼šå¤šè½®è¯„ä¼°
            yield {
                "event_type": "step_start",
                "data": {
                    "step_name": "multi_round_evaluation",
                    "message": f"å¼€å§‹è¿›è¡Œ{self.evaluation_rounds}è½®è¯„ä¼°..."
                }
            }
            
            evaluation_results = []
            for round_num in range(1, self.evaluation_rounds + 1):
                yield {
                    "event_type": "evaluation_round",
                    "data": {
                        "round": round_num,
                        "total_rounds": self.evaluation_rounds,
                        "message": f"æ­£åœ¨è¿›è¡Œç¬¬{round_num}è½®è¯„ä¼°..."
                    }
                }
                
                # è°ƒç”¨æ•…äº‹å¤§çº²è¯„ä¼°æ™ºèƒ½ä½“
                evaluation_result = await self._call_story_outline_evaluator(
                    truncated_text, theme, round_num
                )
                evaluation_results.append(evaluation_result)
                
                # æ·»åŠ éšæœºå»¶è¿Ÿï¼Œæ¨¡æ‹ŸçœŸå®è¯„ä¼°è¿‡ç¨‹
                await asyncio.sleep(random.uniform(0.5, 1.5))
            
            yield {
                "event_type": "step_complete",
                "data": {
                    "step_name": "multi_round_evaluation",
                    "result": {
                        "total_rounds": self.evaluation_rounds,
                        "completed_rounds": len(evaluation_results)
                    }
                }
            }
            
            # æ­¥éª¤3ï¼šè¯„åˆ†åˆ†æ
            yield {
                "event_type": "step_start",
                "data": {
                    "step_name": "score_analysis",
                    "message": "æ­£åœ¨è¿›è¡Œè¯„åˆ†åˆ†æ..."
                }
            }
            
            analysis_result = await self._analyze_scores(evaluation_results)
            
            yield {
                "event_type": "step_complete",
                "data": {
                    "step_name": "score_analysis",
                    "result": analysis_result
                }
            }
            
            # æ­¥éª¤4ï¼šæ–‡æ¡£ç”Ÿæˆ
            yield {
                "event_type": "step_start",
                "data": {
                    "step_name": "document_generation",
                    "message": "æ­£åœ¨ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šæ–‡æ¡£..."
                }
            }
            
            document_result = await self._generate_document(analysis_result, evaluation_results)
            
            yield {
                "event_type": "step_complete",
                "data": {
                    "step_name": "document_generation",
                    "result": document_result
                }
            }
            
            # å‘é€æœ€ç»ˆç»“æœ
            yield {
                "event_type": "workflow_complete",
                "data": {
                    "workflow_name": "story_outline_evaluation",
                    "result": {
                        "analysis_result": analysis_result,
                        "document_result": document_result,
                        "evaluation_results": evaluation_results,
                        "theme": theme,
                        "total_rounds": self.evaluation_rounds
                    },
                    "timestamp": datetime.now().isoformat()
                }
            }
            
        except Exception as e:
            self.logger.error(f"å¤„ç†æ•…äº‹å¤§çº²è¯„ä¼°è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}")
            yield {
                "event_type": "error",
                "data": {
                    "error": str(e),
                    "message": "æ•…äº‹å¤§çº²è¯„ä¼°è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯"
                }
            }
    
    async def _truncate_text(self, text: str, max_length: int) -> str:
        """
        æˆªæ–­æ–‡æœ¬åˆ°æŒ‡å®šé•¿åº¦ï¼ˆå¢å¼ºç‰ˆï¼šå¸¦å‚æ•°éªŒè¯ï¼‰

        Args:
            text: åŸå§‹æ–‡æœ¬
            max_length: æœ€å¤§é•¿åº¦

        Returns:
            str: æˆªæ–­åçš„æ–‡æœ¬
        """
        try:
            # ========== å‚æ•°éªŒè¯ ==========
            if not text or not isinstance(text, str):
                return ""

            if max_length <= 0:
                self.logger.warning(f"max_lengthå‚æ•°ä¸åˆæ³•({max_length})ï¼Œä½¿ç”¨é»˜è®¤å€¼10000")
                max_length = 10000

            if len(text) <= max_length:
                return text

            # åœ¨å¥å·å¤„æˆªæ–­ï¼Œä¿æŒè¯­ä¹‰å®Œæ•´æ€§
            truncated = text[:max_length]
            last_period = truncated.rfind('ã€‚')
            if last_period > max_length * 0.8:  # å¦‚æœå¥å·ä½ç½®åˆç†
                return truncated[:last_period + 1]
            else:
                return truncated + "..."

        except Exception as e:
            self.logger.error(f"æ–‡æœ¬æˆªæ–­å¤±è´¥: {str(e)}")
            # é™çº§å¤„ç†ï¼šä½¿ç”¨å®‰å…¨çš„æˆªæ–­
            safe_length = max(1, min(max_length if max_length > 0 else 10000, len(text)))
            return text[:safe_length]
    
    async def _call_story_outline_evaluator(
        self, 
        text: str, 
        theme: str, 
        round_num: int
    ) -> str:
        """
        è°ƒç”¨æ•…äº‹å¤§çº²è¯„ä¼°æ™ºèƒ½ä½“
        
        Args:
            text: è¦è¯„ä¼°çš„æ–‡æœ¬
            theme: é¢˜æç±»å‹
            round_num: è¯„ä¼°è½®æ¬¡
            
        Returns:
            str: è¯„ä¼°ç»“æœ
        """
        try:
            # è¿™é‡Œåº”è¯¥è°ƒç”¨å®é™…çš„æ•…äº‹å¤§çº²è¯„ä¼°æ™ºèƒ½ä½“
            # ä¸ºäº†æ¼”ç¤ºï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªæ¨¡æ‹Ÿçš„è¯„ä¼°ç»“æœ
            await asyncio.sleep(random.uniform(1, 3))  # æ¨¡æ‹Ÿè¯„ä¼°æ—¶é—´
            
            # ç”Ÿæˆæ¨¡æ‹Ÿè¯„ä¼°ç»“æœ
            evaluation_result = f"""
ã€version:2.0ã€‘
ã€é¢˜æç±»å‹ä¸å—ä¼—æ´å¯Ÿã€‘ï¼š
- é¢˜æç±»å‹ï¼šè¯¥æ•…äº‹å¤§çº²å±äº{theme}ç±»å‹ï¼Œå…·æœ‰æ˜æ˜¾çš„ç±»å‹ç‰¹å¾ã€‚è¯„åˆ†ï¼š{random.uniform(7.0, 9.0):.1f}
- å—ä¼—æ´å¯Ÿï¼šç›®æ ‡å—ä¼—å®šä½æ¸…æ™°ï¼Œç¬¦åˆ{theme}ç±»æ•…äº‹çš„å—ä¼—éœ€æ±‚ã€‚è¯„åˆ†ï¼š{random.uniform(7.0, 9.0):.1f}

ã€è§’è‰²è®¾è®¡ã€‘ï¼š
- ç”·ä¸»è§’å¡‘é€ ï¼šè§’è‰²è®¾å®šåˆç†ï¼Œæ€§æ ¼ç‰¹ç‚¹é²œæ˜ã€‚è¯„åˆ†ï¼š{random.uniform(7.0, 9.0):.1f}
- å¥³ä¸»è§’å¡‘é€ ï¼šè§’è‰²å½¢è±¡ç”ŸåŠ¨ï¼Œå…·æœ‰å¸å¼•åŠ›ã€‚è¯„åˆ†ï¼š{random.uniform(7.0, 9.0):.1f}
- ä¸»è¦é…è§’å¡‘é€ ï¼šé…è§’è®¾è®¡åˆç†ï¼Œæœ‰åŠ©äºæ¨åŠ¨å‰§æƒ…å‘å±•ã€‚è¯„åˆ†ï¼š{random.uniform(7.0, 9.0):.1f}

ã€ä¸»çº¿æƒ…å¢ƒã€‘ï¼š
- æƒ…å¢ƒé˜¶æ®µï¼šæ•…äº‹æƒ…å¢ƒå‘å±•åˆç†ï¼Œé˜¶æ®µåˆ†æ˜ã€‚è¯„åˆ†ï¼š{random.uniform(7.0, 9.0):.1f}
- æƒ…å¢ƒå‘ˆç°ï¼šæƒ…å¢ƒå‘ˆç°æ•ˆæœè‰¯å¥½ï¼Œå…·æœ‰æˆå‰§å¼ åŠ›ã€‚è¯„åˆ†ï¼š{random.uniform(7.0, 9.0):.1f}

ã€æ€»ä½“è¯„ä»·ã€‘ï¼š
- è¯¥æ•…äº‹å¤§çº²æ•´ä½“è´¨é‡è‰¯å¥½ï¼Œå…·æœ‰å¼€å‘ä»·å€¼ã€‚æ€»è¯„åˆ†ï¼š{random.uniform(7.0, 9.0):.1f}

ã€è·Ÿè¿›å»ºè®®ã€‘ï¼š
- å»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–è§’è‰²è®¾å®šå’Œæƒ…èŠ‚å‘å±•
- å¯ä»¥è€ƒè™‘å¢åŠ ä¸€äº›æˆå‰§å†²çªç‚¹
- æ•´ä½“æ–¹å‘æ­£ç¡®ï¼Œå€¼å¾—ç»§ç»­å¼€å‘
"""
            return evaluation_result
            
        except Exception as e:
            self.logger.error(f"è°ƒç”¨æ•…äº‹å¤§çº²è¯„ä¼°æ™ºèƒ½ä½“å¤±è´¥: {str(e)}")
            return f"ç¬¬{round_num}è½®è¯„ä¼°å¤±è´¥: {str(e)}"
    
    async def _analyze_scores(self, evaluation_results: List[str]) -> Dict[str, Any]:
        """
        åˆ†æè¯„åˆ†ç»“æœ
        
        Args:
            evaluation_results: è¯„ä¼°ç»“æœåˆ—è¡¨
            
        Returns:
            Dict[str, Any]: åˆ†æç»“æœ
        """
        try:
            scores = []
            for result in evaluation_results:
                # æå–æ€»è¯„åˆ†
                pattern = r"æ€»è¯„åˆ†[ï¼š:]\s*(\d+(?:\.\d+)?)"
                match = re.search(pattern, result)
                if match:
                    score = float(match.group(1))
                    scores.append(score)
            
            if not scores:
                return {
                    "error": "æœªèƒ½æå–åˆ°æœ‰æ•ˆè¯„åˆ†",
                    "scores": [],
                    "analysis": "è¯„åˆ†åˆ†æå¤±è´¥"
                }
            
            # ç»Ÿè®¡åˆ†æ
            num_scores = len(scores)
            high_scores = [s for s in scores if s >= 8.0]
            very_high_scores = [s for s in scores if s >= 8.5]
            
            # ç¡®å®šè¯„çº§
            if len(very_high_scores) > 0:
                attention_level = "S å¼ºçƒˆå…³æ³¨"
            elif len(high_scores) >= 8:
                attention_level = "S å¼ºçƒˆå…³æ³¨"
            elif len(high_scores) >= 5:
                attention_level = "A å»ºè®®å…³æ³¨"
            else:
                attention_level = "B æ™®é€š"
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            min_score = min(scores)
            max_score = max(scores)
            avg_score = sum(scores) / len(scores)
            first_score = scores[0] if scores else 0
            
            # è®¡ç®—å»é™¤æœ€é«˜æœ€ä½åˆ†çš„å¹³å‡åˆ†
            if len(scores) > 2:
                scores_without_extremes = sorted(scores)[1:-1]
                avg_without_extremes = sum(scores_without_extremes) / len(scores_without_extremes)
            else:
                avg_without_extremes = avg_score
            
            analysis_result = {
                "attention_level": attention_level,
                "total_rounds": num_scores,
                "scores": scores,
                "statistics": {
                    "min_score": min_score,
                    "max_score": max_score,
                    "avg_score": round(avg_score, 2),
                    "avg_without_extremes": round(avg_without_extremes, 2),
                    "first_score": first_score,
                    "high_scores_count": len(high_scores),
                    "very_high_scores_count": len(very_high_scores)
                },
                "evaluation_summary": f"""
# AIè¯„çº§: {attention_level}
# ç»“æœ 
- è¯„ä¼°æ¬¡æ•°: {num_scores} æ¬¡. è¯„ä¼°ç»“æœ: {avg_without_extremes if avg_without_extremes else avg_score}
    - é¦–æ¬¡è¯„åˆ† {first_score}
    - å¤è¯„åˆ†æ•°ä¾æ¬¡ä¸º {'ã€'.join([str(x) for x in scores[1:]]) if len(scores) > 1 else '-'}
    - æœ€é«˜åˆ† {max_score}
    - æœ€ä½åˆ† {min_score}
    - å¹³å‡åˆ† {avg_score}
# è¯„ä¼°å‚è€ƒ
- ä»¥è¯„ä¼°åæ¬¡ä¸ºåŸºå‡†ï¼š
    - å½“å‡ºç°ä¸åŠäº”æ¬¡8.0åŠä»¥ä¸Šè¯„åˆ†æ—¶ï¼Œè¡¨ç¤ºè¯¥å¤§çº² "æ™®é€š"ï¼Œå¯¹åº”è¯„çº§ä¸ºBã€‚ 
    - å½“å‡ºç°è‡³å°‘äº”æ¬¡8.0åŠä»¥ä¸Šè¯„åˆ†æ—¶ï¼Œè¡¨ç¤ºè¯¥å¤§çº²å¯ "å»ºè®®å…³æ³¨"ï¼Œå¯¹åº”è¯„çº§ä¸ºAã€‚ 
    - å½“å‡ºç°è‡³å°‘å…«æ¬¡8.0åŠä»¥ä¸Šè¯„åˆ†æ—¶ï¼Œè¡¨ç¤ºè¯¥å¤§çº²å¯ "å¼ºçƒˆå…³æ³¨"ï¼Œå¯¹åº”è¯„çº§ä¸ºSã€‚
    - å½“å‡ºç°è‡³å°‘ä¸€æ¬¡8.5åŠä»¥ä¸Šè¯„åˆ†æ—¶ï¼Œæ— è®ºå…¶ä»–è¯„åˆ†å¦‚ä½•ï¼Œå‡è¡¨ç¤ºè¯¥å¤§çº²å¯ "å¼ºçƒˆå…³æ³¨"ï¼Œå¯¹åº”è¯„çº§ä¸ºSã€‚
"""
            }
            
            return analysis_result
            
        except Exception as e:
            self.logger.error(f"è¯„åˆ†åˆ†æå¤±è´¥: {str(e)}")
            return {
                "error": str(e),
                "scores": [],
                "analysis": "è¯„åˆ†åˆ†æå¤±è´¥"
            }
    
    async def _generate_document(
        self, 
        analysis_result: Dict[str, Any], 
        evaluation_results: List[str]
    ) -> Dict[str, Any]:
        """
        ç”Ÿæˆè¯„ä¼°æŠ¥å‘Šæ–‡æ¡£
        
        Args:
            analysis_result: åˆ†æç»“æœ
            evaluation_results: è¯„ä¼°ç»“æœåˆ—è¡¨
            
        Returns:
            Dict[str, Any]: æ–‡æ¡£ç”Ÿæˆç»“æœ
        """
        try:
            # æ„å»ºæ–‡æ¡£å†…å®¹
            document_content = analysis_result.get("evaluation_summary", "")
            
            # æ·»åŠ è¯¦ç»†è¯„ä¼°ç»“æœ
            for i, result in enumerate(evaluation_results, 1):
                document_content += f"\n## ç¬¬{i}æ¬¡æ‰§è¡Œç»“æœ: \n{result}\n"
            
            # æ¨¡æ‹Ÿæ–‡æ¡£ç”Ÿæˆ
            document_url = f"https://example.com/document/{int(time.time())}"
            document_title = f"æ•…äº‹å¤§çº²è¯„ä¼°æŠ¥å‘Š_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            
            document_result = {
                "url": document_url,
                "title": document_title,
                "content": document_content,
                "status": "success"
            }
            
            return document_result
            
        except Exception as e:
            self.logger.error(f"æ–‡æ¡£ç”Ÿæˆå¤±è´¥: {str(e)}")
            return {
                "error": str(e),
                "url": "",
                "title": "",
                "content": "",
                "status": "failed"
            }
    
    def get_tool_info(self) -> Dict[str, Any]:
        """è·å–å·¥å…·ä¿¡æ¯"""
        return {
            "tool_name": "story_outline_evaluation",
            "description": "æ•…äº‹å¤§çº²è¯„ä¼°ä¸åˆ†ææ™ºèƒ½ä½“",
            "function": "å¯¹æ•…äº‹å¤§çº²è¿›è¡Œå¤šç»´åº¦è¯„ä¼°å’Œåˆ†æ",
            "input_parameters": {
                "file": "str - æ•…äº‹å¤§çº²æ–‡ä»¶å†…å®¹",
                "theme": "str - æ•…äº‹é¢˜æç±»å‹",
                "length_size": "int - æ–‡æœ¬æˆªæ–­é•¿åº¦"
            },
            "output": {
                "analysis_result": "dict - è¯„åˆ†åˆ†æç»“æœ",
                "document_result": "dict - æ–‡æ¡£ç”Ÿæˆç»“æœ",
                "evaluation_results": "list - è¯¦ç»†è¯„ä¼°ç»“æœ",
                "theme": "str - æ•…äº‹é¢˜æç±»å‹",
                "total_rounds": "int - è¯„ä¼°è½®æ¬¡"
            },
            "workflow_steps": [
                "text_truncation - æ–‡æœ¬æˆªæ–­å¤„ç†",
                "multi_round_evaluation - å¤šè½®è¯„ä¼°",
                "score_analysis - è¯„åˆ†åˆ†æ",
                "document_generation - æ–‡æ¡£ç”Ÿæˆ"
            ],
            "evaluation_rounds": self.evaluation_rounds,
            "max_chunk_size": self.max_chunk_size
        }
