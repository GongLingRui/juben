"""
å¤šçº§ç¼“å­˜ç­–ç•¥

å®ç°ä¸‰çº§ç¼“å­˜æ¶æ„ï¼š
- L1: å†…å­˜ç¼“å­˜ (çƒ­ç‚¹æ•°æ®)
- L2: Redisç¼“å­˜ (ä¼šè¯æ•°æ®)
- L3: æ•°æ®åº“ (æŒä¹…åŒ–)
"""
import asyncio
import time
import json
import hashlib
from typing import Any, Optional, Dict, List, Callable, TypeVar
from dataclasses import dataclass, field
from collections import OrderedDict
from functools import wraps
import pickle

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.logger import JubenLogger
from utils.redis_client import get_redis_client
from utils.database_client import fetch_one, execute

logger = JubenLogger("multi_level_cache")


T = TypeVar('T')


@dataclass
class CacheEntry:
    """ç¼“å­˜æ¡ç›®"""
    value: Any
    created_at: float
    accessed_at: float
    access_count: int = 0
    ttl: Optional[float] = None

    def is_expired(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¿‡æœŸ"""
        if self.ttl is None:
            return False
        return time.time() - self.created_at > self.ttl

    def touch(self):
        """æ›´æ–°è®¿é—®æ—¶é—´å’Œæ¬¡æ•°"""
        self.accessed_at = time.time()
        self.access_count += 1


@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    # L1 å†…å­˜ç¼“å­˜é…ç½®
    l1_max_size: int = 1000
    l1_ttl: float = 300.0  # 5åˆ†é’Ÿ

    # L2 Redisç¼“å­˜é…ç½®
    l2_ttl: float = 3600.0  # 1å°æ—¶
    l2_prefix: str = "juben:cache:"

    # L3 æ•°æ®åº“é…ç½®
    l3_enabled: bool = True
    l3_table: str = "cache_store"

    # é€šç”¨é…ç½®
    key_prefix: str = ""
    serialize: str = "json"  # json æˆ– pickle


class L1MemoryCache:
    """
    L1 å†…å­˜ç¼“å­˜

    ç‰¹ç‚¹ï¼š
    - æœ€å¿«çš„è®¿é—®é€Ÿåº¦
    - å®¹é‡æœ‰é™
    - è¿›ç¨‹çº§åˆ«éš”ç¦»
    """

    def __init__(self, max_size: int = 1000, default_ttl: float = 300.0):
        """
        åˆå§‹åŒ–å†…å­˜ç¼“å­˜

        Args:
            max_size: æœ€å¤§ç¼“å­˜æ¡ç›®æ•°
            default_ttl: é»˜è®¤è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
        """
        self.max_size = max_size
        self.default_ttl = default_ttl

        # ä½¿ç”¨OrderedDictå®ç°LRU
        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            "hits": 0,
            "misses": 0,
            "evictions": 0,
            "expirations": 0
        }

    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        entry = self._cache.get(key)

        if entry is None:
            self._stats["misses"] += 1
            return None

        if entry.is_expired():
            del self._cache[key]
            self._stats["expirations"] += 1
            self._stats["misses"] += 1
            return None

        # æ›´æ–°è®¿é—®ä¿¡æ¯
        entry.touch()
        # ç§»åˆ°æœ«å°¾ï¼ˆLRUï¼‰
        self._cache.move_to_end(key)

        self._stats["hits"] += 1
        return entry.value

    def set(self, key: str, value: Any, ttl: Optional[float] = None):
        """è®¾ç½®ç¼“å­˜å€¼"""
        # å¦‚æœå·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
        if key in self._cache:
            del self._cache[key]

        # å¦‚æœè¶…è¿‡å®¹é‡ï¼Œæ·˜æ±°æœ€è€çš„æ¡ç›®
        if len(self._cache) >= self.max_size:
            oldest_key = next(iter(self._cache))
            del self._cache[oldest_key]
            self._stats["evictions"] += 1

        # æ·»åŠ æ–°æ¡ç›®
        ttl = ttl or self.default_ttl
        self._cache[key] = CacheEntry(
            value=value,
            created_at=time.time(),
            accessed_at=time.time(),
            ttl=ttl
        )

    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜å€¼"""
        if key in self._cache:
            del self._cache[key]
            return True
        return False

    def clear(self):
        """æ¸…ç©ºç¼“å­˜"""
        self._cache.clear()

    def cleanup_expired(self) -> int:
        """æ¸…ç†è¿‡æœŸæ¡ç›®"""
        expired_keys = [
            key for key, entry in self._cache.items()
            if entry.is_expired()
        ]

        for key in expired_keys:
            del self._cache[key]
            self._stats["expirations"] += 1

        return len(expired_keys)

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self._stats["hits"] + self._stats["misses"]
        hit_rate = self._stats["hits"] / total_requests if total_requests > 0 else 0

        return {
            **self._stats,
            "size": len(self._cache),
            "max_size": self.max_size,
            "hit_rate": round(hit_rate, 4)
        }


class L2RedisCache:
    """
    L2 Redisç¼“å­˜

    ç‰¹ç‚¹ï¼š
    - è·¨è¿›ç¨‹å…±äº«
    - å®¹é‡å¤§
    - æ”¯æŒè¿‡æœŸæ—¶é—´
    - æŒä¹…åŒ–å¯é€‰
    """

    def __init__(
        self,
        ttl: float = 3600.0,
        prefix: str = "juben:cache:"
    ):
        """
        åˆå§‹åŒ–Redisç¼“å­˜

        Args:
            ttl: é»˜è®¤è¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
            prefix: é”®å‰ç¼€
        """
        self.ttl = ttl
        self.prefix = prefix
        self._redis_client = None

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            "hits": 0,
            "misses": 0,
            "errors": 0
        }

    async def _get_client(self):
        """è·å–Rediså®¢æˆ·ç«¯"""
        if self._redis_client is None:
            self._redis_client = await get_redis_client()
        return self._redis_client

    def _make_key(self, key: str) -> str:
        """ç”ŸæˆRedisé”®"""
        return f"{self.prefix}{key}"

    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        try:
            client = await self._get_client()
            redis_key = self._make_key(key)
            value = await client.get(redis_key)

            if value is None:
                self._stats["misses"] += 1
                return None

            self._stats["hits"] += 1
            return pickle.loads(value)

        except Exception as e:
            logger.error(f"âŒ Redis GET é”™è¯¯: {e}")
            self._stats["errors"] += 1
            return None

    async def set(self, key: str, value: Any, ttl: Optional[float] = None):
        """è®¾ç½®ç¼“å­˜å€¼"""
        try:
            client = await self._get_client()
            redis_key = self._make_key(key)
            serialized = pickle.dumps(value)
            ttl = ttl or self.ttl

            await client.set(redis_key, serialized, ex=int(ttl))

        except Exception as e:
            logger.error(f"âŒ Redis SET é”™è¯¯: {e}")
            self._stats["errors"] += 1

    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜å€¼"""
        try:
            client = await self._get_client()
            redis_key = self._make_key(key)
            result = await client.delete(redis_key)
            return result > 0

        except Exception as e:
            logger.error(f"âŒ Redis DELETE é”™è¯¯: {e}")
            return False

    async def clear(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜ï¼ˆåªåˆ é™¤å¸¦æœ‰å‰ç¼€çš„é”®ï¼‰"""
        try:
            client = await self._get_client()
            pattern = f"{self.prefix}*"
            keys = []

            async for key in client.scan_iter(match=pattern):
                keys.append(key)

            if keys:
                await client.delete(*keys)

        except Exception as e:
            logger.error(f"âŒ Redis CLEAR é”™è¯¯: {e}")

    async def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total_requests = self._stats["hits"] + self._stats["misses"]
        hit_rate = self._stats["hits"] / total_requests if total_requests > 0 else 0

        return {
            **self._stats,
            "hit_rate": round(hit_rate, 4)
        }


class L3DatabaseCache:
    """
    L3 æ•°æ®åº“ç¼“å­˜

    ç‰¹ç‚¹ï¼š
    - æŒä¹…åŒ–å­˜å‚¨
    - å®¹é‡æœ€å¤§
    - è®¿é—®æœ€æ…¢
    - ç”¨äºå†·æ•°æ®
    """

    def __init__(self, enabled: bool = True, table: str = "cache_store"):
        """
        åˆå§‹åŒ–æ•°æ®åº“ç¼“å­˜

        Args:
            enabled: æ˜¯å¦å¯ç”¨
            table: ç¼“å­˜è¡¨å
        """
        self.enabled = enabled
        self.table = table

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            "hits": 0,
            "misses": 0,
            "errors": 0
        }

    async def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        if not self.enabled:
            return None

        try:
            row = await fetch_one(f"SELECT value FROM {self.table} WHERE key = $1", key)

            if row:
                self._stats["hits"] += 1
                return pickle.loads(row["value"])

            self._stats["misses"] += 1
            return None

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ GET é”™è¯¯: {e}")
            self._stats["errors"] += 1
            return None

    async def set(self, key: str, value: Any):
        """è®¾ç½®ç¼“å­˜å€¼"""
        if not self.enabled:
            return

        try:
            serialized = pickle.dumps(value)
            await execute(
                f\"\"\"
                INSERT INTO {self.table} (key, value, updated_at)
                VALUES ($1, $2, NOW())
                ON CONFLICT (key) DO UPDATE SET value = EXCLUDED.value, updated_at = NOW()
                \"\"\",
                key,
                serialized,
            )

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ SET é”™è¯¯: {e}")
            self._stats["errors"] += 1

    async def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜å€¼"""
        if not self.enabled:
            return False

        try:
            row = await fetch_one(f\"DELETE FROM {self.table} WHERE key = $1 RETURNING key\", key)
            return bool(row)

        except Exception as e:
            logger.error(f"âŒ æ•°æ®åº“ DELETE é”™è¯¯: {e}")
            return False

    async def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return self._stats.copy()


class MultiLevelCache:
    """
    å¤šçº§ç¼“å­˜ç®¡ç†å™¨

    ç¼“å­˜ç­–ç•¥ï¼š
    1. è¯»å–æ—¶ï¼šL1 -> L2 -> L3 -> æºæ•°æ®
    2. å†™å…¥æ—¶ï¼šåŒæ—¶å†™å…¥L1ã€L2ã€L3
    3. æ·˜æ±°æ—¶ï¼šä½¿ç”¨LRUç­–ç•¥
    """

    def __init__(self, config: Optional[CacheConfig] = None):
        """
        åˆå§‹åŒ–å¤šçº§ç¼“å­˜

        Args:
            config: ç¼“å­˜é…ç½®
        """
        self.config = config or CacheConfig()

        # åˆå§‹åŒ–å„çº§ç¼“å­˜
        self.l1 = L1MemoryCache(
            max_size=self.config.l1_max_size,
            default_ttl=self.config.l1_ttl
        )
        self.l2 = L2RedisCache(
            ttl=self.config.l2_ttl,
            prefix=self.config.l2_prefix
        )
        self.l3 = L3DatabaseCache(
            enabled=self.config.l3_enabled,
            table=self.config.l3_table
        )

        logger.info("âœ… å¤šçº§ç¼“å­˜åˆå§‹åŒ–å®Œæˆ")

    def _make_key(self, key: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        if self.config.key_prefix:
            return f"{self.config.key_prefix}:{key}"
        return key

    async def get(self, key: str) -> Optional[Any]:
        """
        è·å–ç¼“å­˜å€¼ï¼ˆé€çº§æŸ¥æ‰¾ï¼‰

        Args:
            key: ç¼“å­˜é”®

        Returns:
            ç¼“å­˜å€¼æˆ–None
        """
        cache_key = self._make_key(key)

        # L1: å†…å­˜ç¼“å­˜
        value = self.l1.get(cache_key)
        if value is not None:
            logger.debug(f"âœ… L1 å‘½ä¸­: {key}")
            return value

        # L2: Redisç¼“å­˜
        value = await self.l2.get(cache_key)
        if value is not None:
            logger.debug(f"âœ… L2 å‘½ä¸­: {key}")
            # å›å¡«L1
            self.l1.set(cache_key, value, self.config.l1_ttl)
            return value

        # L3: æ•°æ®åº“ç¼“å­˜
        value = await self.l3.get(cache_key)
        if value is not None:
            logger.debug(f"âœ… L3 å‘½ä¸­: {key}")
            # å›å¡«L1å’ŒL2
            self.l1.set(cache_key, value, self.config.l1_ttl)
            await self.l2.set(cache_key, value, self.config.l2_ttl)
            return value

        logger.debug(f"âŒ æœªå‘½ä¸­: {key}")
        return None

    async def set(
        self,
        key: str,
        value: Any,
        l1_ttl: Optional[float] = None,
        l2_ttl: Optional[float] = None
    ):
        """
        è®¾ç½®ç¼“å­˜å€¼ï¼ˆå†™å…¥æ‰€æœ‰çº§åˆ«ï¼‰

        Args:
            key: ç¼“å­˜é”®
            value: ç¼“å­˜å€¼
            l1_ttl: L1è¿‡æœŸæ—¶é—´
            l2_ttl: L2è¿‡æœŸæ—¶é—´
        """
        cache_key = self._make_key(key)

        # å†™å…¥L1
        self.l1.set(cache_key, value, l1_ttl or self.config.l1_ttl)

        # å†™å…¥L2
        await self.l2.set(cache_key, value, l2_ttl or self.config.l2_ttl)

        # å†™å…¥L3
        await self.l3.set(cache_key, value)

    async def delete(self, key: str) -> bool:
        """
        åˆ é™¤ç¼“å­˜å€¼ï¼ˆä»æ‰€æœ‰çº§åˆ«åˆ é™¤ï¼‰

        Args:
            key: ç¼“å­˜é”®

        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        cache_key = self._make_key(key)

        # ä»L1åˆ é™¤
        self.l1.delete(cache_key)

        # ä»L2åˆ é™¤
        await self.l2.delete(cache_key)

        # ä»L3åˆ é™¤
        return await self.l3.delete(cache_key)

    async def clear(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self.l1.clear()
        await self.l2.clear()
        # L3ä¸æ¸…ç©ºï¼Œé€šå¸¸ä¸éœ€è¦

    async def get_stats(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰çº§åˆ«çš„ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "l1": self.l1.get_stats(),
            "l2": await self.l2.get_stats(),
            "l3": await self.l3.get_stats()
        }

    async def cleanup(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        l1_cleaned = self.l1.cleanup_expired()
        logger.info(f"ğŸ§¹ æ¸…ç†äº† {l1_cleaned} ä¸ªL1è¿‡æœŸç¼“å­˜")


# å…¨å±€ç¼“å­˜å®ä¾‹
_global_cache: Optional[MultiLevelCache] = None


def get_cache(config: Optional[CacheConfig] = None) -> MultiLevelCache:
    """
    è·å–å…¨å±€ç¼“å­˜å®ä¾‹

    Args:
        config: ç¼“å­˜é…ç½®

    Returns:
        MultiLevelCache: ç¼“å­˜å®ä¾‹
    """
    global _global_cache

    if _global_cache is None:
        if config is None:
            from utils.cache_policy import get_cache_config
            config = get_cache_config()
        _global_cache = MultiLevelCache(config)

    return _global_cache


# ç¼“å­˜è£…é¥°å™¨
def cached(
    key_func: Optional[Callable[..., str]] = None,
    l1_ttl: float = 300.0,
    l2_ttl: float = 3600.0
):
    """
    ç¼“å­˜è£…é¥°å™¨

    ç”¨æ³•:
    ```python
    @cached(l1_ttl=60, l2_ttl=300)
    async def expensive_function(param1, param2):
        return await compute_something(param1, param2)
    ```

    Args:
        key_func: ç”Ÿæˆç¼“å­˜é”®çš„å‡½æ•°
        l1_ttl: L1ç¼“å­˜æ—¶é—´
        l2_ttl: L2ç¼“å­˜æ—¶é—´
    """
    def decorator(func: Callable[..., Awaitable[T]]) -> Callable[..., Awaitable[T]]:
        @wraps(func)
        async def wrapper(*args, **kwargs) -> T:
            # ç”Ÿæˆç¼“å­˜é”®
            if key_func:
                cache_key = key_func(*args, **kwargs)
            else:
                # é»˜è®¤ä½¿ç”¨å‡½æ•°åå’Œå‚æ•°
                key_parts = [func.__name__]
                key_parts.extend(str(arg) for arg in args)
                key_parts.extend(f"{k}={v}" for k, v in sorted(kwargs.items()))
                cache_key = ":".join(key_parts)

            # å°è¯•ä»ç¼“å­˜è·å–
            cache = get_cache()
            cached_value = await cache.get(cache_key)

            if cached_value is not None:
                return cached_value

            # æ‰§è¡Œå‡½æ•°
            result = await func(*args, **kwargs)

            # å†™å…¥ç¼“å­˜
            await cache.set(cache_key, result, l1_ttl=l1_ttl, l2_ttl=l2_ttl)

            return result

        return wrapper
    return decorator


# ç¼“å­˜é”®ç”Ÿæˆå™¨
def generate_cache_key(*parts: str, **kwargs) -> str:
    """
    ç”Ÿæˆç¼“å­˜é”®

    Args:
        *parts: é”®çš„éƒ¨åˆ†
        **kwargs: é¢å¤–å‚æ•°

    Returns:
        str: ç¼“å­˜é”®
    """
    key = ":".join(str(p) for p in parts)

    if kwargs:
        sorted_kwargs = sorted(kwargs.items())
        query_string = "&".join(f"{k}={v}" for k, v in sorted_kwargs)
        key = f"{key}?{query_string}"

    # å¦‚æœå¤ªé•¿ï¼Œä½¿ç”¨å“ˆå¸Œ
    if len(key) > 200:
        key = hashlib.md5(key.encode()).hexdigest()

    return key


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_multi_level_cache():
        """æµ‹è¯•å¤šçº§ç¼“å­˜"""
        cache = MultiLevelCache(
            CacheConfig(
                l1_max_size=100,
                l1_ttl=60.0,
                l2_ttl=300.0
            )
        )

        # æµ‹è¯•å†™å…¥å’Œè¯»å–
        await cache.set("test_key", {"data": "æµ‹è¯•æ•°æ®"})
        value = await cache.get("test_key")
        logger.info(f"è¯»å–ç¼“å­˜: {value}")

        # æµ‹è¯•ç»Ÿè®¡ä¿¡æ¯
        stats = await cache.get_stats()
        logger.info("\n=== ç¼“å­˜ç»Ÿè®¡ ===")
        logger.info(json.dumps(stats, indent=2, ensure_ascii=False))

        # æµ‹è¯•æ¸…ç†
        await cache.cleanup()

    asyncio.run(test_multi_level_cache())
