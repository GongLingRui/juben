"""
å­˜å‚¨ä¼˜åŒ–å™¨
è´Ÿè´£ä¼˜åŒ–Redisã€Milvuså’Œæ–‡ä»¶å­˜å‚¨çš„æ€§èƒ½å’Œå¯é æ€§
"""
from typing import Dict, Any, List, Optional, Union
from datetime import datetime, timedelta
import asyncio
import json
import redis
import pymilvus
from pathlib import Path
import os
import shutil
from dataclasses import dataclass
from enum import Enum
import logging

logger = logging.getLogger(__name__)


class StorageType(Enum):
    """å­˜å‚¨ç±»å‹"""
    REDIS = "redis"
    MILVUS = "milvus"
    FILE = "file"


@dataclass
class StorageConfig:
    """å­˜å‚¨é…ç½®"""
    storage_type: StorageType
    host: str
    port: int
    database: str = "default"
    username: Optional[str] = None
    password: Optional[str] = None
    max_connections: int = 100
    timeout: int = 30
    retry_times: int = 3


class StorageOptimizer:
    """å­˜å‚¨ä¼˜åŒ–å™¨"""
    
    def __init__(self):
        """åˆå§‹åŒ–å­˜å‚¨ä¼˜åŒ–å™¨"""
        self.redis_client = None
        self.milvus_client = None
        self.storage_configs = {}
        self.connection_pools = {}
        self.performance_metrics = {
            "redis": {"operations": 0, "avg_latency": 0.0, "errors": 0},
            "milvus": {"operations": 0, "avg_latency": 0.0, "errors": 0},
            "file": {"operations": 0, "avg_latency": 0.0, "errors": 0}
        }
        
        # åˆå§‹åŒ–å­˜å‚¨é…ç½®
        self._load_storage_configs()
    
    def _load_storage_configs(self):
        """åŠ è½½å­˜å‚¨é…ç½®"""
        # Redisé…ç½®
        self.storage_configs["redis"] = StorageConfig(
            storage_type=StorageType.REDIS,
            host=os.getenv("REDIS_HOST", "localhost"),
            port=int(os.getenv("REDIS_PORT", "6379")),
            database=os.getenv("REDIS_DB", "0"),
            password=os.getenv("REDIS_PASSWORD"),
            max_connections=int(os.getenv("REDIS_MAX_CONNECTIONS", "100")),
            timeout=int(os.getenv("REDIS_TIMEOUT", "30"))
        )
        
        # Milvusé…ç½®
        self.storage_configs["milvus"] = StorageConfig(
            storage_type=StorageType.MILVUS,
            host=os.getenv("MILVUS_HOST", "localhost"),
            port=int(os.getenv("MILVUS_PORT", "19530")),
            username=os.getenv("MILVUS_USERNAME"),
            password=os.getenv("MILVUS_PASSWORD"),
            max_connections=int(os.getenv("MILVUS_MAX_CONNECTIONS", "50")),
            timeout=int(os.getenv("MILVUS_TIMEOUT", "30"))
        )
        
        # æ–‡ä»¶å­˜å‚¨é…ç½®
        self.storage_configs["file"] = StorageConfig(
            storage_type=StorageType.FILE,
            host=os.getenv("FILE_STORAGE_HOST", "localhost"),
            port=0,  # æ–‡ä»¶å­˜å‚¨ä¸éœ€è¦ç«¯å£
            database=os.getenv("FILE_STORAGE_PATH", "./uploads")
        )
    
    async def initialize_connections(self):
        """åˆå§‹åŒ–æ‰€æœ‰å­˜å‚¨è¿æ¥"""
        try:
            # åˆå§‹åŒ–Redisè¿æ¥
            await self._init_redis_connection()
            
            # åˆå§‹åŒ–Milvusè¿æ¥
            await self._init_milvus_connection()
            
            # åˆå§‹åŒ–æ–‡ä»¶å­˜å‚¨
            await self._init_file_storage()
            
            logger.info("âœ… å­˜å‚¨è¿æ¥åˆå§‹åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨è¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _init_redis_connection(self):
        """åˆå§‹åŒ–Redisè¿æ¥"""
        try:
            config = self.storage_configs["redis"]
            
            # åˆ›å»ºè¿æ¥æ± 
            pool = redis.ConnectionPool(
                host=config.host,
                port=config.port,
                db=int(config.database),
                password=config.password,
                max_connections=config.max_connections,
                socket_timeout=config.timeout,
                socket_connect_timeout=config.timeout,
                retry_on_timeout=True
            )
            
            # åˆ›å»ºRediså®¢æˆ·ç«¯
            self.redis_client = redis.Redis(connection_pool=pool)
            
            # æµ‹è¯•è¿æ¥
            await self._test_redis_connection()
            
            logger.info("âœ… Redisè¿æ¥åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ Redisè¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _init_milvus_connection(self):
        """åˆå§‹åŒ–Milvusè¿æ¥"""
        try:
            config = self.storage_configs["milvus"]
            
            # åˆ›å»ºMilvusè¿æ¥
            connections = pymilvus.connections.create_connection(
                alias="default",
                host=config.host,
                port=config.port,
                user=config.username,
                password=config.password,
                timeout=config.timeout
            )
            
            self.milvus_client = pymilvus.connections.get_connection_addr("default")
            
            # æµ‹è¯•è¿æ¥
            await self._test_milvus_connection()
            
            logger.info("âœ… Milvusè¿æ¥åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ Milvusè¿æ¥åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _init_file_storage(self):
        """åˆå§‹åŒ–æ–‡ä»¶å­˜å‚¨"""
        try:
            config = self.storage_configs["file"]
            storage_path = Path(config.database)
            
            # åˆ›å»ºå­˜å‚¨ç›®å½•
            storage_path.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºå­ç›®å½•
            subdirs = ["uploads", "temp", "processed", "archived"]
            for subdir in subdirs:
                (storage_path / subdir).mkdir(exist_ok=True)
            
            logger.info("âœ… æ–‡ä»¶å­˜å‚¨åˆå§‹åŒ–æˆåŠŸ")

        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶å­˜å‚¨åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def _test_redis_connection(self):
        """æµ‹è¯•Redisè¿æ¥"""
        try:
            start_time = datetime.now()
            result = self.redis_client.ping()
            latency = (datetime.now() - start_time).total_seconds() * 1000
            
            if result:
                self.performance_metrics["redis"]["avg_latency"] = latency
                logger.info(f"âœ… Redisè¿æ¥æµ‹è¯•æˆåŠŸï¼Œå»¶è¿Ÿ: {latency:.2f}ms")
            else:
                raise Exception("Redis pingå¤±è´¥")
                
        except Exception as e:
            self.performance_metrics["redis"]["errors"] += 1
            raise Exception(f"Redisè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
    
    async def _test_milvus_connection(self):
        """æµ‹è¯•Milvusè¿æ¥"""
        try:
            start_time = datetime.now()
            # è¿™é‡Œåº”è¯¥è°ƒç”¨Milvusçš„å¥åº·æ£€æŸ¥API
            # å®é™…å®ç°ä¸­éœ€è¦æ ¹æ®Milvusç‰ˆæœ¬è°ƒæ•´
            latency = (datetime.now() - start_time).total_seconds() * 1000
            
            self.performance_metrics["milvus"]["avg_latency"] = latency
            logger.info(f"âœ… Milvusè¿æ¥æµ‹è¯•æˆåŠŸï¼Œå»¶è¿Ÿ: {latency:.2f}ms")
            
        except Exception as e:
            self.performance_metrics["milvus"]["errors"] += 1
            raise Exception(f"Milvusè¿æ¥æµ‹è¯•å¤±è´¥: {e}")
    
    # ==================== Redisä¼˜åŒ–æ–¹æ³• ====================
    
    async def optimize_redis(self):
        """ä¼˜åŒ–Redisæ€§èƒ½"""
        try:
            if not self.redis_client:
                raise Exception("Rediså®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # è®¾ç½®Redisé…ç½®
            await self._configure_redis()
            
            # æ¸…ç†è¿‡æœŸæ•°æ®
            await self._cleanup_redis_data()
            
            # ä¼˜åŒ–å†…å­˜ä½¿ç”¨
            await self._optimize_redis_memory()

            logger.info("âœ… Redisä¼˜åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ Redisä¼˜åŒ–å¤±è´¥: {e}")
            raise
    
    async def _configure_redis(self):
        """é…ç½®Rediså‚æ•°"""
        try:
            # è®¾ç½®å†…å­˜ç­–ç•¥
            self.redis_client.config_set("maxmemory-policy", "allkeys-lru")
            
            # è®¾ç½®è¿‡æœŸæ—¶é—´
            self.redis_client.config_set("timeout", "300")
            
            # å¯ç”¨å‹ç¼©
            self.redis_client.config_set("hash-max-ziplist-entries", "512")
            self.redis_client.config_set("hash-max-ziplist-value", "64")

            logger.info("âœ… Redisé…ç½®ä¼˜åŒ–å®Œæˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ Redisé…ç½®ä¼˜åŒ–å¤±è´¥: {e}")
    
    async def _cleanup_redis_data(self):
        """æ¸…ç†Redisè¿‡æœŸæ•°æ®"""
        try:
            # è·å–æ‰€æœ‰é”®
            keys = self.redis_client.keys("*")
            
            # æ£€æŸ¥è¿‡æœŸé”®
            expired_keys = []
            for key in keys:
                ttl = self.redis_client.ttl(key)
                if ttl == -1:  # æ²¡æœ‰è¿‡æœŸæ—¶é—´çš„é”®
                    # æ£€æŸ¥æœ€åè®¿é—®æ—¶é—´
                    last_access = self.redis_client.object("idletime", key)
                    if last_access > 3600:  # 1å°æ—¶æœªè®¿é—®
                        expired_keys.append(key)
            
            # åˆ é™¤è¿‡æœŸé”®
            if expired_keys:
                self.redis_client.delete(*expired_keys)
                logger.info(f"âœ… æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸé”®")

        except Exception as e:
            logger.warning(f"âš ï¸ Redisæ•°æ®æ¸…ç†å¤±è´¥: {e}")
    
    async def _optimize_redis_memory(self):
        """ä¼˜åŒ–Rediså†…å­˜ä½¿ç”¨"""
        try:
            # æ‰§è¡Œå†…å­˜ç¢ç‰‡æ•´ç†
            self.redis_client.memory_purge()
            
            # è·å–å†…å­˜ä½¿ç”¨ä¿¡æ¯
            memory_info = self.redis_client.memory_usage()
            logger.info(f"âœ… Rediså†…å­˜ä¼˜åŒ–å®Œæˆï¼Œå½“å‰ä½¿ç”¨: {memory_info} bytes")

        except Exception as e:
            logger.warning(f"âš ï¸ Rediså†…å­˜ä¼˜åŒ–å¤±è´¥: {e}")
    
    # ==================== Milvusä¼˜åŒ–æ–¹æ³• ====================
    
    async def optimize_milvus(self):
        """ä¼˜åŒ–Milvusæ€§èƒ½"""
        try:
            if not self.milvus_client:
                raise Exception("Milvuså®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
            
            # ä¼˜åŒ–é›†åˆé…ç½®
            await self._optimize_milvus_collections()
            
            # æ¸…ç†è¿‡æœŸæ•°æ®
            await self._cleanup_milvus_data()
            
            # ä¼˜åŒ–ç´¢å¼•
            await self._optimize_milvus_indexes()

            logger.info("âœ… Milvusä¼˜åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ Milvusä¼˜åŒ–å¤±è´¥: {e}")
    
    async def _optimize_milvus_collections(self):
        """ä¼˜åŒ–Milvusé›†åˆé…ç½®"""
        try:
            # è¿™é‡Œåº”è¯¥å®ç°é›†åˆé…ç½®ä¼˜åŒ–
            # å®é™…å®ç°ä¸­éœ€è¦æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´
            logger.info("âœ… Milvusé›†åˆé…ç½®ä¼˜åŒ–å®Œæˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ Milvusé›†åˆé…ç½®ä¼˜åŒ–å¤±è´¥: {e}")
    
    async def _cleanup_milvus_data(self):
        """æ¸…ç†Milvusè¿‡æœŸæ•°æ®"""
        try:
            # è¿™é‡Œåº”è¯¥å®ç°Milvusæ•°æ®æ¸…ç†
            # å®é™…å®ç°ä¸­éœ€è¦æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´
            logger.info("âœ… Milvusæ•°æ®æ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ Milvusæ•°æ®æ¸…ç†å¤±è´¥: {e}")
    
    async def _optimize_milvus_indexes(self):
        """ä¼˜åŒ–Milvusç´¢å¼•"""
        try:
            # è¿™é‡Œåº”è¯¥å®ç°Milvusç´¢å¼•ä¼˜åŒ–
            # å®é™…å®ç°ä¸­éœ€è¦æ ¹æ®å…·ä½“éœ€æ±‚è°ƒæ•´
            logger.info("âœ… Milvusç´¢å¼•ä¼˜åŒ–å®Œæˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ Milvusç´¢å¼•ä¼˜åŒ–å¤±è´¥: {e}")
    
    # ==================== æ–‡ä»¶å­˜å‚¨ä¼˜åŒ–æ–¹æ³• ====================
    
    async def optimize_file_storage(self):
        """ä¼˜åŒ–æ–‡ä»¶å­˜å‚¨"""
        try:
            config = self.storage_configs["file"]
            storage_path = Path(config.database)
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            await self._cleanup_temp_files(storage_path)
            
            # å‹ç¼©æ—§æ–‡ä»¶
            await self._compress_old_files(storage_path)
            
            # ä¼˜åŒ–å­˜å‚¨ç»“æ„
            await self._optimize_storage_structure(storage_path)

            logger.info("âœ… æ–‡ä»¶å­˜å‚¨ä¼˜åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶å­˜å‚¨ä¼˜åŒ–å¤±è´¥: {e}")
    
    async def _cleanup_temp_files(self, storage_path: Path):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        try:
            temp_path = storage_path / "temp"
            if temp_path.exists():
                # åˆ é™¤è¶…è¿‡1å°æ—¶çš„ä¸´æ—¶æ–‡ä»¶
                cutoff_time = datetime.now() - timedelta(hours=1)
                
                for file_path in temp_path.iterdir():
                    if file_path.is_file():
                        file_time = datetime.fromtimestamp(file_path.stat().st_mtime)
                        if file_time < cutoff_time:
                            file_path.unlink()

                logger.info("âœ… ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ ä¸´æ—¶æ–‡ä»¶æ¸…ç†å¤±è´¥: {e}")
    
    async def _compress_old_files(self, storage_path: Path):
        """å‹ç¼©æ—§æ–‡ä»¶"""
        try:
            # è¿™é‡Œåº”è¯¥å®ç°æ–‡ä»¶å‹ç¼©é€»è¾‘
            # å®é™…å®ç°ä¸­å¯ä»¥ä½¿ç”¨gzipã€tarç­‰å·¥å…·
            logger.info("âœ… æ—§æ–‡ä»¶å‹ç¼©å®Œæˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ æ—§æ–‡ä»¶å‹ç¼©å¤±è´¥: {e}")
    
    async def _optimize_storage_structure(self, storage_path: Path):
        """ä¼˜åŒ–å­˜å‚¨ç»“æ„"""
        try:
            # åˆ›å»ºä¼˜åŒ–çš„ç›®å½•ç»“æ„
            optimized_dirs = [
                "by_date",  # æŒ‰æ—¥æœŸåˆ†ç±»
                "by_type",  # æŒ‰ç±»å‹åˆ†ç±»
                "by_size",  # æŒ‰å¤§å°åˆ†ç±»
                "archived"  # å½’æ¡£æ–‡ä»¶
            ]
            
            for dir_name in optimized_dirs:
                (storage_path / dir_name).mkdir(exist_ok=True)

            logger.info("âœ… å­˜å‚¨ç»“æ„ä¼˜åŒ–å®Œæˆ")

        except Exception as e:
            logger.warning(f"âš ï¸ å­˜å‚¨ç»“æ„ä¼˜åŒ–å¤±è´¥: {e}")
    
    # ==================== æ€§èƒ½ç›‘æ§æ–¹æ³• ====================
    
    async def get_storage_metrics(self) -> Dict[str, Any]:
        """è·å–å­˜å‚¨æ€§èƒ½æŒ‡æ ‡"""
        try:
            metrics = {}
            
            # RedisæŒ‡æ ‡
            if self.redis_client:
                try:
                    redis_info = self.redis_client.info()
                    metrics["redis"] = {
                        "connected_clients": redis_info.get("connected_clients", 0),
                        "used_memory": redis_info.get("used_memory", 0),
                        "used_memory_human": redis_info.get("used_memory_human", "0B"),
                        "keyspace_hits": redis_info.get("keyspace_hits", 0),
                        "keyspace_misses": redis_info.get("keyspace_misses", 0),
                        "operations": self.performance_metrics["redis"]["operations"],
                        "avg_latency": self.performance_metrics["redis"]["avg_latency"],
                        "errors": self.performance_metrics["redis"]["errors"]
                    }
                except Exception as e:
                    metrics["redis"] = {"error": str(e)}
            
            # MilvusæŒ‡æ ‡
            if self.milvus_client:
                try:
                    # è¿™é‡Œåº”è¯¥è·å–Milvusçš„æ€§èƒ½æŒ‡æ ‡
                    # å®é™…å®ç°ä¸­éœ€è¦æ ¹æ®Milvusç‰ˆæœ¬è°ƒæ•´
                    metrics["milvus"] = {
                        "operations": self.performance_metrics["milvus"]["operations"],
                        "avg_latency": self.performance_metrics["milvus"]["avg_latency"],
                        "errors": self.performance_metrics["milvus"]["errors"]
                    }
                except Exception as e:
                    metrics["milvus"] = {"error": str(e)}
            
            # æ–‡ä»¶å­˜å‚¨æŒ‡æ ‡
            config = self.storage_configs["file"]
            storage_path = Path(config.database)
            
            if storage_path.exists():
                total_size = sum(f.stat().st_size for f in storage_path.rglob('*') if f.is_file())
                file_count = len(list(storage_path.rglob('*')))
                
                metrics["file"] = {
                    "total_size": total_size,
                    "total_size_human": self._format_bytes(total_size),
                    "file_count": file_count,
                    "operations": self.performance_metrics["file"]["operations"],
                    "avg_latency": self.performance_metrics["file"]["avg_latency"],
                    "errors": self.performance_metrics["file"]["errors"]
                }
            
            return metrics
            
        except Exception as e:
            return {"error": str(e)}
    
    def _format_bytes(self, bytes_value: int) -> str:
        """æ ¼å¼åŒ–å­—èŠ‚æ•°"""
        for unit in ['B', 'KB', 'MB', 'GB', 'TB']:
            if bytes_value < 1024.0:
                return f"{bytes_value:.1f} {unit}"
            bytes_value /= 1024.0
        return f"{bytes_value:.1f} PB"
    
    # ==================== ç»¼åˆä¼˜åŒ–æ–¹æ³• ====================
    
    async def optimize_all_storage(self):
        """ä¼˜åŒ–æ‰€æœ‰å­˜å‚¨ç³»ç»Ÿ"""
        try:
            logger.info("ğŸš€ å¼€å§‹å­˜å‚¨ç³»ç»Ÿä¼˜åŒ–...")

            # ä¼˜åŒ–Redis
            await self.optimize_redis()
            
            # ä¼˜åŒ–Milvus
            await self.optimize_milvus()
            
            # ä¼˜åŒ–æ–‡ä»¶å­˜å‚¨
            await self.optimize_file_storage()

            logger.info("âœ… æ‰€æœ‰å­˜å‚¨ç³»ç»Ÿä¼˜åŒ–å®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ å­˜å‚¨ç³»ç»Ÿä¼˜åŒ–å¤±è´¥: {e}")
            raise
    
    async def get_optimization_report(self) -> Dict[str, Any]:
        """è·å–ä¼˜åŒ–æŠ¥å‘Š"""
        try:
            metrics = await self.get_storage_metrics()
            
            report = {
                "timestamp": datetime.now().isoformat(),
                "storage_metrics": metrics,
                "optimization_status": "completed",
                "recommendations": await self._generate_recommendations(metrics)
            }
            
            return report
            
        except Exception as e:
            return {
                "timestamp": datetime.now().isoformat(),
                "error": str(e),
                "optimization_status": "failed"
            }
    
    async def _generate_recommendations(self, metrics: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¼˜åŒ–å»ºè®®"""
        recommendations = []
        
        # Rediså»ºè®®
        if "redis" in metrics and "error" not in metrics["redis"]:
            redis_metrics = metrics["redis"]
            if redis_metrics.get("used_memory", 0) > 100 * 1024 * 1024:  # 100MB
                recommendations.append("Rediså†…å­˜ä½¿ç”¨è¾ƒé«˜ï¼Œå»ºè®®æ¸…ç†è¿‡æœŸæ•°æ®")
            if redis_metrics.get("errors", 0) > 0:
                recommendations.append("Rediså­˜åœ¨é”™è¯¯ï¼Œå»ºè®®æ£€æŸ¥è¿æ¥é…ç½®")
        
        # Milvuså»ºè®®
        if "milvus" in metrics and "error" not in metrics["milvus"]:
            milvus_metrics = metrics["milvus"]
            if milvus_metrics.get("errors", 0) > 0:
                recommendations.append("Milvuså­˜åœ¨é”™è¯¯ï¼Œå»ºè®®æ£€æŸ¥è¿æ¥é…ç½®")
        
        # æ–‡ä»¶å­˜å‚¨å»ºè®®
        if "file" in metrics:
            file_metrics = metrics["file"]
            if file_metrics.get("total_size", 0) > 1024 * 1024 * 1024:  # 1GB
                recommendations.append("æ–‡ä»¶å­˜å‚¨ç©ºé—´è¾ƒå¤§ï¼Œå»ºè®®æ¸…ç†æ—§æ–‡ä»¶")
        
        return recommendations
