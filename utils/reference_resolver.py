"""
Jubenæ™ºèƒ½å¼•ç”¨è§£æå™¨
æä¾›æ™ºèƒ½å¼•ç”¨è§£æåŠŸèƒ½å’Œæ™ºèƒ½ç‰‡æ®µè¯»å–
"""
import re
import asyncio
import os
import json
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
from pathlib import Path
import numpy as np

try:
    from .logger import JubenLogger
    from .notes_manager import get_notes_manager
    from .storage_manager import get_storage
    from .project_manager import ProjectManager
    from .aliyun_embedding_client import aliyun_embedding_client
    from .milvus_client import get_milvus_client
except ImportError:
    import sys
    from pathlib import Path
    sys.path.insert(0, str(Path(__file__).parent.parent))

    from utils.logger import JubenLogger
    from utils.notes_manager import get_notes_manager
    from utils.storage_manager import get_storage
    from utils.project_manager import ProjectManager
    from utils.aliyun_embedding_client import aliyun_embedding_client
    from utils.milvus_client import get_milvus_client


class JubenReferenceResolver:
    """
    Jubenæ™ºèƒ½å¼•ç”¨è§£æå™¨

    æ ¸å¿ƒåŠŸèƒ½ï¼š
    1. è§£ææ–‡æœ¬ä¸­çš„å¼•ç”¨æ ‡è®°
    2. æ™ºèƒ½åŒ¹é…Noteså’Œä¸Šä¸‹æ–‡
    3. ç”Ÿæˆå¼•ç”¨é“¾æ¥å’Œé”šç‚¹
    4. æ”¯æŒå¤šç§å¼•ç”¨æ ¼å¼
    5. ç¼“å­˜è§£æç»“æœ

    æ™ºèƒ½ç‰‡æ®µè¯»å–ï¼ˆæ–°å¢ï¼‰ï¼š
    1. æ£€æŸ¥å¼•ç”¨æ–‡ä»¶å¤§å°ï¼Œ>10KB è§¦å‘ Milvus å‘é‡æ£€ç´¢
    2. æ ¹æ®ç”¨æˆ·å½“å‰é—®é¢˜ï¼Œæå–æœ€ç›¸å…³çš„å‰3ä¸ªç‰‡æ®µ
    3. ä¿æŒä¸ BaseJubenAgent çš„æ¥å£å…¼å®¹æ€§
    """

    # æ–‡ä»¶å¤§å°é˜ˆå€¼ï¼š10KB
    FILE_SIZE_THRESHOLD = 10 * 1024

    # å‘é‡æœç´¢è¿”å›çš„ç‰‡æ®µæ•°é‡
    TOP_FRAGMENTS = 3

    # Milvus é›†åˆåç§°
    FILE_FRAGMENTS_COLLECTION = "file_fragments"

    def __init__(self):
        self.logger = JubenLogger("ReferenceResolver")
        self.notes_manager = get_notes_manager()
        self.storage = get_storage()
        self.project_manager = ProjectManager()
        self._reference_cache = {}  # å¼•ç”¨ç¼“å­˜
        self._embedding_client = aliyun_embedding_client
        self._milvus_client = None
        self._file_cache = {}  # æ–‡ä»¶å†…å®¹ç¼“å­˜
        self._reference_trace: List[Dict[str, Any]] = []

        # å¼•ç”¨æ¨¡å¼å®šä¹‰
        self.reference_patterns = {
            # Noteåç§°ç›´æ¥å¼•ç”¨: @character_profile_1, @plot_point_2 
            'note_name': re.compile(r'@([a-z_]+_[a-z0-9_]+)', re.IGNORECASE),

            # Noteå¼•ç”¨: @note[id] æˆ– @note[title]
            'note': re.compile(r'@note\[([^\]]+)\]', re.IGNORECASE),

            # ä¼šè¯å¼•ç”¨: @session[å†…å®¹]
            'session': re.compile(r'@session\[([^\]]+)\]', re.IGNORECASE),

            # æ–‡ä»¶å¼•ç”¨: @file[id] æˆ– @file[name] æˆ– @file[name, query]
            'file': re.compile(r'@file\[([^\]]+)\]', re.IGNORECASE),

            # ç”¨æˆ·å¼•ç”¨: @user[å†…å®¹]
            'user': re.compile(r'@user\[([^\]]+)\]', re.IGNORECASE),

            # æ—¶é—´å¼•ç”¨: @time[æ ¼å¼]
            'time': re.compile(r'@time\[([^\]]+)\]', re.IGNORECASE),
        }

        # Agentåç§°åˆ°actionçš„æ˜ å°„ï¼ˆç”¨äºNoteåç§°å¼•ç”¨ï¼‰
        self.agent_action_mapping = {
            'character_profile': 'character_profile_generator',
            'character_relationship': 'character_relationship_analyzer',
            'plot_point': 'plot_points_analyzer',
            'story_outline': 'story_summary_generator',
            'major_plot': 'major_plot_points_agent',
            'detailed_plot': 'detailed_plot_points_agent',
            'script': 'short_drama_creator_agent',
            'drama_plan': 'short_drama_planner_agent',
            'evaluation': 'script_evaluation_agent',
            'mind_map': 'mind_map_agent',
        }

        # ç”¨æˆ·å½“å‰é—®é¢˜ï¼ˆç”¨äºå‘é‡æœç´¢ï¼‰
        self._current_query = ""

        self.logger.info("æ™ºèƒ½å¼•ç”¨è§£æå™¨åˆå§‹åŒ–å®Œæˆ")

    async def _get_milvus_client(self):
        """è·å– Milvus å®¢æˆ·ç«¯"""
        if self._milvus_client is None:
            try:
                self._milvus_client = await get_milvus_client()
                # ç¡®ä¿é›†åˆå­˜åœ¨
                await self._ensure_collection_exists()
            except Exception as e:
                self.logger.warning(f"Milvus å®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
                self._milvus_client = None
        return self._milvus_client

    async def _ensure_collection_exists(self):
        """ç¡®ä¿ Milvus é›†åˆå­˜åœ¨"""
        if self._milvus_client is None:
            return

        try:
            from pymilvus import utility
            if not utility.has_collection(self.FILE_FRAGMENTS_COLLECTION):
                await self._milvus_client.create_collection(
                    collection_name=self.FILE_FRAGMENTS_COLLECTION,
                    dimension=self._embedding_client.dimension,
                    metric_type="COSINE",
                    description="æ–‡ä»¶ç‰‡æ®µå‘é‡åº“ï¼Œç”¨äºæ™ºèƒ½å¼•ç”¨è§£æ"
                )
                self.logger.info(f"âœ… åˆ›å»º Milvus é›†åˆ: {self.FILE_FRAGMENTS_COLLECTION}")
        except Exception as e:
            self.logger.error(f"ç¡®ä¿é›†åˆå­˜åœ¨å¤±è´¥: {e}")

    def set_current_query(self, query: str):
        """è®¾ç½®ç”¨æˆ·å½“å‰é—®é¢˜ï¼Œç”¨äºæ™ºèƒ½ç‰‡æ®µæ£€ç´¢"""
        self._current_query = query

    async def resolve_references(
        self,
        text: str,
        user_id: str,
        session_id: str,
        query: str = "",
        project_id: Optional[str] = None
    ) -> str:
        """
        è§£ææ–‡æœ¬ä¸­çš„æ‰€æœ‰å¼•ç”¨

        Args:
            text: è¾“å…¥æ–‡æœ¬
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID
            query: ç”¨æˆ·å½“å‰é—®é¢˜ï¼ˆç”¨äºæ™ºèƒ½ç‰‡æ®µæ£€ç´¢ï¼‰

        Returns:
            str: è§£æåçš„æ–‡æœ¬
        """
        try:
            if not text:
                return text

            # é‡ç½®å¼•ç”¨è¿½è¸ª
            self._reference_trace = []

            # è®¾ç½®å½“å‰æŸ¥è¯¢
            if query:
                self.set_current_query(query)

            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"{user_id}:{session_id}:{project_id}:{hash(text)}:{hash(query)}"
            if cache_key in self._reference_cache:
                self.logger.debug("âœ… ä½¿ç”¨ç¼“å­˜çš„å¼•ç”¨è§£æç»“æœ")
                return self._reference_cache[cache_key]

            resolved_text = text

            # è§£æå„ç§ç±»å‹çš„å¼•ç”¨
            # ğŸ†• ä¼˜å…ˆè§£ænote_nameå¼•ç”¨ï¼ˆç®€æ´çš„@å¼•ç”¨æ–¹å¼ï¼‰
            resolved_text = await self._resolve_note_name_references(resolved_text, user_id, session_id)
            resolved_text = await self._resolve_note_references(resolved_text, user_id, session_id)
            resolved_text = await self._resolve_session_references(resolved_text, user_id, session_id)
            resolved_text = await self._resolve_file_references(
                resolved_text,
                user_id,
                session_id,
                project_id=project_id
            )
            resolved_text = await self._resolve_user_references(resolved_text, user_id)
            resolved_text = await self._resolve_time_references(resolved_text)

            # ç¼“å­˜ç»“æœ
            self._reference_cache[cache_key] = resolved_text

            self.logger.debug("âœ… æ™ºèƒ½å¼•ç”¨è§£æå®Œæˆ")
            return resolved_text

        except Exception as e:
            self.logger.error(f"âŒ å¼•ç”¨è§£æå¤±è´¥: {e}")
            return text

    def get_reference_trace(self) -> List[Dict[str, Any]]:
        """è·å–æœ€è¿‘ä¸€æ¬¡è§£æçš„å¼•ç”¨è¿½è¸ª"""
        return list(self._reference_trace)

    async def _resolve_note_name_references(self, text: str, user_id: str, session_id: str) -> str:
        """
        ğŸ†• è§£æNoteåç§°ç›´æ¥å¼•ç”¨ï¼ˆç®€æ´çš„@å¼•ç”¨æ–¹å¼ï¼‰

        æ”¯æŒæ ¼å¼ï¼š
        - @character_profile_1 - å¼•ç”¨ç¬¬1ä¸ªäººç‰©å°ä¼ 
        - @plot_point_2 - å¼•ç”¨ç¬¬2ä¸ªæƒ…èŠ‚ç‚¹
        - @story_outline_1 - å¼•ç”¨ç¬¬1ä¸ªæ•…äº‹å¤§çº²

        Args:
            text: è¾“å…¥æ–‡æœ¬
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID

        Returns:
            str: è§£æåçš„æ–‡æœ¬
        """
        async def replace_note_name_ref(match):
            note_name = match.group(1)
            return await self._resolve_single_note_name_ref(note_name, user_id, session_id)

        # ä½¿ç”¨å¼‚æ­¥æ›¿æ¢
        pattern = self.reference_patterns['note_name']
        matches = list(pattern.finditer(text))
        if not matches:
            return text

        # æŒ‰ä½ç½®ä»åå¾€å‰æ›¿æ¢ï¼Œé¿å…ä½ç½®åç§»
        for match in reversed(matches):
            note_name = match.group(1)
            resolved = await self._resolve_single_note_name_ref(note_name, user_id, session_id)
            text = text[:match.start()] + resolved + text[match.end():]

        return text

    async def _resolve_single_note_name_ref(self, note_name: str, user_id: str, session_id: str) -> str:
        """
        è§£æå•ä¸ªNoteåç§°å¼•ç”¨

        Args:
            note_name: Noteåç§°ï¼ˆå¦‚ character_profile_1, plot_point_2ï¼‰
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID

        Returns:
            str: è§£æåçš„å¼•ç”¨æ–‡æœ¬
        """
        try:
            # è§£ænote_name: character_profile_1 -> action=character_profile_generator, name=1
            parts = note_name.rsplit('_', 1)
            if len(parts) != 2:
                return f"[Invalid reference: @{note_name}]"

            name_prefix, index = parts[0], parts[1]

            # è·å–å¯¹åº”çš„action
            action = self.agent_action_mapping.get(name_prefix)
            if not action:
                return f"[Unknown note type: @{note_name}]"

            # ä»storageè·å–note
            storage = await get_storage()
            notes = await storage.get_notes_by_action(user_id, session_id, action)

            # æŸ¥æ‰¾æŒ‡å®šç´¢å¼•çš„noteï¼ˆç´¢å¼•ä»1å¼€å§‹ï¼‰
            if notes and len(notes) >= int(index):
                note = notes[int(index) - 1]
                note_title = note.get('title') or note.get('name', note_name)
                note_context = note.get('context', '')

                # è¿”å›æ ¼å¼åŒ–çš„å¼•ç”¨å†…å®¹
                preview = note_context[:300] + "..." if len(note_context) > 300 else note_context
                return f"\n\n**å¼•ç”¨: {note_title}**\n{preview}\n\n"
            else:
                return f"[Note not found: @{note_name}]"

        except Exception as e:
            self.logger.error(f"âŒ è§£æNoteåç§°å¼•ç”¨å¤±è´¥: {e}")
            return f"[Note error: @{note_name}]"

    async def _resolve_note_references(self, text: str, user_id: str, session_id: str) -> str:
        """è§£æNoteå¼•ç”¨"""
        def replace_note_ref(match):
            note_ref = match.group(1)
            return asyncio.run(self._resolve_single_note_ref(note_ref, user_id, session_id))

        return self.reference_patterns['note'].sub(replace_note_ref, text)

    async def _resolve_single_note_ref(self, note_ref: str, user_id: str, session_id: str) -> str:
        """è§£æå•ä¸ªNoteå¼•ç”¨"""
        try:
            # å°è¯•æŒ‰IDæŸ¥æ‰¾
            if self._is_uuid(note_ref):
                note = await self.notes_manager.get_note(note_ref, user_id)
            else:
                # æŒ‰æ ‡é¢˜æœç´¢
                notes = await self.notes_manager.search_notes(
                    user_id=user_id,
                    query=note_ref,
                    session_id=session_id,
                    limit=1
                )
                note = notes[0] if notes else None

            if note:
                return f"[{note['title']}](note:{note['note_id']})"
            else:
                return f"[Note not found: {note_ref}]"

        except Exception as e:
            self.logger.error(f"âŒ è§£æNoteå¼•ç”¨å¤±è´¥: {e}")
            return f"[Note error: {note_ref}]"

    async def _resolve_session_references(self, text: str, user_id: str, session_id: str) -> str:
        """è§£æä¼šè¯å¼•ç”¨"""
        def replace_session_ref(match):
            session_content = match.group(1)
            return asyncio.run(self._resolve_single_session_ref(session_content, user_id, session_id))

        return self.reference_patterns['session'].sub(replace_session_ref, text)

    async def _resolve_single_session_ref(self, content: str, user_id: str, session_id: str) -> str:
        """è§£æå•ä¸ªä¼šè¯å¼•ç”¨"""
        try:
            # åœ¨ä¼šè¯å†å²ä¸­æœç´¢ç›¸å…³å†…å®¹
            chat_messages = await self.storage.get_chat_messages(
                user_id=user_id,
                session_id=session_id,
                limit=50
            )

            # ç®€å•çš„å†…å®¹åŒ¹é…
            for message in chat_messages:
                if content.lower() in message.get('content', '').lower():
                    return f"[Session: {content[:30]}...](session:{session_id})"

            return f"[Session content: {content}]"

        except Exception as e:
            self.logger.error(f"âŒ è§£æä¼šè¯å¼•ç”¨å¤±è´¥: {e}")
            return f"[Session error: {content}]"

    async def _resolve_file_references(
        self,
        text: str,
        user_id: str,
        session_id: str,
        project_id: Optional[str] = None
    ) -> str:
        """è§£ææ–‡ä»¶å¼•ç”¨ï¼ˆæ”¯æŒæ™ºèƒ½ç‰‡æ®µè¯»å–ï¼‰"""
        async def replace_file_ref(match):
            file_ref = match.group(1)
            return await self._resolve_single_file_ref(file_ref, user_id, session_id, project_id=project_id)

        # ä½¿ç”¨å¼‚æ­¥æ›¿æ¢
        pattern = self.reference_patterns['file']
        matches = list(pattern.finditer(text))
        if not matches:
            return text

        # æŒ‰ä½ç½®ä»åå¾€å‰æ›¿æ¢ï¼Œé¿å…ä½ç½®åç§»
        for match in reversed(matches):
            file_ref = match.group(1)
            resolved = await self._resolve_single_file_ref(file_ref, user_id, session_id, project_id=project_id)
            text = text[:match.start()] + resolved + text[match.end():]

        return text

    async def _resolve_single_file_ref(
        self,
        file_ref: str,
        user_id: str,
        session_id: str,
        project_id: Optional[str] = None
    ) -> str:
        """
        è§£æå•ä¸ªæ–‡ä»¶å¼•ç”¨ï¼ˆæ™ºèƒ½ç‰‡æ®µè¯»å–ï¼‰

        æ–°å¢åŠŸèƒ½ï¼š
        1. æ£€æŸ¥æ–‡ä»¶å¤§å°
        2. >10KB ä½¿ç”¨ Milvus å‘é‡æ£€ç´¢
        3. è¿”å›æœ€ç›¸å…³çš„å‰3ä¸ªç‰‡æ®µ

        Args:
            file_ref: æ–‡ä»¶å¼•ç”¨ï¼Œæ ¼å¼ï¼šid æˆ– name æˆ– name, query

        Returns:
            str: è§£æåçš„å¼•ç”¨æ–‡æœ¬
        """
        try:
            # è§£ææ–‡ä»¶å¼•ç”¨å‚æ•°
            file_params = [p.strip() for p in file_ref.split(',')]
            file_identifier = file_params[0]
            search_query = file_params[1] if len(file_params) > 1 else self._current_query

            # ä¼˜å…ˆä»é¡¹ç›®æ–‡ä»¶ä¸­è§£æ
            project_file = None
            if project_id:
                project_file = await self._get_project_file(project_id, file_identifier)
            if project_file:
                return await self._render_project_file_content(project_file, search_query)

            # è·å–æ–‡ä»¶è·¯å¾„ï¼ˆå­˜å‚¨ç³»ç»Ÿï¼‰
            file_path = await self._get_file_path(file_identifier, user_id)
            if not file_path:
                return f"[File not found: {file_identifier}]"

            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            file_size = self._get_file_size(file_path)
            self._reference_trace.append({
                "source": "storage_file",
                "file_identifier": file_identifier,
                "file_path": file_path,
                "query": search_query
            })

            if file_size > self.FILE_SIZE_THRESHOLD:
                # å¤§æ–‡ä»¶ï¼šä½¿ç”¨æ™ºèƒ½ç‰‡æ®µè¯»å–
                return await self._intelligent_fragment_read(
                    file_path, file_identifier, search_query, user_id
                )
            else:
                # å°æ–‡ä»¶ï¼šç›´æ¥è¿”å›å…¨æ–‡
                content = await self._read_file_content(file_path)
                if content:
                    preview = content[:200] + "..." if len(content) > 200 else content
                    return f"[ğŸ“„ {file_identifier}]\n{preview}"
                else:
                    return f"[File: {file_identifier}]"

        except Exception as e:
            self.logger.error(f"âŒ è§£ææ–‡ä»¶å¼•ç”¨å¤±è´¥: {e}")
            return f"[File error: {file_ref}]"

    async def _get_file_path(self, file_identifier: str, user_id: str) -> Optional[str]:
        """
        è·å–æ–‡ä»¶è·¯å¾„

        Args:
            file_identifier: æ–‡ä»¶æ ‡è¯†ç¬¦ï¼ˆIDæˆ–åç§°ï¼‰
            user_id: ç”¨æˆ·ID

        Returns:
            Optional[str]: æ–‡ä»¶è·¯å¾„
        """
        try:
            # ä»å­˜å‚¨ç®¡ç†å™¨è·å–æ–‡ä»¶
            if self._is_uuid(file_identifier):
                # æŒ‰ ID æŸ¥æ‰¾
                file_info = await self.storage.get_file(file_identifier, user_id)
            else:
                # æŒ‰åç§°æœç´¢
                files = await self.storage.list_user_files(user_id)
                file_info = None
                for f in files:
                    if f.get('name') == file_identifier or f.get('filename') == file_identifier:
                        file_info = f
                        break

            if file_info:
                return file_info.get('path') or file_info.get('file_path')

            # æ£€æŸ¥æ˜¯å¦æ˜¯æœ¬åœ°æ–‡ä»¶è·¯å¾„
            local_path = Path(file_identifier)
            if local_path.exists():
                return str(local_path)

            return None

        except Exception as e:
            self.logger.error(f"è·å–æ–‡ä»¶è·¯å¾„å¤±è´¥: {e}")
            return None

    async def _get_project_file(self, project_id: str, file_identifier: str) -> Optional[Dict[str, Any]]:
        """ä»é¡¹ç›®æ–‡ä»¶ä¸­è·å–æ–‡ä»¶å†…å®¹ï¼ˆæ”¯æŒIDæˆ–æ–‡ä»¶åï¼‰"""
        try:
            if self._is_uuid(file_identifier):
                project_file = await self.project_manager.get_file(project_id, file_identifier)
                return project_file.dict() if project_file else None

            files = await self.project_manager.get_project_files(project_id)
            for file_item in files:
                if file_item.filename == file_identifier:
                    return file_item.dict()
            # å°è¯•å¿½ç•¥å¤§å°å†™åŒ¹é…
            for file_item in files:
                if file_item.filename.lower() == file_identifier.lower():
                    return file_item.dict()
            return None
        except Exception as e:
            self.logger.error(f"è·å–é¡¹ç›®æ–‡ä»¶å¤±è´¥: {e}")
            return None

    async def _render_project_file_content(self, project_file: Dict[str, Any], query: str) -> str:
        """æ¸²æŸ“é¡¹ç›®æ–‡ä»¶å†…å®¹ï¼ˆæ–‡æœ¬ä¼˜å…ˆï¼Œè¶…é•¿åˆ™æˆªå–ç›¸å…³ç‰‡æ®µï¼‰"""
        try:
            filename = project_file.get("filename") or project_file.get("id", "æ–‡ä»¶")
            content = project_file.get("content", "")
            project_id = project_file.get("project_id")
            file_id = project_file.get("id")
            if content is None:
                return f"[ğŸ“„ {filename}]\n(æ–‡ä»¶å†…å®¹ä¸ºç©º)"

            if not isinstance(content, str):
                content = json.dumps(content, ensure_ascii=False, indent=2)

            # è®°å½•å¼•ç”¨è¿½è¸ª
            self._reference_trace.append({
                "source": "project_file",
                "file_id": project_file.get("id"),
                "filename": filename,
                "query": query or self._current_query
            })

            content_size = len(content.encode("utf-8"))
            if content_size <= self.FILE_SIZE_THRESHOLD:
                preview = content[:200] + "..." if len(content) > 200 else content
                return f"[ğŸ“„ {filename}]\n{preview}"

            # å¤§æ–‡ä»¶ï¼šä¼˜å…ˆç”¨å‘é‡æ£€ç´¢è·å–ç‰‡æ®µ
            search_query = query or self._current_query
            if search_query and project_id and file_id:
                vector_snippets = await self._search_project_file_fragments(
                    project_id=project_id,
                    file_id=file_id,
                    query=search_query,
                    filename=filename
                )
                if vector_snippets:
                    formatted = "\n\n".join(vector_snippets)
                    return f"[ğŸ“„ {filename}]\n{formatted}"

            # å…œåº•ï¼šå…³é”®è¯ç‰‡æ®µ
            snippets = self._extract_relevant_snippets(content, search_query)
            if snippets:
                formatted = "\n\n".join(snippets)
                return f"[ğŸ“„ {filename}]\n{formatted}"

            preview = content[:400] + "..." if len(content) > 400 else content
            return f"[ğŸ“„ {filename}]\n{preview}"
        except Exception as e:
            self.logger.error(f"æ¸²æŸ“é¡¹ç›®æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return "[File render error]"

    def _extract_relevant_snippets(self, content: str, query: str) -> List[str]:
        """åŸºäºå…³é”®è¯çš„è½»é‡ç‰‡æ®µæå–ï¼ˆé¿å…æ— å‘é‡åº“æ—¶ç©ºç™½ï¼‰"""
        if not query:
            return []

        query_lower = query.lower()
        content_lower = content.lower()
        snippets = []
        start = 0

        while len(snippets) < self.TOP_FRAGMENTS:
            idx = content_lower.find(query_lower, start)
            if idx == -1:
                break
            left = max(idx - 120, 0)
            right = min(idx + 200, len(content))
            snippet = content[left:right].strip()
            if snippet:
                snippets.append(f"...{snippet}...")
            start = idx + len(query_lower)

        return snippets

    async def _search_project_file_fragments(
        self,
        project_id: str,
        file_id: str,
        query: str,
        filename: Optional[str] = None
    ) -> List[str]:
        """ä» Milvus ä¸­æ£€ç´¢é¡¹ç›®æ–‡ä»¶ç‰‡æ®µ"""
        try:
            client = await self._get_milvus_client()
            if not client:
                return []

            vector = self._embedding_client.embed_text(query)
            if not vector:
                return []

            expr = f'text_id like "{project_id}:{file_id}:%"'
            results = await client.search_vectors(
                collection_name=self.FILE_FRAGMENTS_COLLECTION,
                query_vectors=[vector],
                top_k=self.TOP_FRAGMENTS,
                score_threshold=0.0,
                metadata_filter=expr
            )

            hits = results[0] if results else []
            if hits:
                self._reference_trace.append({
                    "source": "project_file_vector",
                    "project_id": project_id,
                    "file_id": file_id,
                    "filename": filename,
                    "query": query,
                    "result_count": len(hits)
                })
            return [hit.get("content", "") for hit in hits if hit.get("content")]
        except Exception as e:
            self.logger.warning(f"é¡¹ç›®æ–‡ä»¶å‘é‡æ£€ç´¢å¤±è´¥: {e}")
            return []

    def _get_file_size(self, file_path: str) -> int:
        """
        è·å–æ–‡ä»¶å¤§å°

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            int: æ–‡ä»¶å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        """
        try:
            if file_path in self._file_cache:
                return len(self._file_cache[file_path].encode('utf-8'))

            path = Path(file_path)
            if path.exists():
                return path.stat().st_size
            return 0
        except Exception as e:
            self.logger.error(f"è·å–æ–‡ä»¶å¤§å°å¤±è´¥: {e}")
            return 0

    async def _read_file_content(self, file_path: str) -> str:
        """
        è¯»å–æ–‡ä»¶å†…å®¹

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            str: æ–‡ä»¶å†…å®¹
        """
        try:
            # æ£€æŸ¥ç¼“å­˜
            if file_path in self._file_cache:
                return self._file_cache[file_path]

            path = Path(file_path)
            if not path.exists():
                return ""

            # è¯»å–æ–‡ä»¶
            with open(path, 'r', encoding='utf-8') as f:
                content = f.read()

            # ç¼“å­˜å†…å®¹ï¼ˆé™åˆ¶ç¼“å­˜å¤§å°ï¼‰
            if len(content) < 100 * 1024:  # åªç¼“å­˜å°äº100KBçš„æ–‡ä»¶
                self._file_cache[file_path] = content

            return content

        except Exception as e:
            self.logger.error(f"è¯»å–æ–‡ä»¶å†…å®¹å¤±è´¥: {e}")
            return ""

    async def _intelligent_fragment_read(
        self,
        file_path: str,
        file_identifier: str,
        query: str,
        user_id: str
    ) -> str:
        """
        æ™ºèƒ½ç‰‡æ®µè¯»å–

        æ ¸å¿ƒé€»è¾‘ï¼š
        1. è¯»å–æ–‡ä»¶å†…å®¹å¹¶åˆ†å—
        2. å¯¹æŸ¥è¯¢æ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
        3. ä½¿ç”¨ Milvus è¿›è¡Œç›¸ä¼¼åº¦æ£€ç´¢
        4. è¿”å›æœ€ç›¸å…³çš„å‰3ä¸ªç‰‡æ®µ

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            file_identifier: æ–‡ä»¶æ ‡è¯†ç¬¦
            query: æœç´¢æŸ¥è¯¢
            user_id: ç”¨æˆ·ID

        Returns:
            str: æ ¼å¼åŒ–çš„ç›¸å…³ç‰‡æ®µ
        """
        try:
            # å¦‚æœæ²¡æœ‰æŸ¥è¯¢ï¼Œè¿”å›æ–‡ä»¶æ‘˜è¦
            if not query or not query.strip():
                content = await self._read_file_content(file_path)
                return self._generate_file_summary(file_identifier, content)

            # è¯»å–æ–‡ä»¶å†…å®¹
            content = await self._read_file_content(file_path)
            if not content:
                return f"[File content unavailable: {file_identifier}]"

            # åˆ†å—å¤„ç†
            fragments = self._split_into_fragments(content)
            if not fragments:
                return f"[File is empty: {file_identifier}]"

            # è·å–æŸ¥è¯¢å‘é‡
            query_embedding = self._embedding_client.embed_text(query)
            if not query_embedding:
                self.logger.warning("æŸ¥è¯¢å‘é‡åŒ–å¤±è´¥ï¼Œä½¿ç”¨å‰3ä¸ªç‰‡æ®µ")
                return self._format_fragments(file_identifier, fragments[:self.TOP_FRAGMENTS])

            # è®¡ç®—æ¯ä¸ªç‰‡æ®µçš„ç›¸ä¼¼åº¦
            scored_fragments = []
            for i, fragment in enumerate(fragments):
                fragment_embedding = self._embedding_client.embed_text(fragment)
                if fragment_embedding:
                    similarity = self._cosine_similarity(query_embedding, fragment_embedding)
                    scored_fragments.append((similarity, i, fragment))

            # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–å‰3ä¸ª
            scored_fragments.sort(key=lambda x: x[0], reverse=True)
            top_fragments = [f[2] for f in scored_fragments[:self.TOP_FRAGMENTS]]

            # å¯é€‰ï¼šå°†ç‰‡æ®µç´¢å¼•åˆ° Milvus ä»¥ä¾¿ä¸‹æ¬¡å¿«é€Ÿæ£€ç´¢
            await self._index_fragments_to_milvus(file_path, fragments, user_id)

            return self._format_fragments(file_identifier, top_fragments, scored_fragments[:self.TOP_FRAGMENTS])

        except Exception as e:
            self.logger.error(f"âŒ æ™ºèƒ½ç‰‡æ®µè¯»å–å¤±è´¥: {e}")
            # é™çº§ï¼šè¿”å›å‰3ä¸ªç‰‡æ®µ
            content = await self._read_file_content(file_path)
            fragments = self._split_into_fragments(content)
            return self._format_fragments(file_identifier, fragments[:self.TOP_FRAGMENTS])

    def _split_into_fragments(self, content: str, chunk_size: int = 1000) -> List[str]:
        """
        å°†æ–‡æœ¬åˆ†å‰²æˆè¯­ä¹‰ç›¸å…³çš„ç‰‡æ®µ

        Args:
            content: æ–‡æœ¬å†…å®¹
            chunk_size: ç‰‡æ®µå¤§å°

        Returns:
            List[str]: ç‰‡æ®µåˆ—è¡¨
        """
        fragments = []

        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = content.split('\n\n')
        current_fragment = ""

        for paragraph in paragraphs:
            paragraph = paragraph.strip()
            if not paragraph:
                continue

            # å¦‚æœå½“å‰ç‰‡æ®µåŠ ä¸Šæ–°æ®µè½è¶…è¿‡å¤§å°é™åˆ¶
            if len(current_fragment) + len(paragraph) > chunk_size:
                if current_fragment:
                    fragments.append(current_fragment.strip())
                current_fragment = paragraph
            else:
                current_fragment += "\n\n" + paragraph if current_fragment else paragraph

        # æ·»åŠ æœ€åä¸€ä¸ªç‰‡æ®µ
        if current_fragment:
            fragments.append(current_fragment.strip())

        # å¦‚æœæ²¡æœ‰æ®µè½åˆ†å‰²ï¼ŒæŒ‰å¥å­åˆ†å‰²
        if not fragments:
            sentences = re.split(r'[ã€‚ï¼ï¼Ÿ\n]', content)
            current_fragment = ""

            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue

                if len(current_fragment) + len(sentence) > chunk_size:
                    if current_fragment:
                        fragments.append(current_fragment.strip())
                    current_fragment = sentence
                else:
                    current_fragment += sentence

            if current_fragment:
                fragments.append(current_fragment.strip())

        return fragments

    def _cosine_similarity(self, vec1: List[float], vec2: List[float]) -> float:
        """
        è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦

        Args:
            vec1: å‘é‡1
            vec2: å‘é‡2

        Returns:
            float: ç›¸ä¼¼åº¦åˆ†æ•° (0-1)
        """
        try:
            v1 = np.array(vec1)
            v2 = np.array(vec2)

            dot_product = np.dot(v1, v2)
            norm1 = np.linalg.norm(v1)
            norm2 = np.linalg.norm(v2)

            if norm1 == 0 or norm2 == 0:
                return 0.0

            return float(dot_product / (norm1 * norm2))
        except Exception as e:
            self.logger.error(f"è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦å¤±è´¥: {e}")
            return 0.0

    async def _index_fragments_to_milvus(
        self,
        file_path: str,
        fragments: List[str],
        user_id: str
    ) -> bool:
        """
        å°†ç‰‡æ®µç´¢å¼•åˆ° Milvus

        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            fragments: ç‰‡æ®µåˆ—è¡¨
            user_id: ç”¨æˆ·ID

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            milvus_client = await self._get_milvus_client()
            if not milvus_client:
                return False

            # ä¸ºæ¯ä¸ªç‰‡æ®µç”Ÿæˆ ID
            text_ids = []
            embeddings = []
            metadata_list = []

            for i, fragment in enumerate(fragments):
                text_id = f"{user_id}_{file_path}_{i}"
                embedding = self._embedding_client.embed_text(fragment)

                if embedding:
                    text_ids.append(text_id)
                    embeddings.append(embedding)
                    metadata_list.append({
                        "file_path": file_path,
                        "fragment_index": i,
                        "user_id": user_id
                    })

            # æ‰¹é‡æ’å…¥
            if text_ids and embeddings:
                await milvus_client.insert_vectors(
                    collection_name=self.FILE_FRAGMENTS_COLLECTION,
                    text_ids=text_ids,
                    contents=fragments[:len(text_ids)],
                    vectors=embeddings,
                    metadata_list=metadata_list
                )
                self.logger.debug(f"âœ… ç´¢å¼• {len(text_ids)} ä¸ªç‰‡æ®µåˆ° Milvus")
                return True

            return False

        except Exception as e:
            self.logger.error(f"ç´¢å¼•ç‰‡æ®µåˆ° Milvus å¤±è´¥: {e}")
            return False

    async def _search_fragments_from_milvus(
        self,
        query: str,
        file_path: str,
        top_k: int = 3
    ) -> List[Dict[str, Any]]:
        """
        ä» Milvus æœç´¢ç›¸å…³ç‰‡æ®µ

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            file_path: æ–‡ä»¶è·¯å¾„
            top_k: è¿”å›æ•°é‡

        Returns:
            List[Dict]: æœç´¢ç»“æœ
        """
        try:
            milvus_client = await self._get_milvus_client()
            if not milvus_client:
                return []

            # è·å–æŸ¥è¯¢å‘é‡
            query_embedding = self._embedding_client.embed_text(query)
            if not query_embedding:
                return []

            # æœç´¢
            metadata_filter = f'file_path == "{file_path}"'
            results = await milvus_client.search_vectors(
                collection_name=self.FILE_FRAGMENTS_COLLECTION,
                query_vectors=[query_embedding],
                top_k=top_k,
                score_threshold=0.3,
                metadata_filter=metadata_filter
            )

            if results and results[0]:
                return results[0]

            return []

        except Exception as e:
            self.logger.error(f"ä» Milvus æœç´¢ç‰‡æ®µå¤±è´¥: {e}")
            return []

    def _format_fragments(
        self,
        file_identifier: str,
        fragments: List[str],
        scored_fragments: List[Tuple] = None
    ) -> str:
        """
        æ ¼å¼åŒ–ç‰‡æ®µè¾“å‡º

        Args:
            file_identifier: æ–‡ä»¶æ ‡è¯†ç¬¦
            fragments: ç‰‡æ®µåˆ—è¡¨
            scored_fragments: å¸¦åˆ†æ•°çš„ç‰‡æ®µåˆ—è¡¨

        Returns:
            str: æ ¼å¼åŒ–çš„æ–‡æœ¬
        """
        output = f"[ğŸ“„ {file_identifier} - ç›¸å…³ç‰‡æ®µ]\n\n"

        for i, fragment in enumerate(fragments, 1):
            output += f"**ç‰‡æ®µ {i}**"

            # æ·»åŠ ç›¸ä¼¼åº¦åˆ†æ•°
            if scored_fragments and i <= len(scored_fragments):
                similarity = scored_fragments[i - 1][0]
                output += f" (ç›¸å…³åº¦: {similarity:.2%})"

            output += f"\n{fragment[:500]}"
            if len(fragment) > 500:
                output += "..."
            output += "\n\n"

        return output

    def _generate_file_summary(self, file_identifier: str, content: str) -> str:
        """
        ç”Ÿæˆæ–‡ä»¶æ‘˜è¦

        Args:
            file_identifier: æ–‡ä»¶æ ‡è¯†ç¬¦
            content: æ–‡ä»¶å†…å®¹

        Returns:
            str: æ‘˜è¦æ–‡æœ¬
        """
        lines = content.split('\n')
        total_lines = len(lines)
        total_chars = len(content)

        # è·å–å‰å‡ è¡Œå’Œåå‡ è¡Œ
        preview_lines = 5
        preview = '\n'.join(lines[:preview_lines])
        if total_lines > preview_lines:
            preview += f"\n\n... (å…± {total_lines} è¡Œ, {total_chars} å­—ç¬¦)"

        return f"[ğŸ“„ {file_identifier}]\n{preview}"

    async def _resolve_user_references(self, text: str, user_id: str) -> str:
        """è§£æç”¨æˆ·å¼•ç”¨"""
        def replace_user_ref(match):
            user_content = match.group(1)
            return f"[User: {user_content}]"

        return self.reference_patterns['user'].sub(replace_user_ref, text)

    async def _resolve_time_references(self, text: str) -> str:
        """è§£ææ—¶é—´å¼•ç”¨"""
        def replace_time_ref(match):
            time_format = match.group(1)
            try:
                if time_format == "now":
                    return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                elif time_format == "date":
                    return datetime.now().strftime("%Y-%m-%d")
                elif time_format == "time":
                    return datetime.now().strftime("%H:%M:%S")
                else:
                    return datetime.now().strftime(time_format)
            except:
                return f"[Time: {time_format}]"

        return self.reference_patterns['time'].sub(replace_time_ref, text)

    def _is_uuid(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦ä¸ºUUIDæ ¼å¼"""
        uuid_pattern = re.compile(r'^[0-9a-f]{8}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{4}-[0-9a-f]{12}$', re.IGNORECASE)
        return bool(uuid_pattern.match(text))

    async def extract_references(self, text: str) -> Dict[str, List[str]]:
        """
        æå–æ–‡æœ¬ä¸­çš„æ‰€æœ‰å¼•ç”¨

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            Dict: æŒ‰ç±»å‹åˆ†ç»„çš„å¼•ç”¨åˆ—è¡¨
        """
        references = {}

        for ref_type, pattern in self.reference_patterns.items():
            matches = pattern.findall(text)
            if matches:
                references[ref_type] = matches

        return references

    def clear_cache(self, user_id: str = None, session_id: str = None):
        """æ¸…ç†å¼•ç”¨ç¼“å­˜"""
        if user_id and session_id:
            # æ¸…ç†ç‰¹å®šä¼šè¯çš„ç¼“å­˜
            keys_to_remove = [
                key for key in self._reference_cache.keys()
                if key.startswith(f"{user_id}:{session_id}:")
            ]
        elif user_id:
            # æ¸…ç†ç”¨æˆ·çš„æ‰€æœ‰ç¼“å­˜
            keys_to_remove = [
                key for key in self._reference_cache.keys()
                if key.startswith(f"{user_id}:")
            ]
        else:
            # æ¸…ç†æ‰€æœ‰ç¼“å­˜
            keys_to_remove = list(self._reference_cache.keys())

        for key in keys_to_remove:
            del self._reference_cache[key]

        # æ¸…ç†æ–‡ä»¶ç¼“å­˜
        if not user_id:
            self._file_cache.clear()

        self.logger.debug(f"âœ… æ¸…ç†å¼•ç”¨ç¼“å­˜æˆåŠŸ: {len(keys_to_remove)}æ¡")


# å…¨å±€å®ä¾‹
_reference_resolver = None


def get_juben_reference_resolver() -> JubenReferenceResolver:
    """è·å–æ™ºèƒ½å¼•ç”¨è§£æå™¨å®ä¾‹"""
    global _reference_resolver
    if _reference_resolver is None:
        _reference_resolver = JubenReferenceResolver()
    return _reference_resolver
