"""
LLMæ‰¹å¤„ç†å™¨

æ”¯æŒæ‰¹é‡å¤„ç†LLMè¯·æ±‚ï¼Œæé«˜ååé‡å’Œæ•ˆç‡
"""
import asyncio
import time
from typing import List, Dict, Any, Optional, Callable, Awaitable
from dataclasses import dataclass, field
from collections import defaultdict
import hashlib
import json

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from utils.logger import JubenLogger
from utils.llm_client import get_llm_client

logger = JubenLogger("llm_batch_processor")


@dataclass
class BatchRequest:
    """æ‰¹å¤„ç†è¯·æ±‚"""
    request_id: str
    messages: List[Dict[str, str]]
    kwargs: Dict[str, Any] = field(default_factory=dict)
    model_provider: str = "zhipu"

    def __hash__(self):
        """ç”¨äºå»é‡çš„å“ˆå¸Œå€¼"""
        content = json.dumps({
            "messages": self.messages,
            "kwargs": self.kwargs,
            "model_provider": self.model_provider
        }, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()


@dataclass
class BatchResult:
    """æ‰¹å¤„ç†ç»“æœ"""
    request_id: str
    result: Optional[str] = None
    error: Optional[Exception] = None
    duration: float = 0.0


@dataclass
class BatchConfig:
    """æ‰¹å¤„ç†é…ç½®"""
    # æ‰¹å¤„ç†å¤§å°
    batch_size: int = 10

    # æ‰¹å¤„ç†è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    batch_timeout: float = 2.0

    # æœ€å¤§å¹¶å‘æ‰¹æ¬¡æ•°
    max_concurrent_batches: int = 5

    # æ˜¯å¦å¯ç”¨è¯·æ±‚å»é‡
    enable_deduplication: bool = True

    # å»é‡ç¼“å­˜å¤§å°
    dedup_cache_size: int = 1000

    # ç»“æœç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰
    result_cache_ttl: int = 3600


class LLMBatchProcessor:
    """
    LLMæ‰¹å¤„ç†å™¨

    åŠŸèƒ½ï¼š
    1. æ‰¹é‡åˆå¹¶ç›¸ä¼¼è¯·æ±‚
    2. å¹¶å‘æ‰§è¡Œå¤šä¸ªè¯·æ±‚
    3. è¯·æ±‚å»é‡
    4. ç»“æœç¼“å­˜
    """

    def __init__(self, config: Optional[BatchConfig] = None):
        """
        åˆå§‹åŒ–æ‰¹å¤„ç†å™¨

        Args:
            config: æ‰¹å¤„ç†é…ç½®
        """
        self.config = config or BatchConfig()

        # å¾…å¤„ç†è¯·æ±‚é˜Ÿåˆ—
        self._pending_requests: List[BatchRequest] = []

        # è¯·æ±‚å»é‡ç¼“å­˜
        self._request_cache: Dict[int, BatchResult] = {}

        # ç»“æœç¼“å­˜
        self._result_cache: Dict[str, tuple[str, float]] = {}

        # æ­£åœ¨å¤„ç†çš„æ‰¹æ¬¡
        self._processing_batches: set[str] = set()

        # ä¿¡å·é‡æ§åˆ¶å¹¶å‘
        self._semaphore = asyncio.Semaphore(self.config.max_concurrent_batches)

        # ç»Ÿè®¡ä¿¡æ¯
        self._stats = {
            "total_requests": 0,
            "batched_requests": 0,
            "deduped_requests": 0,
            "cache_hits": 0,
            "total_duration": 0.0
        }

        # æ‰¹å¤„ç†ä»»åŠ¡
        self._batch_task: Optional[asyncio.Task] = None

        logger.info("âœ… LLMæ‰¹å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆ")

    async def process(
        self,
        messages: List[Dict[str, str]],
        model_provider: str = "zhipu",
        **kwargs
    ) -> str:
        """
        å¤„ç†å•ä¸ªLLMè¯·æ±‚

        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            model_provider: æ¨¡å‹æä¾›å•†
            **kwargs: é¢å¤–å‚æ•°

        Returns:
            str: LLMå“åº”
        """
        request = BatchRequest(
            request_id=f"{time.time()}_{id(messages)}",
            messages=messages,
            kwargs=kwargs,
            model_provider=model_provider
        )

        # æ£€æŸ¥ç»“æœç¼“å­˜
        cache_key = self._get_cache_key(request)
        if cache_key in self._result_cache:
            result, _ = self._result_cache[cache_key]
            self._stats["cache_hits"] += 1
            logger.debug(f"âœ… ç¼“å­˜å‘½ä¸­: {cache_key}")
            return result

        # æ£€æŸ¥å»é‡ç¼“å­˜
        request_hash = hash(request)
        if self.config.enable_deduplication and request_hash in self._request_cache:
            self._stats["deduped_requests"] += 1
            logger.debug(f"âœ… è¯·æ±‚å»é‡: {request.request_id}")
            cached_result = self._request_cache[request_hash]
            if cached_result.error:
                raise cached_result.error
            return cached_result.result

        # æ·»åŠ åˆ°å¾…å¤„ç†é˜Ÿåˆ—
        self._pending_requests.append(request)
        self._stats["total_requests"] += 1

        # å¯åŠ¨æ‰¹å¤„ç†ä»»åŠ¡
        if self._batch_task is None or self._batch_task.done():
            self._batch_task = asyncio.create_task(self._batch_loop())

        # ç­‰å¾…ç»“æœ
        return await self._wait_for_result(request)

    async def _wait_for_result(self, request: BatchRequest) -> str:
        """ç­‰å¾…è¯·æ±‚ç»“æœ"""
        request_hash = hash(request)
        timeout = self.config.batch_timeout + 30  # æ‰¹å¤„ç†è¶…æ—¶ + LLMè¶…æ—¶

        start_time = time.time()
        while time.time() - start_time < timeout:
            if request_hash in self._request_cache:
                result = self._request_cache[request_hash]
                if result.error:
                    raise result.error
                return result.result

            await asyncio.sleep(0.01)

        # è¶…æ—¶å¤„ç†
        if request in self._pending_requests:
            self._pending_requests.remove(request)

        raise TimeoutError(f"è¯·æ±‚å¤„ç†è¶…æ—¶: {request.request_id}")

    async def _batch_loop(self):
        """æ‰¹å¤„ç†å¾ªç¯"""
        try:
            while self._pending_requests:
                # è·å–ä¸€æ‰¹è¯·æ±‚
                batch = self._get_next_batch()

                if not batch:
                    await asyncio.sleep(0.01)
                    continue

                # å¤„ç†æ‰¹æ¬¡
                async with self._semaphore:
                    await self._process_batch(batch)

        except Exception as e:
            logger.error(f"âŒ æ‰¹å¤„ç†å¾ªç¯é”™è¯¯: {e}", exc_info=True)

    def _get_next_batch(self) -> List[BatchRequest]:
        """è·å–ä¸‹ä¸€æ‰¹è¯·æ±‚"""
        if not self._pending_requests:
            return []

        # æŒ‰æ¨¡å‹æä¾›å•†åˆ†ç»„
        grouped = defaultdict(list)
        for request in self._pending_requests[:self.config.batch_size]:
            grouped[request.model_provider].append(request)

        # è¿”å›ç¬¬ä¸€æ‰¹
        if grouped:
            first_provider = next(iter(grouped))
            batch = grouped[first_provider]
            # ä»å¾…å¤„ç†é˜Ÿåˆ—ä¸­ç§»é™¤
            for req in batch:
                if req in self._pending_requests:
                    self._pending_requests.remove(req)
            return batch

        return []

    async def _process_batch(self, batch: List[BatchRequest]):
        """å¤„ç†ä¸€æ‰¹è¯·æ±‚"""
        batch_id = f"batch_{time.time()}_{len(batch)}"
        self._processing_batches.add(batch_id)

        try:
            logger.info(f"ğŸ”„ å¤„ç†æ‰¹æ¬¡ {batch_id}: {len(batch)} ä¸ªè¯·æ±‚")
            self._stats["batched_requests"] += len(batch)

            start_time = time.time()

            # å¹¶å‘å¤„ç†æ‰€æœ‰è¯·æ±‚
            tasks = [
                self._process_single_request(req)
                for req in batch
            ]

            results = await asyncio.gather(*tasks, return_exceptions=True)

            # å¤„ç†ç»“æœ
            for req, result in zip(batch, results):
                request_hash = hash(req)

                if isinstance(result, Exception):
                    self._request_cache[request_hash] = BatchResult(
                        request_id=req.request_id,
                        error=result
                    )
                else:
                    self._request_cache[request_hash] = BatchResult(
                        request_id=req.request_id,
                        result=result,
                        duration=time.time() - start_time
                    )

                    # ç¼“å­˜ç»“æœ
                    cache_key = self._get_cache_key(req)
                    self._result_cache[cache_key] = (result, time.time())

            duration = time.time() - start_time
            self._stats["total_duration"] += duration

            logger.info(
                f"âœ… æ‰¹æ¬¡ {batch_id} å®Œæˆ: "
                f"{len(batch)} ä¸ªè¯·æ±‚, è€—æ—¶ {duration:.2f}s"
            )

        except Exception as e:
            logger.error(f"âŒ æ‰¹æ¬¡ {batch_id} å¤„ç†å¤±è´¥: {e}", exc_info=True)

        finally:
            self._processing_batches.discard(batch_id)

    async def _process_single_request(self, request: BatchRequest) -> str:
        """å¤„ç†å•ä¸ªè¯·æ±‚"""
        try:
            client = get_llm_client(request.model_provider)
            response = await client.chat(request.messages, **request.kwargs)
            return response

        except Exception as e:
            logger.error(f"âŒ è¯·æ±‚ {request.request_id} å¤±è´¥: {e}")
            raise

    def _get_cache_key(self, request: BatchRequest) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = json.dumps({
            "messages": request.messages,
            "kwargs": request.kwargs,
            "model_provider": request.model_provider
        }, sort_keys=True)
        return hashlib.md5(content.encode()).hexdigest()

    async def cleanup_cache(self):
        """æ¸…ç†è¿‡æœŸç¼“å­˜"""
        current_time = time.time()
        expired_keys = [
            key for key, (_, timestamp) in self._result_cache.items()
            if current_time - timestamp > self.config.result_cache_ttl
        ]

        for key in expired_keys:
            del self._result_cache[key]

        # æ¸…ç†è¯·æ±‚ç¼“å­˜
        if len(self._request_cache) > self.config.dedup_cache_size:
            # ä¿ç•™æœ€è¿‘çš„ä¸€åŠ
            items_to_keep = list(self._request_cache.items())[
                -self.config.dedup_cache_size // 2:
            ]
            self._request_cache = dict(items_to_keep)

        logger.debug(f"ğŸ§¹ æ¸…ç†äº† {len(expired_keys)} ä¸ªè¿‡æœŸç¼“å­˜")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        pending = len(self._pending_requests)
        processing = len(self._processing_batches)

        efficiency = 0
        if self._stats["total_requests"] > 0:
            efficiency = (
                self._stats["batched_requests"] / self._stats["total_requests"]
            )

        avg_duration = 0
        if self._stats["batched_requests"] > 0:
            avg_duration = (
                self._stats["total_duration"] / self._stats["batched_requests"]
            )

        return {
            "pending_requests": pending,
            "processing_batches": processing,
            "cache_size": len(self._result_cache),
            "request_cache_size": len(self._request_cache),
            "statistics": {
                **self._stats,
                "efficiency": round(efficiency, 4),
                "avg_duration": round(avg_duration, 4)
            }
        }

    async def shutdown(self):
        """å…³é—­æ‰¹å¤„ç†å™¨"""
        # ç­‰å¾…æ‰€æœ‰å¾…å¤„ç†è¯·æ±‚å®Œæˆ
        timeout = 30
        start_time = time.time()

        while self._pending_requests and time.time() - start_time < timeout:
            await asyncio.sleep(0.1)

        if self._pending_requests:
            logger.warning(
                f"âš ï¸ å…³é—­æ—¶ä»æœ‰ {len(self._pending_requests)} ä¸ªå¾…å¤„ç†è¯·æ±‚"
            )

        # å–æ¶ˆæ‰¹å¤„ç†ä»»åŠ¡
        if self._batch_task and not self._batch_task.done():
            self._batch_task.cancel()

        logger.info("âœ… LLMæ‰¹å¤„ç†å™¨å·²å…³é—­")


# å…¨å±€æ‰¹å¤„ç†å™¨å®ä¾‹
_global_batch_processor: Optional[LLMBatchProcessor] = None


def get_batch_processor(config: Optional[BatchConfig] = None) -> LLMBatchProcessor:
    """
    è·å–å…¨å±€æ‰¹å¤„ç†å™¨å®ä¾‹

    Args:
        config: æ‰¹å¤„ç†é…ç½®

    Returns:
        LLMBatchProcessor: æ‰¹å¤„ç†å™¨å®ä¾‹
    """
    global _global_batch_processor

    if _global_batch_processor is None:
        _global_batch_processor = LLMBatchProcessor(config)

        # å¯åŠ¨ç¼“å­˜æ¸…ç†ä»»åŠ¡
        asyncio.create_task(_cleanup_task())

    return _global_batch_processor


async def _cleanup_task():
    """å®šæœŸæ¸…ç†ç¼“å­˜"""
    while True:
        try:
            await asyncio.sleep(300)  # æ¯5åˆ†é’Ÿæ¸…ç†ä¸€æ¬¡
            if _global_batch_processor:
                await _global_batch_processor.cleanup_cache()
        except asyncio.CancelledError:
            break
        except Exception as e:
            logger.error(f"âŒ ç¼“å­˜æ¸…ç†ä»»åŠ¡é”™è¯¯: {e}")


# è£…é¥°å™¨
def with_batch_processing(model_provider: str = "zhipu"):
    """
    æ‰¹å¤„ç†è£…é¥°å™¨

    ç”¨æ³•:
    ```python
    @with_batch_processing("zhipu")
    async def generate_text(prompt: str):
        return await llm_client.chat([{"role": "user", "content": prompt}])
    ```
    """
    def decorator(func: Callable[..., Awaitable[str]]) -> Callable[..., Awaitable[str]]:
        async def wrapper(*args, **kwargs) -> str:
            # æå–æ¶ˆæ¯
            messages = kwargs.get("messages")
            if not messages and args:
                messages = args[0]

            if not messages:
                return await func(*args, **kwargs)

            processor = get_batch_processor()
            return await processor.process(
                messages=messages,
                model_provider=model_provider
            )

        return wrapper
    return decorator


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_batch_processor():
        """æµ‹è¯•æ‰¹å¤„ç†å™¨"""
        processor = LLMBatchProcessor(
            BatchConfig(
                batch_size=5,
                batch_timeout=1.0,
                max_concurrent_batches=2
            )
        )

        # å‘é€å¤šä¸ªè¯·æ±‚
        tasks = []
        for i in range(20):
            task = processor.process(
                messages=[{"role": "user", "content": f"æµ‹è¯•æ¶ˆæ¯ {i+1}"}],
                model_provider="zhipu"
            )
            tasks.append(task)

        # ç­‰å¾…æ‰€æœ‰è¯·æ±‚å®Œæˆ
        results = await asyncio.gather(*tasks, return_exceptions=True)

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        stats = processor.get_stats()
        logger.info("\n=== æ‰¹å¤„ç†ç»Ÿè®¡ ===")
        logger.info(json.dumps(stats, indent=2, ensure_ascii=False))

        await processor.shutdown()

    asyncio.run(test_batch_processor())
