"""
LangSmithè¿½è¸ªå®¢æˆ·ç«¯
 æ¶æ„çš„LangSmithé›†æˆ
"""
import os
import json
import logging
from typing import Dict, Any, Optional, List, AsyncGenerator
from datetime import datetime
import uuid

# LangSmithç›¸å…³å¯¼å…¥
try:
    from langsmith import Client as LangSmithClient
    from langsmith import RunTree
    LANGSMITH_AVAILABLE = True
except ImportError:
    LangSmithClient = None
    RunTree = None
    LANGSMITH_AVAILABLE = False


class LangSmithTracer:
    """
    LangSmithè¿½è¸ªå™¨
    
    åŠŸèƒ½ï¼š
    1. åˆ›å»ºå’Œç®¡ç†LangSmithè¿è¡Œè¿½è¸ª
    2. è®°å½•LLMè°ƒç”¨å’Œå“åº”
    3. ç”Ÿæˆè¿½è¸ªæŠ¥å‘Š
    """
    
    def __init__(self, enable_tracing: bool = True):
        self.enable_tracing = enable_tracing
        self.logger = logging.getLogger(__name__)
        self.langsmith_client = None
        self.project_name = os.getenv("LANGCHAIN_PROJECT", "juben-drama-planner")
        
        if self.enable_tracing:
            self._init_langsmith()
    
    def _init_langsmith(self):
        """åˆå§‹åŒ–LangSmithå®¢æˆ·ç«¯"""
        try:
            if not LANGSMITH_AVAILABLE:
                self.logger.warning("âš ï¸ LangSmithåº“æœªå®‰è£…ï¼Œç¦ç”¨è¿½è¸ª")
                self.enable_tracing = False
                return
            
            # æ£€æŸ¥ç¯å¢ƒå˜é‡
            api_key = os.getenv("LANGCHAIN_API_KEY")
            if not api_key:
                self.logger.warning("âš ï¸ LANGCHAIN_API_KEYæœªè®¾ç½®ï¼Œç¦ç”¨LangSmithè¿½è¸ª")
                self.enable_tracing = False
                return
            
            # éªŒè¯APIå¯†é’¥æ ¼å¼
            if not api_key.startswith(('lsv2_pt_', 'ls__')):
                self.logger.warning("âš ï¸ LANGCHAIN_API_KEYæ ¼å¼å¯èƒ½ä¸æ­£ç¡®ï¼Œç¦ç”¨LangSmithè¿½è¸ª")
                self.enable_tracing = False
                return
            
            # åˆå§‹åŒ–LangSmithå®¢æˆ·ç«¯
            self.langsmith_client = LangSmithClient(
                api_key=api_key,
                api_url=os.getenv("LANGCHAIN_ENDPOINT", "https://api.smith.langchain.com")
            )
            
            self.logger.info(f"âœ… LangSmithè¿½è¸ªåˆå§‹åŒ–æˆåŠŸï¼Œé¡¹ç›®: {self.project_name}")
            
        except Exception as e:
            self.logger.error(f"âŒ LangSmithåˆå§‹åŒ–å¤±è´¥: {e}")
            self.enable_tracing = False
    
    def create_run_tree(
        self,
        name: str,
        run_type: str = "chain",
        inputs: Optional[Dict[str, Any]] = None,
        metadata: Optional[Dict[str, Any]] = None,
        parent_run_id: Optional[str] = None
    ) -> Optional[RunTree]:
        """
        åˆ›å»ºè¿è¡Œæ ‘
        
        Args:
            name: è¿è¡Œåç§°
            run_type: è¿è¡Œç±»å‹
            inputs: è¾“å…¥æ•°æ®
            metadata: å…ƒæ•°æ®
            parent_run_id: çˆ¶è¿è¡ŒID
            
        Returns:
            RunTree: è¿è¡Œæ ‘å¯¹è±¡
        """
        if not self.enable_tracing or not self.langsmith_client:
            return None
        
        try:
            run_tree = RunTree(
                name=name,
                run_type=run_type,
                inputs=inputs or {},
                metadata=metadata or {},
                parent_run_id=parent_run_id,
                project_name=self.project_name
            )
            
            self.logger.debug(f"ğŸ” åˆ›å»ºè¿è¡Œæ ‘: {name}, ç±»å‹: {run_type}")
            return run_tree
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºè¿è¡Œæ ‘å¤±è´¥: {e}")
            return None
    
    def end_run(
        self,
        run_tree: RunTree,
        outputs: Optional[Dict[str, Any]] = None,
        error: Optional[str] = None
    ) -> bool:
        """
        ç»“æŸè¿è¡Œ
        
        Args:
            run_tree: è¿è¡Œæ ‘å¯¹è±¡
            outputs: è¾“å‡ºæ•°æ®
            error: é”™è¯¯ä¿¡æ¯
            
        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        if not self.enable_tracing or not run_tree:
            return False
        
        try:
            run_tree.end(
                outputs=outputs or {},
                error=error
            )
            
            self.logger.debug(f"ğŸ” ç»“æŸè¿è¡Œæ ‘: {run_tree.name}")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ ç»“æŸè¿è¡Œæ ‘å¤±è´¥: {e}")
            return False
    
    def create_child_run(
        self,
        parent_run: RunTree,
        name: str,
        run_type: str = "tool",
        inputs: Optional[Dict[str, Any]] = None,
        metadata: Optional[Dict[str, Any]] = None
    ) -> Optional[RunTree]:
        """
        åˆ›å»ºå­è¿è¡Œ
        
        Args:
            parent_run: çˆ¶è¿è¡Œå¯¹è±¡
            name: è¿è¡Œåç§°
            run_type: è¿è¡Œç±»å‹
            inputs: è¾“å…¥æ•°æ®
            metadata: å…ƒæ•°æ®
            
        Returns:
            RunTree: å­è¿è¡Œå¯¹è±¡
        """
        if not self.enable_tracing or not parent_run:
            return None
        
        try:
            child_run = parent_run.create_child(
                name=name,
                run_type=run_type,
                inputs=inputs or {},
                metadata=metadata or {}
            )
            
            self.logger.debug(f"ğŸ” åˆ›å»ºå­è¿è¡Œ: {name}, ç±»å‹: {run_type}")
            return child_run
            
        except Exception as e:
            self.logger.error(f"âŒ åˆ›å»ºå­è¿è¡Œå¤±è´¥: {e}")
            return None


class LangSmithLLMClient:
    """
    å¸¦LangSmithè¿½è¸ªçš„LLMå®¢æˆ·ç«¯
    
    åŠŸèƒ½ï¼š
    1. é›†æˆLangSmithè¿½è¸ª
    2. è®°å½•LLMè°ƒç”¨è¯¦æƒ…
    3. Tokenç»Ÿè®¡
    """
    
    def __init__(self, base_llm_client, enable_tracing: bool = True):
        self.base_llm_client = base_llm_client
        self.tracer = LangSmithTracer(enable_tracing)
        self.logger = logging.getLogger(__name__)

    async def chat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """
        ä¸åŸºç¡€LLMå®¢æˆ·ç«¯ä¿æŒä¸€è‡´çš„å…¼å®¹æ¥å£ã€‚
        é»˜è®¤ä¸æ³¨å…¥ä¸šåŠ¡ç»´åº¦å­—æ®µï¼Œé¿å…å½±å“è°ƒç”¨æ–¹ç­¾åã€‚
        """
        return await self.chat_with_tracing(messages=messages, **kwargs)

    async def stream_chat(self, messages: List[Dict[str, str]], **kwargs) -> AsyncGenerator[str, None]:
        """
        ä¸åŸºç¡€LLMå®¢æˆ·ç«¯ä¿æŒä¸€è‡´çš„å…¼å®¹æµå¼æ¥å£ã€‚
        """
        async for chunk in self.stream_chat_with_tracing(messages=messages, **kwargs):
            yield chunk

    async def achat(self, messages: List[Dict[str, str]], **kwargs) -> str:
        """å…¼å®¹éƒ¨åˆ†ä»£ç ä½¿ç”¨çš„ achat å‘½åã€‚"""
        return await self.chat(messages, **kwargs)

    async def astream_chat(self, messages: List[Dict[str, str]], **kwargs) -> AsyncGenerator[str, None]:
        """å…¼å®¹éƒ¨åˆ†ä»£ç ä½¿ç”¨çš„ astream_chat å‘½åã€‚"""
        async for chunk in self.stream_chat(messages, **kwargs):
            yield chunk
    
    async def chat_with_tracing(
        self,
        messages: List[Dict[str, str]],
        agent_name: str = "unknown",
        user_id: str = "unknown",
        session_id: str = "unknown",
        token_accumulator_key: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        å¸¦è¿½è¸ªçš„èŠå¤©è°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            agent_name: Agentåç§°
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID
            token_accumulator_key: Tokenç´¯åŠ å™¨é”®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            str: å“åº”å†…å®¹
        """
        # åˆ›å»ºä¸»è¿è¡Œæ ‘
        main_run = self.tracer.create_run_tree(
            name=f"{agent_name}_chat",
            run_type="chain",
            inputs={"messages": messages, "user_id": user_id, "session_id": session_id},
            metadata={"agent_name": agent_name, "provider": getattr(self.base_llm_client, 'provider', 'unknown')}
        )
        
        try:
            # åˆ›å»ºLLMè°ƒç”¨å­è¿è¡Œ
            llm_run = self.tracer.create_child_run(
                main_run,
                name="llm_call",
                run_type="llm",
                inputs={"messages": messages},
                metadata={"agent_name": agent_name}
            )
            
            # è°ƒç”¨åŸºç¡€LLMå®¢æˆ·ç«¯
            response = await self.base_llm_client.chat(messages, **kwargs)
            
            # ä¼°ç®—tokenä½¿ç”¨é‡
            usage = self._estimate_token_usage(messages, response)
            
            # è®°å½•tokenä½¿ç”¨é‡
            if token_accumulator_key:
                await self._record_token_usage(
                    token_accumulator_key, usage, agent_name, 
                    getattr(self.base_llm_client, 'provider', 'unknown')
                )
            
            # ç»“æŸLLMè¿è¡Œ
            self.tracer.end_run(
                llm_run,
                outputs={"response": response, "usage": usage.to_dict()}
            )
            
            # ç»“æŸä¸»è¿è¡Œ
            self.tracer.end_run(
                main_run,
                outputs={"response": response, "usage": usage.to_dict()}
            )
            
            return response
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            self.tracer.end_run(main_run, error=str(e))
            raise
    
    async def stream_chat_with_tracing(
        self,
        messages: List[Dict[str, str]],
        agent_name: str = "unknown",
        user_id: str = "unknown",
        session_id: str = "unknown",
        token_accumulator_key: Optional[str] = None,
        **kwargs
    ) -> AsyncGenerator[str, None]:
        """
        å¸¦è¿½è¸ªçš„æµå¼èŠå¤©è°ƒç”¨
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            agent_name: Agentåç§°
            user_id: ç”¨æˆ·ID
            session_id: ä¼šè¯ID
            token_accumulator_key: Tokenç´¯åŠ å™¨é”®
            **kwargs: å…¶ä»–å‚æ•°
            
        Returns:
            AsyncGenerator[str, None]: å“åº”æµ
        """
        # åˆ›å»ºä¸»è¿è¡Œæ ‘
        main_run = self.tracer.create_run_tree(
            name=f"{agent_name}_stream_chat",
            run_type="chain",
            inputs={"messages": messages, "user_id": user_id, "session_id": session_id},
            metadata={"agent_name": agent_name, "provider": getattr(self.base_llm_client, 'provider', 'unknown')}
        )
        
        full_response = ""
        
        try:
            # åˆ›å»ºLLMè°ƒç”¨å­è¿è¡Œ
            llm_run = self.tracer.create_child_run(
                main_run,
                name="llm_stream_call",
                run_type="llm",
                inputs={"messages": messages},
                metadata={"agent_name": agent_name}
            )
            
            # æµå¼è°ƒç”¨åŸºç¡€LLMå®¢æˆ·ç«¯
            async for chunk in self.base_llm_client.stream_chat(messages, **kwargs):
                if chunk:
                    full_response += chunk
                    yield chunk
            
            # ä¼°ç®—tokenä½¿ç”¨é‡
            usage = self._estimate_token_usage(messages, full_response)
            
            # è®°å½•tokenä½¿ç”¨é‡
            if token_accumulator_key:
                await self._record_token_usage(
                    token_accumulator_key, usage, agent_name,
                    getattr(self.base_llm_client, 'provider', 'unknown')
                )
            
            # ç»“æŸLLMè¿è¡Œ
            self.tracer.end_run(
                llm_run,
                outputs={"response": full_response, "usage": usage.to_dict()}
            )
            
            # ç»“æŸä¸»è¿è¡Œ
            self.tracer.end_run(
                main_run,
                outputs={"response": full_response, "usage": usage.to_dict()}
            )
            
        except Exception as e:
            # è®°å½•é”™è¯¯
            self.tracer.end_run(main_run, error=str(e))
            raise
    
    def _estimate_token_usage(self, messages: List[Dict[str, str]], response: str) -> 'TokenUsage':
        """ä¼°ç®—tokenä½¿ç”¨é‡"""
        from .token_accumulator import TokenUsage
        
        # è®¡ç®—è¾“å…¥tokenæ•°
        input_text = " ".join(msg.get("content", "") for msg in messages)
        prompt_tokens = self._count_tokens(input_text)
        
        # è®¡ç®—è¾“å‡ºtokenæ•°
        completion_tokens = self._count_tokens(response)
        
        return TokenUsage(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=prompt_tokens + completion_tokens
        )
    
    def _count_tokens(self, text: str) -> int:
        """è®¡ç®—tokenæ•°é‡ï¼ˆç®€åŒ–ä¼°ç®—ï¼‰"""
        # ä¸­æ–‡æŒ‰å­—ç¬¦æ•°è®¡ç®—ï¼Œè‹±æ–‡æŒ‰å•è¯æ•°*1.3è®¡ç®—
        chinese_chars = len([c for c in text if '\u4e00' <= c <= '\u9fff'])
        english_words = len(text.replace(' ', '').replace('\n', '').replace('\t', '')) - chinese_chars
        
        # ä¼°ç®—ï¼šä¸­æ–‡1å­—ç¬¦â‰ˆ1tokenï¼Œè‹±æ–‡1å­—ç¬¦â‰ˆ0.5token
        return int(chinese_chars + english_words * 0.5)
    
    async def _record_token_usage(
        self,
        accumulator_key: str,
        usage: 'TokenUsage',
        agent_name: str,
        provider: str
    ):
        """è®°å½•tokenä½¿ç”¨é‡"""
        try:
            from .token_accumulator import add_token_usage
            
            await add_token_usage(
                accumulator_key,
                usage,
                agent_name,
                getattr(self.base_llm_client, 'model_name', 'unknown'),
                provider
            )
        except Exception as e:
            self.logger.warning(f"âš ï¸ Tokenä½¿ç”¨é‡è®°å½•å¤±è´¥: {e}")


# ä¾¿æ·å‡½æ•°
def create_langsmith_llm_client(base_llm_client, enable_tracing: bool = True) -> LangSmithLLMClient:
    """åˆ›å»ºå¸¦LangSmithè¿½è¸ªçš„LLMå®¢æˆ·ç«¯"""
    return LangSmithLLMClient(base_llm_client, enable_tracing)
